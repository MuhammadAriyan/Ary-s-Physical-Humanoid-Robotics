"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[963],{4459:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"part-1-foundations/introduction-to-physical-ai","title":"Introduction to Physical AI","description":"Learning Objectives","source":"@site/docs/part-1-foundations/01-introduction-to-physical-ai.md","sourceDirName":"part-1-foundations","slug":"/part-1-foundations/introduction-to-physical-ai","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-1-foundations/introduction-to-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part-1-foundations/01-introduction-to-physical-ai.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Introduction to Physical AI","sidebar_position":1},"sidebar":"tutorialSidebar","next":{"title":"Week 1-2 Overview: Physical AI Foundations","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-1-foundations/01a-week-1-2-overview"}}');var r=i(4848),t=i(8453);const o={title:"Introduction to Physical AI",sidebar_position:1},a="Chapter 1: Introduction to Physical AI",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"1.1 What is Physical AI?",id:"11-what-is-physical-ai",level:2},{value:"Embodied Intelligence Defined",id:"embodied-intelligence-defined",level:3},{value:"The Physical AI Spectrum",id:"the-physical-ai-spectrum",level:3},{value:"1.2 From Digital AI to Physical AI",id:"12-from-digital-ai-to-physical-ai",level:2},{value:"Key Differences",id:"key-differences",level:3},{value:"Why Embodiment Matters",id:"why-embodiment-matters",level:3},{value:"1.3 Humanoid Robotics Landscape",id:"13-humanoid-robotics-landscape",level:2},{value:"Current Major Players",id:"current-major-players",level:3},{value:"Technical Approaches Comparison",id:"technical-approaches-comparison",level:3},{value:"See Also",id:"see-also",level:3},{value:"1.4 Sensor Systems",id:"14-sensor-systems",level:2},{value:"Visual Perception Systems",id:"visual-perception-systems",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"LIDAR Systems",id:"lidar-systems",level:3},{value:"1.5 Sensor Configuration Example",id:"15-sensor-configuration-example",level:2},{value:"YAML Configuration File",id:"yaml-configuration-file",level:3},{value:"Hardware Requirements Reference",id:"hardware-requirements-reference",level:3},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Key Terminology",id:"key-terminology",level:3},{value:"Further Reading",id:"further-reading",level:3},{value:"Next Chapter",id:"next-chapter",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-1-introduction-to-physical-ai",children:"Chapter 1: Introduction to Physical AI"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Define Physical AI and distinguish it from traditional digital AI systems"}),"\n",(0,r.jsx)(n.li,{children:"Understand the concept of embodied intelligence and why physical embodiment matters"}),"\n",(0,r.jsx)(n.li,{children:"Identify major players in the humanoid robotics landscape"}),"\n",(0,r.jsx)(n.li,{children:"Configure sensor systems for physical AI applications"}),"\n",(0,r.jsx)(n.li,{children:"Implement basic sensor configurations using ROS 2 and Python"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"11-what-is-physical-ai",children:"1.1 What is Physical AI?"}),"\n",(0,r.jsx)(n.p,{children:"Physical AI represents a paradigm shift in artificial intelligence from purely computational systems to agents that interact directly with the physical world. Unlike traditional AI that operates in digital abstraction, Physical AI combines perception, reasoning, and action in real-world environments through robotic platforms."}),"\n",(0,r.jsx)(n.h3,{id:"embodied-intelligence-defined",children:"Embodied Intelligence Defined"}),"\n",(0,r.jsx)(n.p,{children:'Embodied intelligence is the principle that intelligence emerges from the interaction between a system and its physical environment. This contrasts sharply with the "brain in a jar" model of pure software AI. A Physical AI system:'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perceives"})," the world through sensors (vision, touch, proprioception)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reasons"})," about state, goals, and actions using AI models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Acts"})," through actuators that modify the physical environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Learns"})," from physical interactions and consequences"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This closed-loop relationship between perception and action creates capabilities that purely digital systems cannot achieve. A humanoid robot must maintain balance while reaching for an object, anticipate how objects will respond to forces, and adapt its movements based on real-time feedback."}),"\n",(0,r.jsx)(n.h3,{id:"the-physical-ai-spectrum",children:"The Physical AI Spectrum"}),"\n",(0,r.jsx)(n.p,{children:"Physical AI spans a wide range of systems, from fixed industrial arms to humanoid robots designed for general-purpose operation:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"System Type"}),(0,r.jsx)(n.th,{children:"Embodiment"}),(0,r.jsx)(n.th,{children:"Intelligence Level"}),(0,r.jsx)(n.th,{children:"Primary Domain"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Industrial Robots"}),(0,r.jsx)(n.td,{children:"Fixed/gantry"}),(0,r.jsx)(n.td,{children:"Pre-programmed"}),(0,r.jsx)(n.td,{children:"Manufacturing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Mobile Robots"}),(0,r.jsx)(n.td,{children:"Wheeled/track"}),(0,r.jsx)(n.td,{children:"Navigation-focused"}),(0,r.jsx)(n.td,{children:"Logistics"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Manipulation Arms"}),(0,r.jsx)(n.td,{children:"Articulated"}),(0,r.jsx)(n.td,{children:"Task-specific"}),(0,r.jsx)(n.td,{children:"Assembly, surgery"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Humanoids"}),(0,r.jsx)(n.td,{children:"Bipedal anthropomorphic"}),(0,r.jsx)(n.td,{children:"General-purpose"}),(0,r.jsx)(n.td,{children:"Healthcare, domestic"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots represent the most challenging platform for Physical AI due to the complexity of bipedal locomotion and the expectation of operating in human-designed environments."}),"\n",(0,r.jsx)(n.h2,{id:"12-from-digital-ai-to-physical-ai",children:"1.2 From Digital AI to Physical AI"}),"\n",(0,r.jsx)(n.p,{children:"The transition from digital AI to Physical AI involves fundamental changes in how systems are designed, trained, and deployed."}),"\n",(0,r.jsx)(n.h3,{id:"key-differences",children:"Key Differences"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Digital AI"}),(0,r.jsx)(n.th,{children:"Physical AI"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Environment"}),(0,r.jsx)(n.td,{children:"Bounded data space"}),(0,r.jsx)(n.td,{children:"Unstructured real world"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Feedback Latency"}),(0,r.jsx)(n.td,{children:"Batch processing"}),(0,r.jsx)(n.td,{children:"Real-time closed loop"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Failure Modes"}),(0,r.jsx)(n.td,{children:"Incorrect outputs"}),(0,r.jsx)(n.td,{children:"Physical damage, safety hazards"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Generalization"}),(0,r.jsx)(n.td,{children:"Domain-specific"}),(0,r.jsx)(n.td,{children:"Must handle open-world scenarios"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Computation"}),(0,r.jsx)(n.td,{children:"Cloud/\u6570\u636e\u4e2d\u5fc3"}),(0,r.jsx)(n.td,{children:"Edge processing required"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"why-embodiment-matters",children:"Why Embodiment Matters"}),"\n",(0,r.jsx)(n.p,{children:'Consider the challenge of understanding what a "cup" is. A purely digital AI trained on images learns that cups have handles and certain shapes. An embodied robot learns that cups can be grasped in multiple ways, have weight, contain liquids, and must be handled carefully to avoid spills. Physical interaction provides grounding that visual training alone cannot.'}),"\n",(0,r.jsx)(n.p,{children:"The Sim2Real gap\u2014the difference between simulation and reality\u2014remains one of the biggest challenges in Physical AI. Models trained in simulation often fail when deployed on physical robots due to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Sensor noise and calibration differences"}),"\n",(0,r.jsx)(n.li,{children:"Actuator limitations and delays"}),"\n",(0,r.jsx)(n.li,{children:"Unmodeled physical phenomena (friction, compliance, wear)"}),"\n",(0,r.jsx)(n.li,{children:"Environmental variations (lighting, surfaces, objects)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"13-humanoid-robotics-landscape",children:"1.3 Humanoid Robotics Landscape"}),"\n",(0,r.jsx)(n.p,{children:"The humanoid robotics field has evolved dramatically, with several major players pushing the boundaries of what is possible."}),"\n",(0,r.jsx)(n.h3,{id:"current-major-players",children:"Current Major Players"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Boston Dynamics"})," has established itself as a leader in dynamic locomotion with Atlas, demonstrating capabilities like parkour, backflips, and autonomous manipulation. Their\u6db2\u538b actuation system provides exceptional power density, though at the cost of complexity and maintenance. Atlas uses a combination of model-predictive control and learning-based approaches for its movements."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tesla Optimus"})," represents a different approach\u2014optimizing for manufacturing efficiency and eventual consumer affordability. With an estimated target cost of under $20,000, Optimus aims to make humanoid robots commercially viable at scale. Tesla leverages their expertise in AI and manufacturing to create a vertically integrated solution."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Agility Robotics"})," focuses on practical deployment with Digit, a robot designed for logistics tasks. Their approach emphasizes durability and ease of maintenance over maximum performance. Digit has been deployed in real warehouse environments, gathering valuable operational data."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Figure AI"})," has raised significant funding to develop general-purpose humanoid robots. Their approach emphasizes rapid iteration and integration of latest AI advances, partnering with BMW for initial commercial deployment."]}),"\n",(0,r.jsx)(n.h3,{id:"technical-approaches-comparison",children:"Technical Approaches Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Platform"}),(0,r.jsx)(n.th,{children:"Actuation"}),(0,r.jsx)(n.th,{children:"Control Philosophy"}),(0,r.jsx)(n.th,{children:"Development Focus"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Atlas (Boston Dynamics)"}),(0,r.jsx)(n.td,{children:"Hydraulic"}),(0,r.jsx)(n.td,{children:"Model-predictive + learning"}),(0,r.jsx)(n.td,{children:"Research, demonstrations"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Optimus (Tesla)"}),(0,r.jsx)(n.td,{children:"Electric (custom)"}),(0,r.jsx)(n.td,{children:"End-to-end learning"}),(0,r.jsx)(n.td,{children:"Commercial manufacturing"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Digit (Agility)"}),(0,r.jsx)(n.td,{children:"Electric series elastic"}),(0,r.jsx)(n.td,{children:"Reactive control"}),(0,r.jsx)(n.td,{children:"Logistics, durability"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Figure 01"}),(0,r.jsx)(n.td,{children:"Electric"}),(0,r.jsx)(n.td,{children:"Hybrid AI/control"}),(0,r.jsx)(n.td,{children:"General-purpose AI"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"see-also",children:"See Also"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Part 2"}),": ROS 2 Fundamentals covers robot operating system concepts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Part 3"}),": Simulation covers Gazebo and Unity tools"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Appendix A"}),": ",(0,r.jsx)(n.a,{href:"appendix/hardware-specifications",children:"Hardware Specifications"})," provides detailed actuator specs"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"14-sensor-systems",children:"1.4 Sensor Systems"}),"\n",(0,r.jsx)(n.p,{children:"Sensor systems form the perceptual foundation of Physical AI. Without accurate sensing, even the most sophisticated AI cannot make good decisions."}),"\n",(0,r.jsx)(n.h3,{id:"visual-perception-systems",children:"Visual Perception Systems"}),"\n",(0,r.jsx)(n.p,{children:"Cameras provide rich environmental information at relatively low cost and power consumption. Modern humanoid robots typically use:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"RGB Cameras"}),": Standard color imaging for visual recognition tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Cameras"}),": Active stereo (Intel RealSense) or structured light (Microsoft Kinect) for 3D information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Event Cameras"}),": Bio-inspired sensors that detect pixel-level changes with microsecond latency"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.p,{children:"IMUs combine accelerometers and gyroscopes to measure the robot's orientation and acceleration. For humanoid robots, IMUs are critical for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Balance estimation during locomotion"}),"\n",(0,r.jsx)(n.li,{children:"Detecting disturbances and falls"}),"\n",(0,r.jsx)(n.li,{children:"Fusing with other sensors for improved state estimation"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Typical specifications for humanoid IMUs:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Accelerometer range: \xb116g"}),"\n",(0,r.jsx)(n.li,{children:"Gyroscope range: \xb12000 deg/s"}),"\n",(0,r.jsx)(n.li,{children:"Sample rate: 100-1000 Hz"}),"\n",(0,r.jsx)(n.li,{children:"Noise density: < 0.01 deg/s/\u221aHz"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Force/torque sensors measure the interaction forces at the robot's feet and hands:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Foot sensors"}),": Enable balance control by detecting ground contact forces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Wrist sensors"}),": Enable precise manipulation by detecting grasp forces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tactile sensors"}),": Provide detailed contact information for delicate tasks"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-systems",children:"LIDAR Systems"}),"\n",(0,r.jsx)(n.p,{children:"While less common on humanoid robots due to weight constraints, LIDAR provides accurate distance measurements useful for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Long-range obstacle detection"}),"\n",(0,r.jsx)(n.li,{children:"Creating precise 3D maps of the environment"}),"\n",(0,r.jsx)(n.li,{children:"Navigation in GPS-denied environments"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"15-sensor-configuration-example",children:"1.5 Sensor Configuration Example"}),"\n",(0,r.jsx)(n.p,{children:"The following example demonstrates how to configure sensor systems using ROS 2 and Python for a humanoid robot platform."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nHumanoid Robot Sensor Configuration Module\n\nThis module provides sensor configuration and initialization\nfor Physical AI applications using ROS 2 and Python.\n"""\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nimport numpy as np\nfrom enum import Enum\n\n\nclass SensorType(Enum):\n    """Enumeration of supported sensor types."""\n    CAMERA_RGB = "camera_rgb"\n    CAMERA_DEPTH = "camera_depth"\n    IMU = "imu"\n    FORCE_TORQUE = "force_torque"\n    LIDAR = "lidar"\n    TACTILE = "tactile"\n\n\n@dataclass\nclass SensorConfig:\n    """Configuration for a single sensor."""\n    name: str\n    sensor_type: SensorType\n    topic: str\n    frame_id: str\n    sample_rate: float\n    noise_std: float\n    calibration_file: Optional[str] = None\n\n    def validate(self) -> bool:\n        """Validate sensor configuration."""\n        if self.sample_rate <= 0:\n            raise ValueError(f"Sample rate must be positive: {self.sample_rate}")\n        if self.noise_std < 0:\n            raise ValueError(f"Noise std must be non-negative: {self.noise_std}")\n        return True\n\n\nclass SensorSuite:\n    """\n    Manages sensor configuration and initialization for humanoid robots.\n\n    Attributes:\n        sensors: Dictionary of configured sensors by name\n        calibration_data: Loaded calibration parameters\n        sample_rates: Current sample rates for all sensors\n    """\n\n    def __init__(self):\n        self.sensors: Dict[str, SensorConfig] = {}\n        self.calibration_data: Dict[str, np.ndarray] = {}\n        self.sample_rates: Dict[str, float] = {}\n        self._is_initialized = False\n\n    def add_sensor(self, config: SensorConfig) -> bool:\n        """\n        Add a sensor to the configuration.\n\n        Args:\n            config: Sensor configuration parameters\n\n        Returns:\n            True if sensor was added successfully\n        """\n        config.validate()\n        self.sensors[config.name] = config\n        self.sample_rates[config.name] = config.sample_rate\n        return True\n\n    def configure_imu(self, name: str, frame_id: str,\n                      sample_rate: float = 200.0) -> SensorConfig:\n        """\n        Configure an IMU sensor with standard parameters.\n\n        Args:\n            name: Unique sensor identifier\n            frame_id: ROS frame for sensor data\n            sample_rate: Target sampling rate in Hz\n\n        Returns:\n            Configured IMU sensor\n        """\n        config = SensorConfig(\n            name=name,\n            sensor_type=SensorType.IMU,\n            topic=f"/{name}/imu",\n            frame_id=frame_id,\n            sample_rate=sample_rate,\n            noise_std=0.01,\n            calibration_file=f"/config/calibration/{name}_calibration.yaml"\n        )\n        self.add_sensor(config)\n        return config\n\n    def configure_force_torque(self, name: str, frame_id: str,\n                                location: str = "wrist") -> SensorConfig:\n        """\n        Configure a force/torque sensor for manipulation or balance.\n\n        Args:\n            name: Unique sensor identifier\n            frame_id: ROS frame for sensor data\n            location: Sensor mounting location (wrist, foot)\n\n        Returns:\n            Configured force/torque sensor\n        """\n        # Sample rates vary by location - higher for manipulation\n        sample_rates = {"wrist": 100.0, "foot": 50.0}\n        rate = sample_rates.get(location, 100.0)\n\n        config = SensorConfig(\n            name=name,\n            sensor_type=SensorType.FORCE_TORQUE,\n            topic=f"/{name}/wrench",\n            frame_id=frame_id,\n            sample_rate=rate,\n            noise_std=0.5,  # Newton-meters\n            calibration_file=f"/config/calibration/{name}_calibration.yaml"\n        )\n        self.add_sensor(config)\n        return config\n\n    def get_sensor_topics(self) -> List[str]:\n        """Get list of all sensor topics for ROS 2 configuration."""\n        return [s.topic for s in self.sensors.values()]\n\n    def validate_system(self) -> Dict[str, bool]:\n        """\n        Validate sensor system configuration.\n\n        Returns:\n            Dictionary mapping sensor names to validation status\n        """\n        results = {}\n        for name, sensor in self.sensors.items():\n            try:\n                sensor.validate()\n                results[name] = True\n            except ValueError as e:\n                results[name] = False\n                print(f"Sensor {name} validation failed: {e}")\n        return results\n\n    def initialize(self) -> bool:\n        """\n        Initialize all sensors in the suite.\n\n        Returns:\n            True if all sensors initialized successfully\n        """\n        if not self.validate_system():\n            raise RuntimeError("Sensor validation failed")\n\n        # Load calibration data for each sensor\n        for name, sensor in self.sensors.items():\n            if sensor.calibration_file:\n                self._load_calibration(name, sensor.calibration_file)\n\n        self._is_initialized = True\n        return self._is_initialized\n\n    def _load_calibration(self, sensor_name: str,\n                          calibration_file: str) -> None:\n        """\n        Load calibration parameters from file.\n\n        In a real implementation, this would parse YAML/XML files\n        and apply transformations to raw sensor data.\n        """\n        # Placeholder for calibration loading\n        self.calibration_data[sensor_name] = np.eye(6)\n\n\ndef create_humanoid_sensor_suite() -> SensorSuite:\n    """\n    Create a complete sensor suite for a humanoid robot.\n\n    Returns:\n        Configured SensorSuite ready for initialization\n    """\n    suite = SensorSuite()\n\n    # Head-mounted cameras\n    suite.configure_imu("head_imu", "head_link", sample_rate=200.0)\n\n    # Body IMU for balance estimation\n    suite.configure_imu("torso_imu", "torso_link", sample_rate=200.0)\n\n    # Force/torque sensors at wrists\n    suite.configure_force_torque("left_wrist_ft", "left_wrist_link", "wrist")\n    suite.configure_force_torque("right_wrist_ft", "right_wrist_link", "wrist")\n\n    # Force/torque sensors at feet\n    suite.configure_force_torque("left_foot_ft", "left_foot_link", "foot")\n    suite.configure_force_torque("right_foot_ft", "right_foot_link", "foot")\n\n    return suite\n\n\nif __name__ == "__main__":\n    # Example usage\n    suite = create_humanoid_sensor_suite()\n\n    print("Humanoid Sensor Suite Configuration:")\n    print("=" * 50)\n    for name, sensor in suite.sensors.items():\n        print(f"\\nSensor: {name}")\n        print(f"  Type: {sensor.sensor_type.value}")\n        print(f"  Topic: {sensor.topic}")\n        print(f"  Frame: {sensor.frame_id}")\n        print(f"  Sample Rate: {sensor.sample_rate} Hz")\n\n    print("\\n" + "=" * 50)\n    print("Sensor Topics for ROS 2:")\n    for topic in suite.get_sensor_topics():\n        print(f"  {topic}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"yaml-configuration-file",children:"YAML Configuration File"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# /config/sensors/humanoid_sensors.yaml\n# Humanoid Robot Sensor Configuration\n\nsensors:\n  head_imu:\n    type: imu\n    frame_id: head_link\n    topic: /sensors/head/imu\n    sample_rate: 200.0\n    accelerometer:\n      range: 16.0  # g\n      noise_density: 0.0002  # g/\u221aHz\n    gyroscope:\n      range: 2000.0  # deg/s\n      noise_density: 0.01  # deg/s/\u221aHz\n\n  torso_imu:\n    type: imu\n    frame_id: torso_link\n    topic: /sensors/torso/imu\n    sample_rate: 200.0\n    accelerometer:\n      range: 16.0\n      noise_density: 0.0002\n    gyroscope:\n      range: 2000.0\n      noise_density: 0.01\n\n  left_wrist_ft:\n    type: force_torque\n    frame_id: left_wrist_link\n    topic: /sensors/left_wrist/wrench\n    sample_rate: 100.0\n    ranges:\n      fx: 100.0  # N\n      fy: 100.0\n      fz: 200.0\n      tx: 10.0   # Nm\n      ty: 10.0\n      tz: 10.0\n\n# Sensor fusion configuration\nsensor_fusion:\n  imu_samples: 10\n  gravity_vector: [0.0, 0.0, 9.81]\n  fusion_gain: 0.01\n"})}),"\n",(0,r.jsx)(n.h3,{id:"hardware-requirements-reference",children:"Hardware Requirements Reference"}),"\n",(0,r.jsx)(n.p,{children:"For sensor configuration, the following minimum hardware specifications are recommended (see Appendix A for complete specifications):"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Component"}),(0,r.jsx)(n.th,{children:"Minimum"}),(0,r.jsx)(n.th,{children:"Recommended"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"CPU"}),(0,r.jsx)(n.td,{children:"4-core @ 2.0 GHz"}),(0,r.jsx)(n.td,{children:"8-core @ 3.0 GHz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"RAM"}),(0,r.jsx)(n.td,{children:"8 GB"}),(0,r.jsx)(n.td,{children:"16 GB"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"GPU"}),(0,r.jsx)(n.td,{children:"None (CPU processing)"}),(0,r.jsx)(n.td,{children:"NVIDIA Jetson or equivalent"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Storage"}),(0,r.jsx)(n.td,{children:"32 GB SSD"}),(0,r.jsx)(n.td,{children:"128 GB NVMe SSD"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Power"}),(0,r.jsx)(n.td,{children:"50W"}),(0,r.jsx)(n.td,{children:"100W (with GPU)"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,r.jsx)(n.p,{children:"Physical AI represents the integration of artificial intelligence with robotic embodiment, enabling systems to perceive, reason about, and act in the physical world. The transition from digital AI to Physical AI introduces unique challenges including real-time processing requirements, safety considerations, and the need for robust perception systems."}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots face particular challenges due to their complex locomotion requirements and the expectation of operating in human-designed environments. Major players in the field are pursuing different technical approaches, from hydraulic actuation with model-predictive control to electric actuation with learning-based approaches."}),"\n",(0,r.jsx)(n.p,{children:"Sensor systems form the foundation of Physical AI, providing the perceptual data needed for decision-making. A comprehensive sensor suite includes visual systems, inertial measurement units, force/torque sensors, and potentially LIDAR for long-range perception."}),"\n",(0,r.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physical AI"}),": AI systems that interact directly with the physical world through robotic embodiment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embodied Intelligence"}),": Intelligence emerging from the interaction between a system and its physical environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sim2Real Gap"}),": The challenge of transferring models trained in simulation to real-world deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensor types for improved state estimation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Embodiment"}),": The physical instantiation of an AI system through a robotic platform"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Proprioception"}),": The sense of the relative position of body parts and their movement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Actuation"}),": The mechanism by which a robot's joints are moved"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": The process of combining data from multiple sensors"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Springer Handbook of Robotics"})," - Siciliano & Khatib (Eds.)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Physical Intelligence"}),": The next frontier in embodied AI research"]}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 Documentation: Robot Operating System 2 fundamentals"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"next-chapter",children:"Next Chapter"}),"\n",(0,r.jsxs)(n.p,{children:["Chapter 2 explores ",(0,r.jsx)(n.strong,{children:"Sensor Systems and Perception"})," in depth, examining multi-modal sensing architectures, computer vision techniques, and sensor fusion algorithms that enable environmental understanding."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Part 1: Foundations"})," | ",(0,r.jsx)(n.a,{href:"part-1-foundations/01a-week-1-2-overview",children:"Week 1-2 Overview"})," | ",(0,r.jsx)(n.a,{href:"part-2-ros2/ros2-fundamentals",children:"Part 2: ROS 2 Fundamentals"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);