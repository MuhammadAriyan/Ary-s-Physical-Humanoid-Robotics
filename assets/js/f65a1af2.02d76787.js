"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[123],{462:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"part-6-conversational/conversational-robotics","title":"Conversational Robotics","description":"Learning Objectives","source":"@site/docs/part-6-conversational/06-conversational-robotics.md","sourceDirName":"part-6-conversational","slug":"/part-6-conversational/conversational-robotics","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-6-conversational/conversational-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part-6-conversational/06-conversational-robotics.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Conversational Robotics","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Weeks 11-12: Humanoid Robot Development","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-5-humanoid/05a-week-11-12-overview"},"next":{"title":"Week 13 Overview: Conversational AI Integration","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-6-conversational/06a-week-13-overview"}}');var s=t(4848),o=t(8453);const a={title:"Conversational Robotics",sidebar_position:6},r="Chapter 6: Conversational Robotics",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"6.1 Introduction to Conversational AI in Robotics",id:"61-introduction-to-conversational-ai-in-robotics",level:2},{value:"The Conversational AI Pipeline",id:"the-conversational-ai-pipeline",level:3},{value:"See Also",id:"see-also",level:3},{value:"6.2 Integrating GPT Models for Natural Dialogue",id:"62-integrating-gpt-models-for-natural-dialogue",level:2},{value:"6.3 Speech Recognition and Text-to-Speech",id:"63-speech-recognition-and-text-to-speech",level:2},{value:"Automatic Speech Recognition",id:"automatic-speech-recognition",level:3},{value:"6.4 Natural Language Understanding",id:"64-natural-language-understanding",level:2},{value:"6.5 Multi-Modal Interaction",id:"65-multi-modal-interaction",level:2},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Key Terminology",id:"key-terminology",level:3},{value:"Further Reading",id:"further-reading",level:3},{value:"Next Chapter",id:"next-chapter",level:3}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-6-conversational-robotics",children:"Chapter 6: Conversational Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the principles of conversational AI and its application to humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Integrate large language models like GPT for natural dialogue generation"}),"\n",(0,s.jsx)(n.li,{children:"Implement speech recognition and text-to-speech pipelines for voice interaction"}),"\n",(0,s.jsx)(n.li,{children:"Design natural language understanding systems that parse user intent"}),"\n",(0,s.jsx)(n.li,{children:"Create multi-modal interaction systems combining speech, gesture, and vision"}),"\n",(0,s.jsx)(n.li,{children:"Build complete dialogue systems that enable meaningful human-robot conversation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"61-introduction-to-conversational-ai-in-robotics",children:"6.1 Introduction to Conversational AI in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Conversational AI represents a fundamental shift in how humans interact with robots. Rather than relying on joysticks, buttons, or complex programming interfaces, conversational AI enables natural spoken dialogue as the primary interaction modality. For humanoid robots designed to operate in human environments, this capability is essential for seamless integration into domestic, healthcare, and service settings."}),"\n",(0,s.jsx)(n.p,{children:"The evolution from command-based interfaces to conversational interaction reflects broader trends in human-computer interaction. Early robotic systems required users to learn specific command syntax or button combinations. Modern conversational interfaces accept natural language, allowing users to communicate intentions without specialized training. This democratization of robot control opens humanoid robotics to elderly users, children, and individuals without technical backgrounds."}),"\n",(0,s.jsx)(n.p,{children:'Humanoid robots present unique opportunities for conversational AI due to their anthropomorphic form. When a robot can speak naturally, gesture expressively, and maintain eye contact during conversation, the interaction feels more intuitive and engaging. A humanoid robot asking "Would you like me to bring you a cup of tea?" is far more natural than a screen-based assistant presenting the same question. The embodied nature of the interaction creates expectations of social intelligence that conversational AI must fulfill.'}),"\n",(0,s.jsx)(n.p,{children:"The technical challenges of conversational robotics extend beyond simple speech processing. A conversational humanoid robot must maintain context across extended interactions, understand ambiguous references to objects and people in the environment, generate appropriate responses that match the robot's personality and capabilities, and synchronize verbal output with non-verbal cues like facial expressions and gestures. These requirements demand integration across multiple AI subsystems including speech recognition, natural language understanding, dialogue management, natural language generation, and speech synthesis."}),"\n",(0,s.jsx)(n.h3,{id:"the-conversational-ai-pipeline",children:"The Conversational AI Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Conversational AI for robots involves a sophisticated pipeline of processing stages, each building upon the outputs of the previous stage. Understanding this pipeline is essential for designing robust dialogue systems that can handle the variability of real-world conversation."}),"\n",(0,s.jsxs)(n.p,{children:["The pipeline begins with ",(0,s.jsx)(n.strong,{children:"automatic speech recognition"})," (ASR), which converts raw audio input into text transcripts. Modern ASR systems use deep neural networks trained on massive speech corpora to achieve high accuracy across diverse speakers and acoustic conditions. For robot applications, ASR must handle background noise, multiple speakers, and the acoustic properties of the robot's physical environment."]}),"\n",(0,s.jsxs)(n.p,{children:["Following speech recognition, ",(0,s.jsx)(n.strong,{children:"natural language understanding"}),' (NLU) parses the transcribed text to extract semantic meaning. NLU identifies the user\'s intent, extracts relevant entities, and resolves references to objects or people in context. For a command like "Bring me the red cup from the kitchen," NLU identifies the intent as a request for object delivery, extracts the object type (cup), color (red), and location (kitchen).']}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"dialogue manager"})," maintains conversation state and determines appropriate system responses. It tracks what has been said, what the current topic is, and what information has been established. The dialogue manager also handles dialogue acts like asking for clarification, confirming understanding, or providing information."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Natural language generation"})," (NLG) creates the textual response that the system will speak. Modern approaches use large language models for NLG, producing fluent text that matches the conversation context and maintains a consistent persona. NLG must also consider the robot's personality and the appropriateness of different response styles."]}),"\n",(0,s.jsxs)(n.p,{children:["Finally, ",(0,s.jsx)(n.strong,{children:"text-to-speech"})," (TTS) synthesis converts the generated text into audible speech. Modern TTS systems using neural network architectures produce remarkably natural-sounding speech with appropriate prosody and emotional tone. The TTS output must also be synchronized with the robot's lip movements and facial expressions for natural embodiment."]}),"\n",(0,s.jsx)(n.h3,{id:"see-also",children:"See Also"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Part 1"}),": ",(0,s.jsx)(n.a,{href:"part-1-foundations/introduction-to-physical-ai",children:"Introduction to Physical AI"})," covers foundational concepts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Part 3"}),": ",(0,s.jsx)(n.a,{href:"part-3-simulation/gazebo-unity-simulation",children:"Gazebo and Unity Simulation"})," discusses sensor simulation for perception"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Part 5"}),": ",(0,s.jsx)(n.a,{href:"part-5-humanoid/humanoid-robot-development",children:"Humanoid Development"})," covers humanoid robot design"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"62-integrating-gpt-models-for-natural-dialogue",children:"6.2 Integrating GPT Models for Natural Dialogue"}),"\n",(0,s.jsx)(n.p,{children:"Large language models have revolutionized conversational AI by providing unprecedented capabilities for natural language understanding and generation. Models in the GPT family, developed by OpenAI and others, can engage in coherent, contextually appropriate dialogue across a wide range of topics. Integrating these models into robot systems enables natural conversation while maintaining appropriate safety constraints and task focus."}),"\n",(0,s.jsx)(n.p,{children:"The integration of GPT models into robot dialogue systems requires careful architecture design. Direct connection to cloud-based language models introduces latency that can make conversation feel unnatural. Network dependencies also raise reliability concerns for safety-critical applications. Successful integration strategies typically involve a hybrid approach where lightweight local models handle routine interactions while cloud models provide specialized knowledge when needed."}),"\n",(0,s.jsx)(n.p,{children:"The following example demonstrates a dialogue manager that interfaces with a GPT model for response generation while maintaining robot-specific context and safety constraints:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nGPT-Powered Dialogue Manager for Humanoid Robots\n\nThis module provides a complete dialogue management system that integrates\nlarge language models with robot-specific context and constraints.\n"""\n\nimport asyncio\nimport json\nimport os\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum, auto\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom abc import ABC, abstractmethod\n\n\nclass DialogueState(Enum):\n    """Enumeration of possible dialogue states."""\n    IDLE = auto()\n    LISTENING = auto()\n    PROCESSING = auto()\n    SPEAKING = auto()\n    WAITING_FOR_ACTION = auto()\n    ERROR = auto()\n\n\nclass Intent(Enum):\n    """Robot-specific dialogue intents."""\n    GREETING = "greeting"\n    FAREWELL = "farewell"\n    QUESTION = "question"\n    COMMAND = "command"\n    REQUEST = "request"\n    INFORMATION = "information"\n    CONFIRMATION = "confirmation"\n    APOLOGY = "apology"\n    UNKNOWN = "unknown"\n\n\n@dataclass\nclass Entity:\n    """Represents an extracted entity from user input."""\n    type: str\n    value: str\n    confidence: float\n    position: Tuple[int, int]  # Start and end character positions\n\n\n@dataclass\nclass DialogueAct:\n    """Represents a complete dialogue act with intent and entities."""\n    intent: Intent\n    entities: List[Entity] = field(default_factory=list)\n    confidence: float = 1.0\n    raw_text: str = ""\n    timestamp: datetime = field(default_factory=datetime.now)\n\n\n@dataclass\nclass ConversationContext:\n    """Maintains context across a conversation session."""\n    session_id: str\n    user_id: Optional[str]\n    robot_name: str = "Fubuni"\n    conversation_history: List[Dict[str, Any]] = field(default_factory=list)\n    known_entities: Dict[str, Any] = field(default_factory=dict)\n    current_topic: Optional[str] = None\n    last_action_result: Optional[str] = None\n    user_preferences: Dict[str, Any] = field(default_factory=dict)\n\n    def add_exchange(self, user_message: str, robot_response: str) -> None:\n        """Add a user-robot exchange to conversation history."""\n        self.conversation_history.append({\n            "timestamp": datetime.now().isoformat(),\n            "user": user_message,\n            "robot": robot_response\n        })\n        # Keep history manageable for context windows\n        max_history = 20\n        if len(self.conversation_history) > max_history:\n            self.conversation_history = self.conversation_history[-max_history:]\n\n\nclass GPTClient:\n    """\n    Client for interfacing with GPT-based language models.\n\n    Handles prompt construction, API communication, and response parsing\n    while maintaining safety constraints and context management.\n    """\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        model: str = "gpt-4",\n        max_tokens: int = 500,\n        temperature: float = 0.7,\n        system_prompt: Optional[str] = None\n    ):\n        self.api_key = api_key or os.getenv("OPENAI_API_KEY")\n        self.model = model\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.system_prompt = system_prompt or self._get_default_system_prompt()\n        self._client = None\n\n    def _get_default_system_prompt(self) -> str:\n        """Generate the default system prompt for robot dialogue."""\n        return f"""You are Fubuni, a friendly humanoid robot assistant. You are helpful,\npolite, and concise in your responses. You assist users with tasks in a home\nor office environment. You can:\n- Answer questions about your capabilities\n- Help with object location and retrieval\n- Provide information about the environment\n- Execute simple commands like navigation and manipulation\n- Engage in casual conversation\n\nKeep responses brief (1-3 sentences) unless more detail is requested.\nAlways maintain a helpful, positive tone. If you cannot help with a request,\nexplain why clearly and suggest alternatives."""\n\n    def _build_messages(\n        self,\n        context: ConversationContext,\n        user_input: str\n    ) -> List[Dict[str, str]]:\n        """Build the message list for the API request."""\n        messages = [{"role": "system", "content": self.system_prompt}]\n\n        # Add conversation history for context\n        for exchange in context.conversation_history[-5:]:  # Last 5 exchanges\n            messages.append({"role": "user", "content": exchange["user"]})\n            messages.append({"role": "assistant", "content": exchange["robot"]})\n\n        # Add current user input\n        messages.append({"role": "user", "content": user_input})\n\n        return messages\n\n    async def generate_response(\n        self,\n        context: ConversationContext,\n        user_input: str\n    ) -> str:\n        """\n        Generate a response using the GPT model.\n\n        Args:\n            context: Current conversation context\n            user_input: The user\'s input text\n\n        Returns:\n            Generated response text\n        """\n        messages = self._build_messages(context, user_input)\n\n        # Simulated response for demonstration\n        # In production, this would call the actual API\n        response = await self._call_api(messages)\n\n        return response\n\n    async def _call_api(self, messages: List[Dict[str, str]]) -> str:\n        """Make the API call to the language model."""\n        # Placeholder for actual API integration\n        # Would use openai.AsyncOpenAI client in production\n        await asyncio.sleep(0.1)  # Simulate API latency\n\n        user_message = messages[-1]["content"] if messages else ""\n\n        # Simple response generation for demonstration\n        response_templates = {\n            "greeting": ["Hello! How can I help you today?", "Hi there! What can I do for you?"],\n            "command": ["I understand. Let me help with that.", "Got it! I\'ll take care of that."],\n            "question": ["That\'s a great question. Let me think about that.", "I\'d be happy to answer that."],\n            "farewell": ["Goodbye! Feel free to ask if you need anything else.", "See you later!"]\n        }\n\n        # Simple intent-based response (production would use actual NLU)\n        if any(g in user_message.lower() for g in ["hello", "hi", "hey"]):\n            return "Hello! I\'m Fubuni, your humanoid robot assistant. How can I help you today?"\n        elif any(q in user_message.lower() for q in ["what", "how", "why", "where", "when"]):\n            return "That\'s a great question. I\'m here to help you with tasks around the home or office. What would you like to know?"\n        elif any(c in user_message.lower() for c in ["bring", "get", "find", "fetch"]):\n            return "I understand you need help with something. Could you tell me what object you\'re looking for?"\n        elif any(f in user_message.lower() for f in ["bye", "goodbye", "see you"]):\n            return "Goodbye! It was nice chatting with you. Feel free to return if you need anything!"\n        else:\n            return "I\'m here to help! What would you like me to do for you?"\n\n\nclass NaturalLanguageUnderstanding:\n    """\n    NLU module for intent classification and entity extraction.\n\n    Extracts user intent and relevant entities from transcribed speech,\n    enabling appropriate dialogue management and robot action selection.\n    """\n\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()\n        self.entity_extractor = EntityExtractor()\n\n    def parse(self, text: str) -> DialogueAct:\n        """\n        Parse user input into a structured dialogue act.\n\n        Args:\n            text: The transcribed user input\n\n        Returns:\n            DialogueAct with intent and entities\n        """\n        intent = self.intent_classifier.classify(text)\n        entities = self.entity_extractor.extract(text)\n\n        return DialogueAct(\n            intent=intent,\n            entities=entities,\n            raw_text=text,\n            confidence=self._calculate_confidence(intent, entities)\n        )\n\n    def _calculate_confidence(\n        self,\n        intent: Intent,\n        entities: List[Entity]\n    ) -> float:\n        """Calculate overall confidence score for the parse."""\n        base_confidence = 0.9 if intent != Intent.UNKNOWN else 0.5\n\n        # Boost confidence if entities were found for relevant intents\n        entity_boost = min(0.1 * len(entities), 0.1)\n\n        return min(base_confidence + entity_boost, 1.0)\n\n\nclass IntentClassifier:\n    """\n    Simple intent classifier based on keyword matching.\n\n    Production systems would use trained classifiers or LLM-based\n    classification for more robust intent recognition.\n    """\n\n    INTENT_PATTERNS = {\n        Intent.GREETING: ["hello", "hi", "hey", "good morning", "good afternoon"],\n        Intent.FAREWELL: ["bye", "goodbye", "see you", "later", "farewell"],\n        Intent.COMMAND: ["move", "go", "walk", "stop", "turn", "grab", "pick"],\n        Intent.REQUEST: ["bring", "get", "find", "fetch", "bring me", "please"],\n        Intent.QUESTION: ["what", "how", "why", "where", "when", "who", "which"],\n        Intent.INFORMATION: ["tell me", "explain", "describe", "information"],\n        Intent.CONFIRMATION: ["yes", "confirm", "correct", "that\'s right"],\n        Intent.APOLOGY: ["sorry", "apologize", "excuse me", "pardon"]\n    }\n\n    def classify(self, text: str) -> Intent:\n        """Classify the intent of user input."""\n        text_lower = text.lower()\n\n        for intent, patterns in self.INTENT_PATTERNS.items():\n            if any(pattern in text_lower for pattern in patterns):\n                return intent\n\n        return Intent.UNKNOWN\n\n\nclass EntityExtractor:\n    """\n    Entity extractor for common entity types in robot interactions.\n\n    Identifies objects, locations, people, and quantities from natural\n    language input for action execution.\n    """\n\n    ENTITY_TYPES = {\n        "object": ["cup", "bottle", "book", "phone", "keys", "bag", "box", "pen"],\n        "location": ["kitchen", "living room", "bedroom", "office", "bathroom", "table", "desk"],\n        "person": ["me", "you", "him", "her", "them", "my wife", "my husband"],\n        "quantity": ["one", "two", "three", "first", "second", "a", "the"]\n    }\n\n    def extract(self, text: str) -> List[Entity]:\n        """Extract entities from user input."""\n        entities = []\n        text_lower = text.lower()\n\n        for entity_type, keywords in self.ENTITY_TYPES.items():\n            for keyword in keywords:\n                if keyword in text_lower:\n                    position = text_lower.find(keyword)\n                    entities.append(Entity(\n                        type=entity_type,\n                        value=keyword,\n                        confidence=0.85,\n                        position=(position, position + len(keyword))\n                    ))\n\n        return entities\n\n\nclass DialogueManager:\n    """\n    Main dialogue manager coordinating all conversational AI components.\n\n    Manages the dialogue state machine, integrates with the GPT model,\n    and coordinates with robot action systems for task execution.\n    """\n\n    def __init__(\n        self,\n        gpt_client: GPTClient,\n        nlu: NaturalLanguageUnderstanding,\n        robot_name: str = "Fubuni"\n    ):\n        self.gpt_client = gpt_client\n        self.nlu = nlu\n        self.robot_name = robot_name\n        self.state = DialogueState.IDLE\n        self.context: Optional[ConversationContext] = None\n\n    async def start_conversation(\n        self,\n        session_id: str,\n        user_id: Optional[str] = None\n    ) -> str:\n        """Start a new conversation session."""\n        self.context = ConversationContext(\n            session_id=session_id,\n            user_id=user_id,\n            robot_name=self.robot_name\n        )\n        self.state = DialogueState.LISTENING\n\n        return f"Hello! I\'m {self.robot_name}, your humanoid robot assistant. How can I help you today?"\n\n    async def process_input(self, user_input: str) -> str:\n        """\n        Process user input and generate an appropriate response.\n\n        This is the main entry point for the dialogue system, handling\n        the complete pipeline from input to response.\n        """\n        if self.context is None:\n            raise RuntimeError("Conversation not started. Call start_conversation first.")\n\n        self.state = DialogueState.PROCESSING\n\n        try:\n            # Step 1: Natural Language Understanding\n            dialogue_act = self.nlu.parse(user_input)\n\n            # Step 2: Generate response using GPT\n            response = await self.gpt_client.generate_response(\n                self.context,\n                user_input\n            )\n\n            # Step 3: Update conversation context\n            self.context.add_exchange(user_input, response)\n            self._update_context_from_act(dialogue_act)\n\n            self.state = DialogueState.SPEAKING\n\n            return response\n\n        except Exception as e:\n            self.state = DialogueState.ERROR\n            return f"I\'m sorry, I didn\'t understand that. Could you please try again?"\n\n    def _update_context_from_act(self, act: DialogueAct) -> None:\n        """Update conversation context based on parsed dialogue act."""\n        # Update current topic based on intent\n        if act.intent == Intent.COMMAND or act.intent == Intent.REQUEST:\n            # Extract object entities for task context\n            for entity in act.entities:\n                if entity.type == "object":\n                    self.context.known_entities["target_object"] = entity.value\n                elif entity.type == "location":\n                    self.context.known_entities["target_location"] = entity.value\n\n\n# Example usage and demonstration\nasync def main():\n    """Demonstrate the dialogue system with example interactions."""\n\n    # Initialize components\n    gpt_client = GPTClient()\n    nlu = NaturalLanguageUnderstanding()\n    dialogue_manager = DialogueManager(gpt_client, nlu)\n\n    # Start conversation\n    print("=== Fubuni Dialogue System Demo ===\\n")\n\n    welcome = await dialogue_manager.start_conversation(\n        session_id="demo-session-001",\n        user_id="guest"\n    )\n    print(f"Robot: {welcome}\\n")\n\n    # Example user inputs\n    user_inputs = [\n        "Hello Fubuni, can you bring me a cup of water?",\n        "It\'s on the kitchen table.",\n        "Thank you so much!",\n        "Goodbye for now!"\n    ]\n\n    for user_input in user_inputs:\n        print(f"User: {user_input}")\n        response = await dialogue_manager.process_input(user_input)\n        print(f"Robot: {response}\\n")\n\n        # Check current state\n        print(f"[Dialogue State: {dialogue_manager.state.name}]")\n        print(f"[Context: {len(dialogue_manager.context.conversation_history)} exchanges]\\n")\n\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,s.jsx)(n.p,{children:"This example demonstrates the key architecture patterns for GPT integration in robot dialogue systems. The system maintains conversation context, parses user intent, generates appropriate responses, and can be extended to execute physical actions based on user commands."}),"\n",(0,s.jsx)(n.h2,{id:"63-speech-recognition-and-text-to-speech",children:"6.3 Speech Recognition and Text-to-Speech"}),"\n",(0,s.jsx)(n.p,{children:"Speech recognition and speech synthesis form the voice interface layer of conversational robotics. These technologies enable robots to receive verbal commands and provide verbal feedback, creating a natural two-way communication channel. For humanoid robots operating in human environments, voice interaction provides accessibility advantages and supports the social expectations that come with humanoid embodiment."}),"\n",(0,s.jsx)(n.h3,{id:"automatic-speech-recognition",children:"Automatic Speech Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Modern automatic speech recognition systems use deep neural networks to convert audio waveforms into text transcripts. The key architectural components include acoustic models that map audio features to phonemes, language models that provide probabilistic context for word sequences, and pronunciation models that connect words to their phonetic representations. End-to-end models like Whisper from OpenAI have simplified this architecture by training single neural networks that directly map audio to text."}),"\n",(0,s.jsx)(n.p,{children:"For robot applications, speech recognition must handle several practical challenges. Background noise from ventilation, other people, or robot motors can degrade recognition accuracy. Far-field microphone configurations designed for room-scale audio capture introduce reverberation that distorts the acoustic signal. Multiple simultaneous speakers require source separation or speaker diarization to attribute speech correctly."}),"\n",(0,s.jsx)(n.p,{children:"The following example implements a speech recognition node for ROS 2 that integrates with common ASR services:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSpeech Recognition Module for Humanoid Robots\n\nProvides real-time speech recognition integration with ROS 2,\nsupporting both local and cloud-based ASR services.\n"""\n\nimport asyncio\nimport audioop\nimport os\nimport threading\nimport wave\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, List, Optional, Tuple\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Float32\nfrom sensor_msgs.msg import AudioData\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\n\n\n@dataclass\nclass SpeechRecognitionResult:\n    """Result from speech recognition processing."""\n    transcript: str\n    confidence: float\n    is_final: bool\n    timestamp: float\n    audio_energy: float\n\n\nclass AudioPreprocessor:\n    """\n    Audio preprocessing pipeline for speech recognition.\n\n    Handles audio normalization, noise reduction, and feature extraction\n    to improve recognition accuracy in challenging acoustic environments.\n    """\n\n    def __init__(\n        self,\n        sample_rate: int = 16000,\n        chunk_size: int = 1024,\n        noise_reduction: bool = True,\n        normalize_audio: bool = True\n    ):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.noise_reduction = noise_reduction\n        self.normalize_audio = normalize_audio\n        self.audio_buffer = deque(maxlen=10)  # Keep last 10 chunks for VAD\n        self.energy_threshold = 500.0  # Voice activity detection threshold\n        self.adaptive_threshold = True\n\n    def calculate_energy(self, audio_data: bytes) -> float:\n        """Calculate root mean square energy of audio chunk."""\n        try:\n            rms = audioop.rms(audio_data, 2)  # 2 bytes per sample for 16-bit audio\n            return float(rms)\n        except Exception:\n            return 0.0\n\n    def normalize(self, audio_data: bytes) -> bytes:\n        """Normalize audio to consistent volume level."""\n        try:\n            max_amplitude = max(audioop.max(audio_data, 2), 1)\n            scale_factor = 32767.0 / max_amplitude\n            normalized = audioop.mul(audio_data, 2, scale_factor)\n            return normalized\n        except Exception:\n            return audio_data\n\n    def apply_noise_reduction(self, audio_data: bytes) -> bytes:\n        """\n        Simple noise reduction using spectral subtraction.\n\n        In production, more sophisticated algorithms like Wiener filtering\n        or deep learning-based noise suppression would be used.\n        """\n        # Placeholder for noise reduction\n        # Production implementations would use libraries like noisereduce\n        return audio_data\n\n    def preprocess(self, audio_data: bytes) -> Tuple[bytes, float]:\n        """\n        Preprocess audio chunk for recognition.\n\n        Returns:\n            Tuple of (processed_audio, energy_level)\n        """\n        energy = self.calculate_energy(audio_data)\n\n        # Adaptive threshold adjustment\n        if self.adaptive_threshold:\n            self.energy_threshold = 0.9 * self.energy_threshold + 0.1 * energy\n\n        # Apply preprocessing steps\n        if self.normalize_audio:\n            audio_data = self.normalize(audio_data)\n\n        if self.noise_reduction:\n            audio_data = self.apply_noise_reduction(audio_data)\n\n        return audio_data, energy\n\n    def is_speech(self, audio_data: bytes) -> bool:\n        """Voice activity detection using energy threshold."""\n        energy = self.calculate_energy(audio_data)\n        return energy > self.energy_threshold\n\n\nclass ASRProvider(ABC):\n    """Abstract base class for ASR service providers."""\n\n    @abstractmethod\n    async def recognize(self, audio_data: bytes) -> SpeechRecognitionResult:\n        """Perform speech recognition on audio data."""\n        pass\n\n\nclass WhisperASR(ASRProvider):\n    """\n    Whisper-based speech recognition using OpenAI\'s Whisper model.\n\n    Supports both local (Whisper.cpp) and cloud (OpenAI API) deployments.\n    """\n\n    def __init__(\n        self,\n        model: str = "base",\n        language: str = "en",\n        use_cloud: bool = False,\n        api_key: Optional[str] = None\n    ):\n        self.model = model\n        self.language = language\n        self.use_cloud = use_cloud\n        self.api_key = api_key or os.getenv("OPENAI_API_KEY")\n\n    async def recognize(self, audio_data: bytes) -> SpeechRecognitionResult:\n        """\n        Perform speech recognition using Whisper.\n\n        This is a simplified implementation. Production code would:\n        1. Save audio to temporary file (WAV format)\n        2. Call whisper.cpp via subprocess or OpenAI API\n        3. Parse and return results\n        """\n        import time\n        timestamp = time.time()\n\n        # Placeholder implementation\n        # In production, this would interface with whisper.cpp or OpenAI API\n        await asyncio.sleep(0.05)  # Simulate processing time\n\n        return SpeechRecognitionResult(\n            transcript="",  # Would contain actual transcription\n            confidence=0.0,\n            is_final=False,\n            timestamp=timestamp,\n            audio_energy=0.0\n        )\n\n\nclass ROS2SpeechRecognizer(Node):\n    """\n    ROS 2 node for speech recognition integration.\n\n    Subscribes to audio topics, processes audio through the ASR pipeline,\n    and publishes recognition results.\n    """\n\n    def __init__(\n        self,\n        asr_provider: ASRProvider,\n        audio_preprocessor: AudioPreprocessor,\n        node_name: str = "speech_recognizer"\n    ):\n        super().__init__(node_name)\n\n        self.asr_provider = asr_provider\n        self.preprocessor = audio_preprocessor\n\n        # Configuration\n        self.declare_parameter("confidence_threshold", 0.6)\n        self.declare_parameter("partial_results_enabled", True)\n        self.declare_parameter("silence_duration", 0.8)  # Seconds before finalizing\n\n        self.confidence_threshold = self.get_parameter("confidence_threshold").value\n        self.partial_enabled = self.get_parameter("partial_results_enabled").value\n\n        # State\n        self.audio_buffer: List[bytes] = []\n        self.is_speaking = False\n        self.silence_timer: Optional[float] = None\n\n        # QoS profile for audio topics\n        self.audio_qos = QoSProfile(\n            reliability=ReliabilityPolicy.BEST_EFFORT,\n            depth=10\n        )\n\n        # Publishers\n        self.transcript_pub = self.create_publisher(\n            String,\n            "speech/transcript",\n            10\n        )\n\n        self.confidence_pub = self.create_publisher(\n            Float32,\n            "speech/confidence",\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            "speech/status",\n            10\n        )\n\n        # Subscriber\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            "audio/raw",\n            self.audio_callback,\n            self.audio_qos\n        )\n\n        self.get_logger().info("Speech Recognizer initialized")\n\n    def audio_callback(self, msg: AudioData) -> None:\n        """Process incoming audio data."""\n        audio_data = bytes(msg.data)\n        processed_audio, energy = self.preprocessor.preprocess(audio_data)\n\n        # Voice activity detection\n        is_speech = self.preprocessor.is_speech(processed_audio)\n\n        if is_speech:\n            self.audio_buffer.append(processed_audio)\n            self.is_speaking = True\n            self.silence_timer = None\n        else:\n            if self.is_speaking and self.audio_buffer:\n                # Silence detected after speech - process utterance\n                if self.silence_timer is None:\n                    self.silence_timer = self.get_clock().now().nanoseconds / 1e9\n                else:\n                    silence_duration = (\n                        self.get_clock().now().nanoseconds / 1e9 - self.silence_timer\n                    )\n                    if silence_duration > self.get_parameter("silence_duration").value:\n                        self._process_utterance()\n\n    def _process_utterance(self) -> None:\n        """Process accumulated audio buffer as complete utterance."""\n        if not self.audio_buffer:\n            return\n\n        # Combine audio chunks\n        audio_data = b"".join(self.audio_buffer)\n        self.audio_buffer = []\n        self.is_speaking = False\n\n        # Perform recognition\n        result = asyncio.run_coroutine_threadsafe(\n            self.asr_provider.recognize(audio_data),\n            asyncio.new_event_loop()\n        ).result()\n\n        # Publish results\n        if result.confidence >= self.confidence_threshold:\n            transcript_msg = String()\n            transcript_msg.data = result.transcript\n            self.transcript_pub.publish(transcript_msg)\n\n            confidence_msg = Float32()\n            confidence_msg.data = float(result.confidence)\n            self.confidence_pub.publish(confidence_msg)\n\n            self.get_logger().info(\n                f"Recognized: \'{result.transcript}\' (confidence: {result.confidence:.2f})"\n            )\n\n        status_msg = String()\n        status_msg.data = "listening"\n        self.status_pub.publish(status_msg)\n\n\nclass TextToSpeechEngine:\n    """\n    Text-to-speech synthesis engine for robot speech output.\n\n    Supports multiple TTS providers with voice customization options\n    for different interaction contexts.\n    """\n\n    def __init__(self, default_voice: str = "en-US-Female"):\n        self.default_voice = default_voice\n        self.voice_cache: Dict[str, Any] = {}\n\n    async def synthesize(\n        self,\n        text: str,\n        voice: Optional[str] = None,\n        speed: float = 1.0,\n        pitch: float = 1.0\n    ) -> bytes:\n        """\n        Synthesize speech from text.\n\n        Args:\n            text: Text to synthesize\n            voice: Voice identifier (language, speaker, etc.)\n            speed: Speech rate multiplier (0.5 to 2.0)\n            pitch: Pitch multiplier (0.5 to 2.0)\n\n        Returns:\n            Audio data in WAV format\n        """\n        # Placeholder implementation\n        # Production would use gTTS, pyttsx3, or cloud TTS services\n        await asyncio.sleep(0.1)\n\n        # Return empty bytes placeholder\n        return b""\n\n    def preprocess_text(self, text: str) -> str:\n        """Preprocess text for better speech synthesis."""\n        # Expand abbreviations\n        replacements = {\n            "Dr.": "Doctor",\n            "Mr.": "Mister",\n            "Mrs.": "Missus",\n            "etc.": "et cetera",\n            "i.e.": "that is",\n            "e.g.": "for example"\n        }\n\n        processed = text\n        for abbr, full in replacements.items():\n            processed = processed.replace(abbr, full)\n\n        # Add pauses for punctuation\n        processed = processed.replace(".", ". ")\n        processed = processed.replace("?", "? ")\n        processed = processed.replace("!", "! ")\n\n        return processed.strip()\n\n\nclass ROS2SpeechSynthesizer(Node):\n    """\n    ROS 2 node for text-to-speech synthesis.\n\n    Subscribes to text topics and publishes synthesized audio.\n    """\n\n    def __init__(self, tts_engine: TextToSpeechEngine):\n        super().__init__("speech_synthesizer")\n\n        self.tts_engine = tts_engine\n\n        # QoS profile\n        self.text_qos = QoSProfile(depth=10)\n\n        # Publisher for audio\n        self.audio_pub = self.create_publisher(\n            AudioData,\n            "speech/audio",\n            10\n        )\n\n        # Subscriber for text input\n        self.text_sub = self.create_subscription(\n            String,\n            "speech/text",\n            self.synthesize_callback,\n            self.text_qos\n        )\n\n        self.get_logger().info("Speech Synthesizer initialized")\n\n    def synthesize_callback(self, msg: String) -> None:\n        """Process text and synthesize speech."""\n        text = msg.data\n\n        # Preprocess text\n        text = self.tts_engine.preprocess_text(text)\n\n        # Synthesize\n        audio_data = asyncio.run(\n            self.tts_engine.synthesize(text)\n        )\n\n        if audio_data:\n            # Publish audio\n            audio_msg = AudioData()\n            audio_msg.data = audio_data\n            self.audio_pub.publish(audio_msg)\n\n            self.get_logger().info(f"Synthesized speech for: \'{text[:50]}...\'")\n\n\n# Demonstration of the speech recognition pipeline\nasync def demonstrate_speech_pipeline():\n    """Demonstrate the speech recognition and synthesis pipeline."""\n\n    print("=== Speech Recognition Pipeline Demo ===\\n")\n\n    # Initialize components\n    preprocessor = AudioPreprocessor(\n        sample_rate=16000,\n        chunk_size=1024,\n        noise_reduction=True,\n        normalize_audio=True\n    )\n\n    asr_provider = WhisperASR(model="base", language="en")\n\n    print("Audio Preprocessor initialized:")\n    print(f"  - Sample rate: {preprocessor.sample_rate} Hz")\n    print(f"  - Chunk size: {preprocessor.chunk_size} samples")\n    print(f"  - Noise reduction: {preprocessor.noise_reduction}")\n    print(f"  - Adaptive VAD threshold: {preprocessor.adaptive_threshold}\\n")\n\n    print("Whisper ASR initialized:")\n    print(f"  - Model: {asr_provider.model}")\n    print(f"  - Language: {asr_provider.language}")\n    print(f"  - Cloud mode: {asr_provider.use_cloud}\\n")\n\n    # TTS demonstration\n    tts = TextToSpeechEngine(default_voice="en-US-Female")\n    print("Text-to-Speech engine initialized:")\n    print(f"  - Default voice: {tts.default_voice}\\n")\n\n    print("Pipeline components ready for ROS 2 integration.")\n    print("Would subscribe to audio topics and publish recognized text.")\n\n\nif __name__ == "__main__":\n    asyncio.run(demonstrate_speech_pipeline())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"64-natural-language-understanding",children:"6.4 Natural Language Understanding"}),"\n",(0,s.jsx)(n.p,{children:"Natural language understanding transforms raw transcribed text into structured representations that robot systems can act upon. The core tasks include intent classification, entity extraction, reference resolution, and dialogue act interpretation. For robot applications, NLU must be robust to the imprecise and incomplete language that characterizes natural human communication."}),"\n",(0,s.jsx)(n.p,{children:"Intent classification assigns user utterances to predefined categories that map to robot actions or dialogue behaviors. Common intents for humanoid robots include greetings, commands, questions, requests for information, and conversation management acts like asking for clarification or confirming understanding. Multi-label classification allows utterances to carry multiple intents simultaneously, such as a greeting combined with a request."}),"\n",(0,s.jsx)(n.p,{children:"Entity extraction identifies specific pieces of information within user utterances. For robot applications, the most important entity types are objects the robot might manipulate, locations in the environment, people who interact with the robot, and temporal expressions that specify when actions should occur. Entity extraction must handle both explicit mentions and references to previously discussed entities."}),"\n",(0,s.jsx)(n.p,{children:'Reference resolution connects pronouns, demonstratives, and other anaphoric expressions to their referent entities. When a user says "Bring me the cup and put it on the table," the pronoun "it" refers to the cup, not the table. Reference resolution requires tracking discourse entities across conversation turns and understanding the physical context in which references occur.'}),"\n",(0,s.jsx)(n.p,{children:"The following example implements a comprehensive NLU pipeline with intent classification, entity extraction, and reference resolution:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nNatural Language Understanding for Humanoid Robots\n\nProvides comprehensive NLU capabilities including intent classification,\nentity extraction, reference resolution, and semantic parsing.\n"""\n\nimport re\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\n\nclass Intent(Enum):\n    """Comprehensive set of robot interaction intents."""\n    # Core interaction intents\n    GREETING = "greeting"\n    FAREWELL = "farewell"\n    THANK = "thank"\n    APOLOGIZE = "apologize"\n\n    # Task-oriented intents\n    COMMAND = "command"  # Direct robot command\n    REQUEST = "request"  # Polite request\n    QUESTION = "question"\n    SUGGESTION = "suggestion"\n\n    # Information intents\n    INFORM = "inform"\n    CONFIRM = "confirm"\n    DENY = "deny"\n\n    # Dialogue management\n    CLARIFY = "clarify"  # Ask for clarification\n    REPEAT = "repeat"\n    CHANGE_TOPIC = "change_topic"\n\n    # Social\n    COMPLIMENT = "compliment"\n    COMPLAINT = "complaint"\n\n    # Error/unknown\n    UNKNOWN = "unknown"\n    NON_SPEECH = "non_speech"\n\n\n@dataclass\nclass Entity:\n    """Extracted entity with type, value, and metadata."""\n    type: str\n    value: str\n    raw_text: str\n    position: Tuple[int, int]\n    confidence: float = 1.0\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n\n@dataclass\nclass Reference:\n    """Anaphoric reference to a previously mentioned entity."""\n    text: str\n    position: Tuple[int, int]\n    referent_type: str\n    antecedent: Optional[str] = None\n    confidence: float = 1.0\n\n\n@dataclass\nclass NLUParsing:\n    """Complete NLU parsing result."""\n    text: str\n    intent: Intent\n    confidence: float\n    entities: List[Entity]\n    references: List[Reference]\n    sentiment: float  # -1.0 to 1.0\n    urgency: float  # 0.0 to 1.0\n    politeness: float  # 0.0 to 1.0\n    timestamp: datetime = field(default_factory=datetime.now)\n\n\nclass EntityExtractor:\n    """\n    Multi-type entity extractor for robot interaction contexts.\n\n    Extracts objects, locations, people, quantities, times, and actions\n    from natural language input using pattern matching and classification.\n    """\n\n    # Entity patterns\n    ENTITY_PATTERNS = {\n        "object": [\n            r"\\b(cup|mug|glass|bottle|plate|bowl)\\b",\n            r"\\b(book|phone|laptop|tablet|remote)\\b",\n            r"\\b(keys|wallet|sunglasses|hat)\\b",\n            r"\\b(food|drink|snack|meal)\\b",\n            r"\\b(newspaper|magazine|letter)\\b"\n        ],\n        "location": [\n            r"\\b(kitchen|living room|bedroom|bathroom|office)\\b",\n            r"\\b(table|desk|shelf|counter|cabinet)\\b",\n            r"\\b(floor|ceiling|wall|door|window)\\b",\n            r"\\bnear\\s+\\w+|next to\\s+\\w+|on top of\\s+\\w+"\n        ],\n        "person": [\n            r"\\b(me|you|him|her|them)\\b",\n            r"\\b(my\\s+\\w+|your\\s+\\w+|his\\s+\\w+|her\\s+\\w+)\\b",\n            r"\\b(mom|dad|friend|colleague)\\b"\n        ],\n        "quantity": [\n            r"\\b(one|two|three|four|five)\\b",\n            r"\\b(a|an|the)\\b",\n            r"\\b(some|few|several|many)\\b"\n        ],\n        "time": [\n            r"\\b(now|immediately|right away)\\b",\n            r"\\b(later|today|tonight|tomorrow)\\b",\n            r"\\b(in\\s+\\w+\\s+(minutes?|hours?|seconds?))\\b"\n        ],\n        "action": [\n            r"\\b(bring|get|find|fetch|grab)\\b",\n            r"\\b(pick\\s+up|put\\s+down|set\\s+down)\\b",\n            r"\\b(move|go|walk|come|follow)\\b",\n            r"\\b(open|close|turn|switch)\\b"\n        ]\n    }\n\n    def __init__(self):\n        self.entity_values = self._build_entity_values()\n        self.mention_history: List[Dict[str, Any]] = []\n\n    def _build_entity_values(self) -> Dict[str, Set[str]]:\n        """Build sets of known entity values for fuzzy matching."""\n        return {\n            "object": {\n                "cup", "mug", "glass", "bottle", "plate", "bowl",\n                "book", "phone", "laptop", "tablet", "remote",\n                "keys", "wallet", "pen", "notebook", "bag"\n            },\n            "location": {\n                "kitchen", "living room", "bedroom", "bathroom", "office",\n                "table", "desk", "shelf", "counter", "cabinet"\n            },\n            "person": {\n                "me", "you", "him", "her", "them"\n            }\n        }\n\n    def extract(self, text: str) -> List[Entity]:\n        """Extract all entities from text."""\n        entities = []\n        text_lower = text.lower()\n\n        for entity_type, patterns in self.ENTITY_PATTERNS.items():\n            for pattern in patterns:\n                for match in re.finditer(pattern, text_lower):\n                    value = match.group().strip()\n\n                    # Clean up quantified expressions\n                    if entity_type == "quantity" and value in ["a", "an"]:\n                        continue\n\n                    entity = Entity(\n                        type=entity_type,\n                        value=self._normalize_entity_value(entity_type, value),\n                        raw_text=match.group(),\n                        position=(match.start(), match.end()),\n                        confidence=self._calculate_confidence(entity_type, value)\n                    )\n                    entities.append(entity)\n\n        # Sort by position\n        entities.sort(key=lambda e: e.position[0])\n\n        return entities\n\n    def _normalize_entity_value(self, entity_type: str, value: str) -> str:\n        """Normalize extracted entity values."""\n        # Remove articles and quantifiers\n        stopwords = {"a", "an", "the", "some", "few", "several", "many"}\n        words = value.split()\n        filtered = [w for w in words if w not in stopwords]\n        return " ".join(filtered) if filtered else value\n\n    def _calculate_confidence(self, entity_type: str, value: str) -> float:\n        """Calculate extraction confidence based on pattern match quality."""\n        # Direct dictionary matches have higher confidence\n        if value.lower() in self.entity_values.get(entity_type, set()):\n            return 0.95\n        return 0.75\n\n\nclass IntentClassifier:\n    """\n    Intent classifier using pattern matching and semantic features.\n\n    Production systems would use trained classifiers, but pattern-based\n    classification provides interpretable and modifiable rules.\n    """\n\n    # Intent patterns with associated confidence weights\n    INTENT_PATTERNS = {\n        Intent.GREETING: [\n            (r"\\b(hi|hello|hey|greetings|good morning|good afternoon|good evening)\\b", 0.9),\n            (r"\\bhow are you\\b", 0.7),\n        ],\n        Intent.FAREWELL: [\n            (r"\\b(bye|goodbye|see you|farewell|later)\\b", 0.95),\n            (r"\\b(thanks|thank you)\\s+(for|bye|now)\\b", 0.6),\n        ],\n        Intent.THANK: [\n            (r"\\b(thank|thanks|appreciate)\\b", 0.9),\n            (r"\\bthat(\'s| is) (very|really|so) (helpful|nice|great)\\b", 0.8),\n        ],\n        Intent.APOLOGIZE: [\n            (r"\\b(sorry|apologize|excuse me|pardon)\\b", 0.95),\n        ],\n        Intent.COMMAND: [\n            (r"\\b(go|move|walk|stop|turn|grab|pick)\\b", 0.7),\n            (r"\\bdo (something|that)\\b", 0.5),\n        ],\n        Intent.REQUEST: [\n            (r"\\b(please|could you|would you|can you)\\b", 0.85),\n            (r"\\b(i would like|i want|i need)\\b", 0.7),\n        ],\n        Intent.QUESTION: [\n            (r"\\b(what|who|where|when|why|how)\\b", 0.9),\n            (r"\\b(can you tell me|do you know)\\b", 0.75),\n        ],\n        Intent.INFORM: [\n            (r"\\b(it is|they are|there is)\\b", 0.6),\n            (r"\\b(the|this|that) (is|are)\\b", 0.5),\n        ],\n        Intent.CLARIFY: [\n            (r"\\b(what do you mean|i don(\'t| not) understand)\\b", 0.9),\n            (r"\\b(can you repeat|say that again)\\b", 0.85),\n            (r"\\b(which one|what( )?(kind|type))\\b", 0.7),\n        ],\n        Intent.CONFIRM: [\n            (r"\\b(yes|correct|right|that(\'s| is) (right|correct))\\b", 0.9),\n            (r"\\bi (agree|see|understand)\\b", 0.7),\n        ],\n        Intent.DENY: [\n            (r"\\b(no|not|correct|wrong)\\b", 0.8),\n        ],\n    }\n\n    def classify(self, text: str) -> Tuple[Intent, float]:\n        """\n        Classify the intent of user text.\n\n        Returns:\n            Tuple of (intent, confidence_score)\n        """\n        text_lower = text.lower()\n        best_intent = Intent.UNKNOWN\n        best_confidence = 0.0\n\n        for intent, patterns in self.INTENT_PATTERNS.items():\n            for pattern, base_confidence in patterns:\n                if re.search(pattern, text_lower):\n                    # Adjust confidence based on match specifics\n                    confidence = base_confidence\n\n                    # Boost if pattern matches full text\n                    if re.fullmatch(pattern, text_lower):\n                        confidence = min(confidence * 1.1, 1.0)\n\n                    if confidence > best_confidence:\n                        best_confidence = confidence\n                        best_intent = intent\n\n        return best_intent, best_confidence\n\n\nclass ReferenceResolver:\n    """\n    Anaphoric reference resolution for pronouns and descriptions.\n\n    Tracks discourse entities and resolves references across\n    conversation turns.\n    """\n\n    PRONOUN_PATTERNS = {\n        "it": ["object"],\n        "they": ["object", "person"],\n        "them": ["object", "person"],\n        "he": ["person"],\n        "she": ["person"],\n        "him": ["person"],\n        "her": ["person", "object"],\n        "this": ["object", "location"],\n        "that": ["object", "location"],\n    }\n\n    DEMONSTRATIVE_PATTERN = r"\\b(this|that|these|those)\\b\\s*(one|ones)?"\n\n    def __init__(self):\n        self.mention_history: List[Dict[str, Any]] = []\n        self.current_entities: Dict[str, Dict[str, Any]] = {}\n\n    def resolve(self, text: str, entities: List[Entity]) -> List[Reference]:\n        """\n        Resolve references in the current utterance.\n\n        Args:\n            text: Current utterance text\n            entities: Entities extracted from current utterance\n\n        Returns:\n            List of resolved references\n        """\n        references = []\n\n        # Resolve pronouns\n        for match in re.finditer(r"\\b(it|they|them|he|she|him|her|this|that)\\b", text.lower()):\n            reference_text = match.group()\n            pos = (match.start(), match.end())\n\n            # Get possible referent types\n            referent_types = self.PRONOUN_PATTERNS.get(reference_text, ["object"])\n\n            # Find best antecedent\n            antecedent = self._find_antecedent(referent_types, entities)\n\n            if antecedent:\n                references.append(Reference(\n                    text=reference_text,\n                    position=pos,\n                    referent_type=referent_types[0],\n                    antecedent=antecedent,\n                    confidence=0.85\n                ))\n\n        # Update mention history\n        for entity in entities:\n            self.mention_history.append({\n                "type": entity.type,\n                "value": entity.value,\n                "text": entity.raw_text,\n                "timestamp": datetime.now().isoformat()\n            })\n\n        # Keep only recent mentions\n        max_history = 10\n        if len(self.mention_history) > max_history:\n            self.mention_history = self.mention_history[-max_history:]\n\n        return references\n\n    def _find_antecedent(\n        self,\n        referent_types: List[str],\n        current_entities: List[Entity]\n    ) -> Optional[str]:\n        """Find the most likely antecedent for a reference."""\n        # First check current utterance for immediate reference\n        for entity in current_entities:\n            if entity.type in referent_types:\n                return entity.value\n\n        # Then check recent history\n        for mention in reversed(self.mention_history):\n            if mention["type"] in referent_types:\n                return mention["value"]\n\n        return None\n\n\nclass NaturalLanguageUnderstanding:\n    """\n    Complete NLU pipeline combining all components.\n\n    Integrates intent classification, entity extraction, and reference\n    resolution into a unified parsing interface.\n    """\n\n    def __init__(self):\n        self.intent_classifier = IntentClassifier()\n        self.entity_extractor = EntityExtractor()\n        self.reference_resolver = ReferenceResolver()\n\n    def parse(self, text: str) -> NLUParsing:\n        """\n        Complete NLU parsing of user input.\n\n        Args:\n            text: Raw user input text\n\n        Returns:\n            Complete NLUParsing with all analyzed components\n        """\n        # Extract entities\n        entities = self.entity_extractor.extract(text)\n\n        # Classify intent\n        intent, intent_confidence = self.intent_classifier.classify(text)\n\n        # Resolve references\n        references = self.reference_resolver.resolve(text, entities)\n\n        # Calculate additional features\n        sentiment = self._analyze_sentiment(text)\n        urgency = self._analyze_urgency(text)\n        politeness = self._analyze_politeness(text)\n\n        return NLUParsing(\n            text=text,\n            intent=intent,\n            confidence=intent_confidence,\n            entities=entities,\n            references=references,\n            sentiment=sentiment,\n            urgency=urgency,\n            politeness=politeness\n        )\n\n    def _analyze_sentiment(self, text: str) -> float:\n        """Analyze sentiment polarity of input (-1 to 1)."""\n        positive_words = {"good", "great", "nice", "wonderful", "excellent", "happy", "thanks"}\n        negative_words = {"bad", "terrible", "awful", "horrible", "sorry", "wrong", "hate"}\n\n        text_lower = text.lower()\n        score = 0.0\n\n        for word in positive_words:\n            if word in text_lower:\n                score += 0.2\n        for word in negative_words:\n            if word in text_lower:\n                score -= 0.2\n\n        return max(-1.0, min(1.0, score))\n\n    def _analyze_urgency(self, text: str) -> float:\n        """Analyze urgency of input (0 to 1)."""\n        urgency_indicators = {\n            "now": 0.8,\n            "immediately": 0.9,\n            "urgent": 0.7,\n            "asap": 0.85,\n            "quickly": 0.6,\n            "hurry": 0.8,\n            "fast": 0.5,\n            "right away": 0.85\n        }\n\n        text_lower = text.lower()\n        urgency = 0.1  # Base urgency\n\n        for indicator, level in urgency_indicators.items():\n            if indicator in text_lower:\n                urgency = max(urgency, level)\n\n        # Questions are typically less urgent\n        if text.strip().endswith("?"):\n            urgency *= 0.7\n\n        return urgency\n\n    def _analyze_politeness(self, text: str) -> float:\n        """Analyze politeness markers in input (0 to 1)."""\n        polite_markers = {\n            "please": 0.8,\n            "thank you": 0.9,\n            "thanks": 0.7,\n            "would you": 0.75,\n            "could you": 0.75,\n            "would appreciate": 0.85,\n            "if you could": 0.7\n        }\n\n        text_lower = text.lower()\n        politeness = 0.5  # Base politeness\n\n        for marker, level in polite_markers.items():\n            if marker in text_lower:\n                politeness = max(politeness, level)\n\n        return politeness\n\n\n# Demonstration of NLU pipeline\ndef demonstrate_nlu():\n    """Demonstrate the NLU pipeline with example inputs."""\n\n    print("=== Natural Language Understanding Demo ===\\n")\n\n    # Initialize NLU\n    nlu = NaturalLanguageUnderstanding()\n\n    # Example inputs\n    test_inputs = [\n        "Hello Fubuni, how are you today?",\n        "Can you please bring me the red cup from the kitchen?",\n        "What is on the table?",\n        "That one, not this one.",\n        "Thank you so much, you\'re really helpful!",\n        "I don\'t understand what you mean.",\n        "Stop moving and wait there."\n    ]\n\n    for user_input in test_inputs:\n        print(f"Input: \\"{user_input}\\"")\n\n        result = nlu.parse(user_input)\n\n        print(f"  Intent: {result.intent.value} (confidence: {result.confidence:.2f})")\n        print(f"  Sentiment: {result.sentiment:.2f}, Urgency: {result.urgency:.2f}, Politeness: {result.politeness:.2f}")\n\n        if result.entities:\n            entity_strs = [f"{e.type}:\'{e.value}\'" for e in result.entities]\n            print(f"  Entities: {\', \'.join(entity_strs)}")\n\n        if result.references:\n            ref_strs = [f"\'{r.text}\'->{r.antecedent}" for r in result.references]\n            print(f"  References: {\', \'.join(ref_strs)}")\n\n        print()\n\n\nif __name__ == "__main__":\n    demonstrate_nlu()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"65-multi-modal-interaction",children:"6.5 Multi-Modal Interaction"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robot interaction naturally extends beyond speech to encompass gesture, facial expression, eye gaze, and physical action. Multi-modal interaction systems integrate these channels to create more natural and expressive communication. For humanoid robots, the alignment of verbal and non-verbal cues is essential for coherent social interaction."}),"\n",(0,s.jsx)(n.p,{children:'Gesture recognition enables robots to understand and respond to pointing, beckoning, waving, and other communicative gestures. Computer vision systems track hand positions and recognize predefined gesture vocabularies. When combined with speech, gestures provide disambiguation that resolves ambiguity in spoken commands. A user pointing at an object while saying "Bring me that" provides clear reference where speech alone would be ambiguous.'}),"\n",(0,s.jsx)(n.p,{children:"Facial expression recognition and generation create emotional feedback loops in human-robot interaction. Robots that can recognize user emotions through facial expressions can adapt their responses accordingly. Similarly, robots that display appropriate facial expressions communicate their internal state and create more engaging interactions. Expressive eyes, brows, and mouth movements supplement speech with emotional content."}),"\n",(0,s.jsx)(n.p,{children:"Eye gaze direction communicates attention and intention in social interaction. Humans naturally follow the gaze of their conversation partners to understand what they are attending to. Humanoid robots can use eye tracking to understand user attention and guide their own gaze to support joint attention behaviors. Gaze aversion during thinking or listening mimics human social patterns and makes robot behavior feel more natural."}),"\n",(0,s.jsx)(n.p,{children:"The following example implements a multi-modal integration layer that coordinates speech, gesture, and facial expression:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nMulti-Modal Interaction System for Humanoid Robots\n\nIntegrates speech, gesture, facial expression, and gaze for natural\nhuman-robot communication across multiple channels.\n"""\n\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom abc import ABC, abstractmethod\nimport asyncio\n\n\nclass Modality(Enum):\n    """Enumeration of interaction modalities."""\n    SPEECH = "speech"\n    GESTURE = "gesture"\n    FACIAL_EXPRESSION = "facial"\n    EYE_GAZE = "gaze"\n    BODY_POSTURE = "posture"\n    TOUCH = "touch"\n\n\nclass GestureType(Enum):\n    """Predefined gesture types for robot execution."""\n    POINT_LEFT = "point_left"\n    POINT_RIGHT = "point_right"\n    POINT_UP = "point_up"\n    POINT_DOWN = "point_down"\n    WAVE = "wave"\n    NOD = "nod"\n    SHAKE = "shake"\n    THUMBS_UP = "thumbs_up"\n    OPEN_HAND = "open_hand"\n    CLOSED_FIST = "closed_fist"\n    SHRUG = "shrug"\n    beckon = "beckon"\n\n\nclass FacialExpression(Enum):\n    """Facial expressions for robot display."""\n    NEUTRAL = "neutral"\n    HAPPY = "happy"\n    SAD = "sad"\n    SURPRISED = "surprised"\n    FEARFUL = "fearful"\n    ANGRY = "angry"\n    DISGUSTED = "disgusted"\n    CONTEMPLATIVE = "contemplative"\n    CONFUSED = "confused"\n    SMILING = "smiling"\n    LISTENING = "listening"\n    THINKING = "thinking"\n\n\nclass GazeTarget(Enum):\n    """Gaze targets for eye movement."""\n    SPEAKER = "speaker"\n    OBJECT = "object"\n    LOCATION = "location"\n    DOWN = "down"  # Submissive/respectful\n    UP = "up"      # Thinking/reflective\n    AWAY = "away"  # Averting gaze\n    NEUTRAL = "neutral"\n\n\n@dataclass\nclass MultiModalMessage:\n    """\n    Multi-modal message with synchronized expression across channels.\n\n    Coordinates speech, gesture, facial expression, and gaze into\n    a coherent communication package.\n    """\n    text: str\n    gesture: Optional[GestureType] = None\n    facial_expression: FacialExpression = FacialExpression.NEUTRAL\n    gaze_target: GazeTarget = GazeTarget.SPEAKER\n    emotion_intensity: float = 0.5  # 0.0 to 1.0\n    duration: float = 2.0  # Seconds for gesture/expression display\n    priority: int = 0  # Higher priority interrupts lower\n    timestamp: float = field(default_factory=lambda: __import__(\'time\').time())\n\n\nclass GestureController:\n    """\n    Controller for robot gesture execution.\n\n    Manages gesture sequencing, timing, and coordination with\n    other interaction modalities.\n    """\n\n    def __init__(self):\n        self.current_gesture: Optional[GestureType] = None\n        self.gesture_queue: List[MultiModalMessage] = []\n        self.is_executing = False\n\n    async def execute_gesture(\n        self,\n        gesture: GestureType,\n        duration: float = 1.0,\n        repeat: int = 1\n    ) -> bool:\n        """\n        Execute a gesture with specified duration and repetition.\n\n        Args:\n            gesture: The gesture to execute\n            duration: Duration of each gesture cycle in seconds\n            repeat: Number of times to repeat the gesture\n\n        Returns:\n            True if gesture executed successfully\n        """\n        self.current_gesture = gesture\n        self.is_executing = True\n\n        for i in range(repeat):\n            print(f"[Gesture] Executing: {gesture.value}")\n            await asyncio.sleep(duration)\n\n            # Check for interruption\n            if not self.is_executing:\n                return False\n\n        self.is_executing = False\n        self.current_gesture = None\n        return True\n\n    def queue_gesture(self, message: MultiModalMessage) -> None:\n        """Queue a multi-modal message for gesture execution."""\n        self.gesture_queue.append(message)\n\n    async def process_queue(self) -> None:\n        """Process queued gestures in priority order."""\n        while self.gesture_queue:\n            # Sort by priority\n            self.gesture_queue.sort(key=lambda m: m.priority, reverse=True)\n\n            message = self.gesture_queue.pop(0)\n\n            if message.gesture:\n                await self.execute_gesture(\n                    message.gesture,\n                    duration=message.duration\n                )\n\n\nclass FacialExpressionController:\n    """\n    Controller for robot facial expression display.\n\n    Manages expression transitions, blends, and timing for\n    emotionally expressive robot faces.\n    """\n\n    def __init__(self):\n        self.current_expression: FacialExpression = FacialExpression.NEUTRAL\n        self.expression_intensity: float = 0.5\n        self.expression_history: List[Dict[str, Any]] = []\n\n    def set_expression(\n        self,\n        expression: FacialExpression,\n        intensity: float = 0.5,\n        duration: Optional[float] = None\n    ) -> None:\n        """\n        Set the current facial expression.\n\n        Args:\n            expression: The expression to display\n            intensity: Expression intensity (0.0 to 1.0)\n            duration: Optional duration before returning to neutral\n        """\n        self.current_expression = expression\n        self.expression_intensity = intensity\n\n        self.expression_history.append({\n            "expression": expression.value,\n            "intensity": intensity,\n            "timestamp": __import__(\'time\').time()\n        })\n\n        print(f"[Face] Expression: {expression.value} (intensity: {intensity:.2f})")\n\n        if duration:\n            asyncio.create_task(self._reset_after_duration(duration))\n\n    async def _reset_after_duration(self, duration: float) -> None:\n        """Reset to neutral after duration expires."""\n        await asyncio.sleep(duration)\n        self.set_expression(FacialExpression.NEUTRAL, 0.0)\n\n    def get_emotion_for_expression(\n        self,\n        expression: FacialExpression\n    ) -> Dict[str, float]:\n        """Get emotion scores corresponding to a facial expression."""\n        emotion_maps = {\n            FacialExpression.HAPPY: {"joy": 0.9, "sadness": 0.0, "anger": 0.0},\n            FacialExpression.SAD: {"joy": 0.0, "sadness": 0.8, "anger": 0.1},\n            FacialExpression.SURPRISED: {"joy": 0.4, "sadness": 0.0, "anger": 0.0, "fear": 0.6},\n            FacialExpression.ANGRY: {"joy": 0.0, "sadness": 0.1, "anger": 0.9},\n            FacialExpression.FEARFUL: {"joy": 0.0, "sadness": 0.3, "fear": 0.8},\n            FacialExpression.CONTEMPLATIVE: {"joy": 0.3, "sadness": 0.1, "anger": 0.0},\n            FacialExpression.CONFUSED: {"joy": 0.1, "sadness": 0.2, "fear": 0.3},\n            FacialExpression.LISTENING: {"joy": 0.3, "sadness": 0.0, "anger": 0.0},\n            FacialExpression.THINKING: {"joy": 0.2, "sadness": 0.1, "anger": 0.0},\n            FacialExpression.SMILING: {"joy": 0.7, "sadness": 0.0, "anger": 0.0},\n            FacialExpression.NEUTRAL: {"joy": 0.2, "sadness": 0.1, "anger": 0.0},\n        }\n        return emotion_maps.get(expression, {"joy": 0.2, "sadness": 0.1, "anger": 0.0})\n\n\nclass GazeController:\n    """\n    Controller for robot eye gaze direction.\n\n    Manages eye movements for natural social gaze behaviors\n    including joint attention and conversational gaze patterns.\n    """\n\n    def __init__(self):\n        self.current_target: GazeTarget = GazeTarget.NEUTRAL\n        self.gaze_position: Tuple[float, float] = (0.0, 0.0)  # x, y angles\n        self.is_tracking = False\n        self.tracking_target: Optional[str] = None\n\n    def set_gaze(\n        self,\n        target: GazeTarget,\n        position: Optional[Tuple[float, float]] = None,\n        smooth: bool = True\n    ) -> None:\n        """\n        Set gaze direction.\n\n        Args:\n            target: The gaze target type\n            position: Specific position for object/location gazes\n            smooth: Whether to use smooth eye movement\n        """\n        self.current_target = target\n\n        if position:\n            self.gaze_position = position\n\n        gaze_angles = self._target_to_angles(target, position)\n        print(f"[Gaze] Target: {target.value}, Angles: {gaze_angles}")\n\n    def _target_to_angles(\n        self,\n        target: GazeTarget,\n        position: Optional[Tuple[float, float]]\n    ) -> Tuple[float, float]:\n        """Convert gaze target to eye angles."""\n        angle_map = {\n            GazeTarget.SPEAKER: (0.0, 5.0),    # Center, slightly up\n            GazeTarget.NEUTRAL: (0.0, 0.0),    # Center\n            GazeTarget.DOWN: (0.0, -15.0),     # Looking down\n            GazeTarget.UP: (0.0, 15.0),        # Looking up\n            GazeTarget.AWAY: (30.0, 0.0),      # Looking away\n        }\n\n        if target == GazeTarget.OBJECT or target == GazeTarget.LOCATION:\n            return position if position else (0.0, 0.0)\n\n        return angle_map.get(target, (0.0, 0.0))\n\n    async def gaze_at_person(self, person_position: Tuple[float, float]) -> None:\n        """Track a specific person\'s position with gaze."""\n        self.is_tracking = True\n        self.tracking_target = "person"\n\n        # Simulate tracking with updates\n        while self.is_tracking:\n            self.gaze_position = person_position\n            await asyncio.sleep(0.1)\n\n    def stop_tracking(self) -> None:\n        """Stop gaze tracking and return to neutral."""\n        self.is_tracking = False\n        self.tracking_target = None\n        self.set_gaze(GazeTarget.NEUTRAL)\n\n\nclass MultiModalIntegrator:\n    """\n    Main integrator for multi-modal robot interaction.\n\n    Coordinates speech, gesture, facial expression, and gaze\n    into coherent, contextually appropriate responses.\n    """\n\n    def __init__(self):\n        self.gesture_controller = GestureController()\n        self.face_controller = FacialExpressionController()\n        self.gaze_controller = GazeController()\n        self.modality_enabled = {\n            Modality.SPEECH: True,\n            Modality.GESTURE: True,\n            Modality.FACIAL_EXPRESSION: True,\n            Modality.EYE_GAZE: True,\n            Modality.BODY_POSTURE: True,\n            Modality.TOUCH: True\n        }\n\n    def enable_modality(self, modality: Modality, enabled: bool) -> None:\n        """Enable or disable a specific modality."""\n        self.modality_enabled[modality] = enabled\n\n    async def express(\n        self,\n        message: MultiModalMessage,\n        async_speech: bool = True\n    ) -> None:\n        """\n        Express a multi-modal message across all enabled channels.\n\n        Coordinates the timing and execution of expressions across\n        gesture, face, and gaze modalities.\n        """\n        tasks = []\n\n        # Gesture\n        if message.gesture and self.modality_enabled[Modality.GESTURE]:\n            tasks.append(self.gesture_controller.execute_gesture(\n                message.gesture,\n                duration=message.duration\n            ))\n\n        # Facial expression\n        if self.modality_enabled[Modality.FACIAL_EXPRESSION]:\n            self.face_controller.set_expression(\n                message.facial_expression,\n                intensity=message.emotion_intensity,\n                duration=message.duration\n            )\n\n        # Gaze\n        if self.modality_enabled[Modality.EYE_GAZE]:\n            self.gaze_controller.set_gaze(message.gaze_target)\n\n        # Execute all concurrently\n        if tasks:\n            await asyncio.gather(*tasks)\n\n        # Return to neutral after duration\n        await asyncio.sleep(message.duration * 0.5)\n        self.face_controller.set_expression(FacialExpression.NEUTRAL, 0.0)\n        self.gaze_controller.set_gaze(GazeTarget.NEUTRAL)\n\n    def create_happy_response(self, text: str) -> MultiModalMessage:\n        """Create a happy multi-modal response."""\n        return MultiModalMessage(\n            text=text,\n            facial_expression=FacialExpression.SMILING,\n            gaze_target=GazeTarget.SPEAKER,\n            emotion_intensity=0.7,\n            gesture=GestureType.NOD\n        )\n\n    def create_thinking_response(self, text: str) -> MultiModalMessage:\n        """Create a contemplative multi-modal response."""\n        return MultiModalMessage(\n            text=text,\n            facial_expression=FacialExpression.THINKING,\n            gaze_target=GazeTarget.UP,\n            emotion_intensity=0.4\n        )\n\n    def create_confused_response(self, text: str) -> MultiModalMessage:\n        """Create a confused multi-modal response for clarification."""\n        return MultiModalMessage(\n            text=text,\n            facial_expression=FacialExpression.CONFUSED,\n            gaze_target=GazeTarget.AWAY,\n            emotion_intensity=0.5,\n            gesture=GestureType.SHRUG\n        )\n\n    def create_pointing_response(\n        self,\n        text: str,\n        direction: str\n    ) -> MultiModalMessage:\n        """Create a response with pointing gesture."""\n        gesture_map = {\n            "left": GestureType.POINT_LEFT,\n            "right": GestureType.POINT_RIGHT,\n            "up": GestureType.POINT_UP,\n            "down": GestureType.POINT_DOWN\n        }\n\n        return MultiModalMessage(\n            text=text,\n            gesture=gesture_map.get(direction, GestureType.POINT_RIGHT),\n            facial_expression=FacialExpression.NEUTRAL,\n            gaze_target=GazeTarget.OBJECT,\n            emotion_intensity=0.5\n        )\n\n\n# Demonstration of multi-modal interaction\nasync def demonstrate_multi_modal():\n    """Demonstrate multi-modal interaction system."""\n\n    print("=== Multi-Modal Interaction Demo ===\\n")\n\n    # Initialize integrator\n    integrator = MultiModalIntegrator()\n\n    # Example multi-modal messages\n    messages = [\n        MultiModalMessage(\n            text="Hello! I\'m happy to see you!",\n            facial_expression=FacialExpression.SMILING,\n            gesture=GestureType.NOD,\n            gaze_target=GazeTarget.SPEAKER,\n            emotion_intensity=0.8\n        ),\n        MultiModalMessage(\n            text="Let me think about that...",\n            facial_expression=FacialExpression.THINKING,\n            gaze_target=GazeTarget.UP,\n            emotion_intensity=0.5\n        ),\n        MultiModalMessage(\n            text="I\'m not sure which one you mean.",\n            facial_expression=FacialExpression.CONFUSED,\n            gesture=GestureType.SHRUG,\n            gaze_target=GazeTarget.AWAY,\n            emotion_intensity=0.6\n        ),\n        MultiModalMessage(\n            text="The cup is over there on the table.",\n            facial_expression=FacialExpression.NEUTRAL,\n            gesture=GestureType.POINT_RIGHT,\n            gaze_target=GazeTarget.OBJECT,\n            emotion_intensity=0.4\n        ),\n        MultiModalMessage(\n            text="Great! I understand now!",\n            facial_expression=FacialExpression.HAPPY,\n            gesture=GestureType.THUMBS_UP,\n            gaze_target=GazeTarget.SPEAKER,\n            emotion_intensity=0.9\n        )\n    ]\n\n    print("Executing multi-modal expressions:\\n")\n\n    for i, message in enumerate(messages, 1):\n        print(f"--- Expression {i} ---")\n        print(f"Text: \\"{message.text}\\"")\n        print(f"Face: {message.facial_expression.value}")\n        if message.gesture:\n            print(f"Gesture: {message.gesture.value}")\n        print(f"Gaze: {message.gaze_target.value}")\n        print()\n\n        await integrator.express(message)\n        await asyncio.sleep(0.5)\n\n    print("\\nMulti-modal demonstration complete.")\n\n\nif __name__ == "__main__":\n    asyncio.run(demonstrate_multi_modal())\n'})}),"\n",(0,s.jsx)(n.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,s.jsx)(n.p,{children:"This chapter explored the comprehensive topic of conversational AI for humanoid robots, covering the essential technologies and integration patterns that enable natural human-robot dialogue."}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Conversational AI Foundations"}),": The chapter introduced the conversational AI pipeline for robotics, explaining how speech recognition, natural language understanding, dialogue management, and speech synthesis work together to create natural interaction experiences."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPT Integration"}),": We examined patterns for integrating large language models like GPT into robot dialogue systems, including context management, prompt engineering, and hybrid architectures that combine cloud and local processing."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition and Synthesis"}),": The chapter covered the practical implementation of speech interfaces including audio preprocessing, voice activity detection, and text-to-speech synthesis for robot output."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding"}),": We implemented comprehensive NLU capabilities including intent classification, entity extraction, reference resolution, and sentiment analysis for understanding user input."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Modal Interaction"}),": The chapter explored how speech integrates with gesture, facial expression, and eye gaze to create coherent, expressive robot communication across multiple channels."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Conversational Pipeline"}),": The sequence of ASR, NLU, dialogue management, NLG, and TTS that enables voice interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Classification"}),": Assigning user utterances to predefined categories for appropriate response selection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying specific objects, locations, and other relevant information in user speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Modal Integration"}),": Coordinating verbal and non-verbal communication channels for natural expression"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Management"}),": Maintaining conversation state across extended interactions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-terminology",children:"Key Terminology"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ASR (Automatic Speech Recognition)"}),": Technology for converting spoken language to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TTS (Text-to-Speech)"}),": Technology for converting text to spoken language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NLU (Natural Language Understanding)"}),": Processing language to extract meaning and intent"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dialogue Manager"}),": System component that maintains conversation state and coordinates responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Modal Interaction"}),": Using multiple communication channels simultaneously"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech and Language Processing"}),": Jurafsky & Martin - Comprehensive NLP textbook"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Whisper Documentation"}),": State-of-the-art speech recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Audio Packages"}),": Audio processing and ASR integration for ROS"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Interaction Research"}),": Contemporary papers on social robotics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"next-chapter",children:"Next Chapter"}),"\n",(0,s.jsxs)(n.p,{children:["Chapter 7 explores ",(0,s.jsx)(n.strong,{children:"Robot Learning and Adaptation"}),", covering how robots learn from experience, adapt to new situations, and improve their capabilities over time through machine learning techniques."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Part 6: Conversational Robotics"})," | ",(0,s.jsx)(n.a,{href:"part-6-conversational/conversational-robotics",children:"Chapter 6: Conversational Robotics"})," | ",(0,s.jsx)(n.a,{href:"appendix/D-assessment-rubrics",children:"Appendix: Assessment Rubrics"})]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);