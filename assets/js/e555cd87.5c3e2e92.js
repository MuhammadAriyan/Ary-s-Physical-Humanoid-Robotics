"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[21],{3083:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"part-6-conversational/06a-week-13-overview","title":"Week 13 Overview: Conversational AI Integration","description":"This one-week module provides a focused learning plan for integrating conversational AI capabilities into humanoid robot systems. The curriculum builds upon previous knowledge of AI integration and ROS 2 to implement voice interaction, natural language understanding, and multi-modal communication.","source":"@site/docs/part-6-conversational/06a-week-13-overview.md","sourceDirName":"part-6-conversational","slug":"/part-6-conversational/06a-week-13-overview","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-6-conversational/06a-week-13-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part-6-conversational/06a-week-13-overview.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"title":"Week 13 Overview: Conversational AI Integration","sidebar_position":14},"sidebar":"tutorialSidebar","previous":{"title":"Conversational Robotics","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-6-conversational/conversational-robotics"},"next":{"title":"Appendix A: Hardware Specifications","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/appendix/hardware-specifications"}}');var t=i(4848),r=i(8453);const l={title:"Week 13 Overview: Conversational AI Integration",sidebar_position:14},a="Week 13 Overview: Conversational AI Integration",o={},c=[{value:"Week Focus: Conversational AI Integration",id:"week-focus-conversational-ai-integration",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Key Concepts",id:"key-concepts",level:2},{value:"1.1 The Conversational AI Pipeline",id:"11-the-conversational-ai-pipeline",level:3},{value:"1.2 Speech Recognition Considerations",id:"12-speech-recognition-considerations",level:3},{value:"1.3 Intent Classification Strategies",id:"13-intent-classification-strategies",level:3},{value:"1.4 Entity Extraction for Robot Commands",id:"14-entity-extraction-for-robot-commands",level:3},{value:"1.5 Multi-Modal Integration Principles",id:"15-multi-modal-integration-principles",level:3},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Exercise 1: Speech Recognition Setup (3 hours)",id:"exercise-1-speech-recognition-setup-3-hours",level:3},{value:"Exercise 2: Intent Classification System (3 hours)",id:"exercise-2-intent-classification-system-3-hours",level:3},{value:"Exercise 3: Entity Extraction Pipeline (2 hours)",id:"exercise-3-entity-extraction-pipeline-2-hours",level:3},{value:"Exercise 4: Dialogue Manager with GPT (4 hours)",id:"exercise-4-dialogue-manager-with-gpt-4-hours",level:3},{value:"Exercise 5: Multi-Modal Expression System (3 hours)",id:"exercise-5-multi-modal-expression-system-3-hours",level:3},{value:"Estimated Time Commitment",id:"estimated-time-commitment",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:2},{value:"Software",id:"software",level:3},{value:"Optional Hardware",id:"optional-hardware",level:3},{value:"API Keys Required",id:"api-keys-required",level:3},{value:"Discussion Questions",id:"discussion-questions",level:2},{value:"Code Examples Summary",id:"code-examples-summary",level:2},{value:"Speech Recognition Pipeline",id:"speech-recognition-pipeline",level:3},{value:"Intent Classification",id:"intent-classification",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:3},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Documentation",id:"documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Community",id:"community",level:3},{value:"Week 13 Progress Checklist",id:"week-13-progress-checklist",level:2},{value:"Foundation",id:"foundation",level:3},{value:"Speech Recognition",id:"speech-recognition",level:3},{value:"Natural Language Understanding",id:"natural-language-understanding",level:3},{value:"Dialogue Management",id:"dialogue-management",level:3},{value:"Multi-Modal Integration",id:"multi-modal-integration-1",level:3},{value:"Integration",id:"integration",level:3},{value:"Transition to Next Section",id:"transition-to-next-section",level:2},{value:"Quick Preview: Part 7 Topics",id:"quick-preview-part-7-topics",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Tips for Success",id:"tips-for-success",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-13-overview-conversational-ai-integration",children:"Week 13 Overview: Conversational AI Integration"})}),"\n",(0,t.jsx)(n.p,{children:"This one-week module provides a focused learning plan for integrating conversational AI capabilities into humanoid robot systems. The curriculum builds upon previous knowledge of AI integration and ROS 2 to implement voice interaction, natural language understanding, and multi-modal communication."}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsxs)(n.p,{children:["This overview is part of ",(0,t.jsx)(n.strong,{children:"Part 6: Conversational Robotics"})," of the Physical AI & Humanoid Robotics textbook. The complete textbook structure is available in the sidebar navigation."]})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"week-focus-conversational-ai-integration",children:"Week Focus: Conversational AI Integration"}),"\n",(0,t.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of Week 13, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Understand the conversational AI pipeline"})," for robotics, including speech recognition, natural language understanding, dialogue management, and speech synthesis"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Implement speech recognition systems"})," that convert spoken language to text with appropriate preprocessing for robot acoustic environments"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Design natural language understanding"})," components that extract intent and entities from transcribed user speech"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Integrate large language models"})," like GPT for natural dialogue generation while maintaining robot-specific context and constraints"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create multi-modal interaction systems"})," that coordinate speech, gesture, facial expression, and eye gaze for coherent robot communication"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before beginning this week, ensure you have completed:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Part 1"}),": Physical AI Foundations (understanding robot systems)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Part 2"}),": ROS 2 Fundamentals (ROS 2 concepts and patterns)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Part 3"}),": Simulation (sensor and perception basics)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Part 5"}),": AI Integration with ROS 2 (AI model integration patterns)"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"11-the-conversational-ai-pipeline",children:"1.1 The Conversational AI Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Conversational AI for robots involves a sophisticated pipeline of processing stages, each building upon the outputs of the previous stage:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Stage"}),(0,t.jsx)(n.th,{children:"Input"}),(0,t.jsx)(n.th,{children:"Output"}),(0,t.jsx)(n.th,{children:"Key Technologies"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Speech Recognition"})}),(0,t.jsx)(n.td,{children:"Audio waveform"}),(0,t.jsx)(n.td,{children:"Text transcript"}),(0,t.jsx)(n.td,{children:"Whisper, DeepSpeech, streaming ASR"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Natural Language Understanding"})}),(0,t.jsx)(n.td,{children:"Text"}),(0,t.jsx)(n.td,{children:"Intent + entities"}),(0,t.jsx)(n.td,{children:"Pattern matching, classifiers, NER"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Dialogue Management"})}),(0,t.jsx)(n.td,{children:"Intent + entities + context"}),(0,t.jsx)(n.td,{children:"System action"}),(0,t.jsx)(n.td,{children:"State machines, LLMs, context tracking"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Natural Language Generation"})}),(0,t.jsx)(n.td,{children:"System action + context"}),(0,t.jsx)(n.td,{children:"Response text"}),(0,t.jsx)(n.td,{children:"GPT, rule-based templates, T5"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Speech Synthesis"})}),(0,t.jsx)(n.td,{children:"Response text"}),(0,t.jsx)(n.td,{children:"Audio waveform"}),(0,t.jsx)(n.td,{children:"Tacotron, WaveNet, gTTS"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"The pipeline operates in real-time for interactive applications, with each stage adding latency. For natural conversation, total round-trip latency should remain under 500 milliseconds to avoid perceptible delays."}),"\n",(0,t.jsx)(n.h3,{id:"12-speech-recognition-considerations",children:"1.2 Speech Recognition Considerations"}),"\n",(0,t.jsx)(n.p,{children:"Robot applications introduce unique challenges for speech recognition:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Background Noise"}),": Ventilation systems, other people, and robot motors create acoustic interference"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Far-Field Audio"}),": Room-scale audio capture introduces reverberation and distance-related degradation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple Speakers"}),": Home and office environments often have multiple simultaneous speakers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Acoustic Adaptation"}),": The robot's physical embodiment changes the acoustic characteristics of its environment"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Effective ASR pipelines for robots include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-microphone array processing for source localization and noise reduction"}),"\n",(0,t.jsx)(n.li,{children:"Voice activity detection to distinguish speech from background"}),"\n",(0,t.jsx)(n.li,{children:"Adaptive echo cancellation for environments with speech playback"}),"\n",(0,t.jsx)(n.li,{children:"Speaker diarization for multi-speaker scenarios"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"13-intent-classification-strategies",children:"1.3 Intent Classification Strategies"}),"\n",(0,t.jsx)(n.p,{children:"Intent classification assigns user utterances to predefined categories that map to robot actions:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pattern-Based Classification"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Keyword and regex matching for rapid prototyping"}),"\n",(0,t.jsx)(n.li,{children:"High interpretability and easy modification"}),"\n",(0,t.jsx)(n.li,{children:"Limited generalization to novel phrasing"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Machine Learning Classification"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Traditional classifiers (SVM, Random Forest) on handcrafted features"}),"\n",(0,t.jsx)(n.li,{children:"Neural classifiers (CNN, LSTM, Transformer) on raw text"}),"\n",(0,t.jsx)(n.li,{children:"Better generalization to varied input"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"LLM-Based Classification"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Zero-shot classification using instruction-tuned models"}),"\n",(0,t.jsx)(n.li,{children:"Few-shot learning with examples in prompts"}),"\n",(0,t.jsx)(n.li,{children:"Most flexible but requires API access"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For production systems, hybrid approaches combining pattern matching for common cases with ML or LLM fallback for complex inputs often provide the best balance."}),"\n",(0,t.jsx)(n.h3,{id:"14-entity-extraction-for-robot-commands",children:"1.4 Entity Extraction for Robot Commands"}),"\n",(0,t.jsx)(n.p,{children:"Entity extraction identifies specific pieces of information in user utterances:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Entity Type"}),(0,t.jsx)(n.th,{children:"Examples"}),(0,t.jsx)(n.th,{children:"Robot Relevance"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Objects"})}),(0,t.jsx)(n.td,{children:"cup, book, keys"}),(0,t.jsx)(n.td,{children:"Manipulation targets"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Locations"})}),(0,t.jsx)(n.td,{children:"kitchen, table, shelf"}),(0,t.jsx)(n.td,{children:"Navigation destinations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"People"})}),(0,t.jsx)(n.td,{children:"me, you, mom"}),(0,t.jsx)(n.td,{children:"Interaction partners"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Quantities"})}),(0,t.jsx)(n.td,{children:"one, few, several"}),(0,t.jsx)(n.td,{children:"Amount specifications"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Times"})}),(0,t.jsx)(n.td,{children:"now, later, tomorrow"}),(0,t.jsx)(n.td,{children:"Temporal constraints"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Actions"})}),(0,t.jsx)(n.td,{children:"bring, get, find"}),(0,t.jsx)(n.td,{children:"Task type specification"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"Effective entity extraction handles:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Multi-word entities (e.g., "living room")'}),"\n",(0,t.jsx)(n.li,{children:'Entity mentions with modifiers (e.g., "red cup")'}),"\n",(0,t.jsx)(n.li,{children:'Coreference resolution (e.g., "it" referring to previously mentioned object)'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"15-multi-modal-integration-principles",children:"1.5 Multi-Modal Integration Principles"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robot interaction naturally encompasses multiple communication channels:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Synchronization Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lip sync: Speech audio must synchronize with lip movements"}),"\n",(0,t.jsx)(n.li,{children:"Gesture timing: Gestures should align with relevant speech content"}),"\n",(0,t.jsx)(n.li,{children:"Gaze shifts: Eye movements should precede or accompany attention shifts"}),"\n",(0,t.jsx)(n.li,{children:"Expression changes: Facial expressions should reflect emotional content"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Channel Prioritization"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Speech carries primary information content"}),"\n",(0,t.jsx)(n.li,{children:"Gestures emphasize and disambiguate verbal content"}),"\n",(0,t.jsx)(n.li,{children:"Facial expressions convey emotional state and engagement"}),"\n",(0,t.jsx)(n.li,{children:"Gaze communicates attention and social signals"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal Fusion"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Early fusion: Combine raw inputs at feature level"}),"\n",(0,t.jsx)(n.li,{children:"Late fusion: Process each modality independently, combine decisions"}),"\n",(0,t.jsx)(n.li,{children:"Hybrid: Some modalities early-fused, others late-fused"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-speech-recognition-setup-3-hours",children:"Exercise 1: Speech Recognition Setup (3 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Implement a ROS 2 speech recognition node that processes audio input and outputs transcribed text."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Set up audio capture from robot microphones using the ",(0,t.jsx)(n.code,{children:"audio_common"})," package"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement an AudioPreprocessor class with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice activity detection using energy thresholding"}),"\n",(0,t.jsx)(n.li,{children:"Audio normalization for consistent volume levels"}),"\n",(0,t.jsx)(n.li,{children:"Noise reduction preprocessing"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Integrate a speech recognition provider (Whisper via API or local model)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a ROS 2 node that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Subscribes to audio topics"}),"\n",(0,t.jsx)(n.li,{children:"Processes audio through the pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Publishes recognition results"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Python package with working speech recognition node"}),"\n",(0,t.jsx)(n.li,{children:"Documentation of configuration options"}),"\n",(0,t.jsx)(n.li,{children:"Test recordings showing recognition accuracy"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example structure for the exercise\nclass AudioPreprocessor:\n    def __init__(self, sample_rate=16000, chunk_size=1024):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.energy_threshold = 500.0\n\n    def calculate_energy(self, audio_data: bytes) -> float:\n        """Calculate RMS energy of audio chunk."""\n        import audioop\n        return audioop.rms(audio_data, 2)\n\n    def is_speech(self, audio_data: bytes) -> bool:\n        """Voice activity detection."""\n        return self.calculate_energy(audio_data) > self.energy_threshold\n\n    def preprocess(self, audio_data: bytes) -> bytes:\n        """Apply preprocessing to audio."""\n        # Normalization and noise reduction\n        return audio_data\n'})}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-intent-classification-system-3-hours",children:"Exercise 2: Intent Classification System (3 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Build an intent classification system that correctly categorizes user commands."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Design an intent taxonomy for common robot commands (at least 8 intents)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement an IntentClassifier with pattern-based matching:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define intent patterns with associated confidence weights"}),"\n",(0,t.jsx)(n.li,{children:"Handle multi-label classification for complex utterances"}),"\n",(0,t.jsx)(n.li,{children:"Return both intent and confidence score"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Test the classifier with 20+ example utterances"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Evaluate accuracy and identify edge cases"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Intent classifier implementation with test cases"}),"\n",(0,t.jsx)(n.li,{children:"Confusion matrix showing classification performance"}),"\n",(0,t.jsx)(n.li,{children:"Analysis of failure cases and improvements"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-entity-extraction-pipeline-2-hours",children:"Exercise 3: Entity Extraction Pipeline (2 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Create an entity extraction system for robot-specific entity types."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Define entity types relevant to robot commands (object, location, person, etc.)"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement regex-based and keyword-based extraction"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Handle entity normalization and validation"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Add entity extraction to the NLU pipeline"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Entity extractor implementation"}),"\n",(0,t.jsx)(n.li,{children:"Test cases showing extraction on varied input"}),"\n",(0,t.jsx)(n.li,{children:"Documentation of entity types and patterns"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-4-dialogue-manager-with-gpt-4-hours",children:"Exercise 4: Dialogue Manager with GPT (4 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Implement a dialogue manager that uses GPT for natural response generation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a ConversationContext class to track dialogue state"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement a GPTClient for API integration:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Build prompt templates with conversation history"}),"\n",(0,t.jsx)(n.li,{children:"Handle API communication and response parsing"}),"\n",(0,t.jsx)(n.li,{children:"Implement error handling and retries"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Build a DialogueManager that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Processes user input through NLU"}),"\n",(0,t.jsx)(n.li,{children:"Generates responses using GPT"}),"\n",(0,t.jsx)(n.li,{children:"Updates conversation context"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Add robot-specific constraints (safety, capabilities)"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Complete dialogue manager implementation"}),"\n",(0,t.jsx)(n.li,{children:"Example conversations demonstrating the system"}),"\n",(0,t.jsx)(n.li,{children:"Analysis of response quality and limitations"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-5-multi-modal-expression-system-3-hours",children:"Exercise 5: Multi-Modal Expression System (3 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Create a system for coordinated multi-modal robot expression."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Implement individual controllers for:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Facial expression display"}),"\n",(0,t.jsx)(n.li,{children:"Gesture execution"}),"\n",(0,t.jsx)(n.li,{children:"Eye gaze direction"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Create a MultiModalIntegrator class that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Accepts multi-modal messages"}),"\n",(0,t.jsx)(n.li,{children:"Coordinates timing across modalities"}),"\n",(0,t.jsx)(n.li,{children:"Manages modality priority and interruption"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Design and execute multi-modal response scenarios"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Deliverable"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multi-modal integration system"}),"\n",(0,t.jsx)(n.li,{children:"Demonstration of coordinated expressions"}),"\n",(0,t.jsx)(n.li,{children:"Documentation of timing and synchronization"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"estimated-time-commitment",children:"Estimated Time Commitment"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Activity"}),(0,t.jsx)(n.th,{children:"Hours"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Reading (Chapter 6 content)"}),(0,t.jsx)(n.td,{children:"4 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exercise 1: Speech Recognition"}),(0,t.jsx)(n.td,{children:"3 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exercise 2: Intent Classification"}),(0,t.jsx)(n.td,{children:"3 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exercise 3: Entity Extraction"}),(0,t.jsx)(n.td,{children:"2 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exercise 4: Dialogue Manager"}),(0,t.jsx)(n.td,{children:"4 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Exercise 5: Multi-Modal System"}),(0,t.jsx)(n.td,{children:"3 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Troubleshooting and review"}),(0,t.jsx)(n.td,{children:"2 hours"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Total"})}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"21 hours"})})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hardware-and-software-requirements",children:"Hardware and Software Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Version"}),(0,t.jsx)(n.th,{children:"Purpose"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"ROS 2"}),(0,t.jsx)(n.td,{children:"Humble or Jazzy"}),(0,t.jsx)(n.td,{children:"Robot middleware"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Python"}),(0,t.jsx)(n.td,{children:"3.10+"}),(0,t.jsx)(n.td,{children:"Implementation language"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"OpenAI API"}),(0,t.jsx)(n.td,{children:"Latest"}),(0,t.jsx)(n.td,{children:"GPT integration"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Whisper"}),(0,t.jsx)(n.td,{children:"Latest"}),(0,t.jsx)(n.td,{children:"Speech recognition"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"audioop"}),(0,t.jsx)(n.td,{children:"Standard library"}),(0,t.jsx)(n.td,{children:"Audio processing"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"optional-hardware",children:"Optional Hardware"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Component"}),(0,t.jsx)(n.th,{children:"Purpose"}),(0,t.jsx)(n.th,{children:"Recommended"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Microphone array"}),(0,t.jsx)(n.td,{children:"Multi-channel audio capture"}),(0,t.jsx)(n.td,{children:"ReSpeaker 4-mic"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Speakers"}),(0,t.jsx)(n.td,{children:"Speech output"}),(0,t.jsx)(n.td,{children:"Any standard"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Face display"}),(0,t.jsx)(n.td,{children:"Expression capability"}),(0,t.jsx)(n.td,{children:"Robot-specific"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"api-keys-required",children:"API Keys Required"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"OpenAI API key for GPT integration (or local LLM deployment)"}),"\n",(0,t.jsx)(n.li,{children:"Optional: Cloud speech recognition services (Google, Azure, AWS)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"How should conversational AI handle ambiguous commands where multiple interpretations are possible?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What are the trade-offs between cloud-based LLMs and local language models for robot dialogue?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"How can multi-modal input (speech + gesture) improve reference resolution accuracy?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What safety considerations apply to conversational AI systems that can execute robot actions?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"How should the system handle users who speak with accents or in non-native languages?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"What personality traits would you design into a humanoid robot's conversational style?"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"code-examples-summary",children:"Code Examples Summary"}),"\n",(0,t.jsx)(n.h3,{id:"speech-recognition-pipeline",children:"Speech Recognition Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Audio preprocessing and voice activity detection\nclass AudioPreprocessor:\n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n        self.energy_threshold = self._initialize_threshold()\n\n    def is_speech(self, audio_data: bytes) -> bool:\n        energy = audioop.rms(audio_data, 2)\n        return energy > self.energy_threshold\n"})}),"\n",(0,t.jsx)(n.h3,{id:"intent-classification",children:"Intent Classification"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Pattern-based intent classification\nclass IntentClassifier:\n    INTENT_PATTERNS = {\n        Intent.COMMAND: [r"\\b(go|move|grab|pick)\\b", 0.7],\n        Intent.REQUEST: [r"\\b(please|could you)\\b", 0.85],\n    }\n\n    def classify(self, text: str) -> Tuple[Intent, float]:\n        # Pattern matching logic\n        return Intent.UNKNOWN, 0.0\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Coordinated expression across modalities\nclass MultiModalIntegrator:\n    async def express(self, message: MultiModalMessage):\n        tasks = [\n            self.gesture_controller.execute_gesture(message.gesture),\n            self.face_controller.set_expression(message.facial_expression),\n            self.gaze_controller.set_gaze(message.gaze_target)\n        ]\n        await asyncio.gather(*tasks)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://platform.openai.com/docs",children:"OpenAI API Documentation"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"Whisper Speech Recognition"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://wiki.ros.org/audio_common",children:"ROS 2 Audio Packages"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://gtts.readthedocs.io/",children:"gTTS Text-to-Speech"})}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"OpenAI API Quickstart Guide"}),"\n",(0,t.jsx)(n.li,{children:"Whisper Local Installation Tutorial"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 Audio Stream Processing"}),"\n",(0,t.jsx)(n.li,{children:"Multi-Modal HRI Research Papers"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"community",children:"Community"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ROS Discourse - Robotics Software"}),"\n",(0,t.jsx)(n.li,{children:"OpenAI Community Forum"}),"\n",(0,t.jsx)(n.li,{children:"Human-Robot Interaction Conference (HRI)"}),"\n",(0,t.jsx)(n.li,{children:"Reddit r/robotics"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"week-13-progress-checklist",children:"Week 13 Progress Checklist"}),"\n",(0,t.jsx)(n.p,{children:"Use this checklist to track your progress through the week:"}),"\n",(0,t.jsx)(n.h3,{id:"foundation",children:"Foundation"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Understand the conversational AI pipeline architecture"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Identify key components and their interactions"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Recognize trade-offs in different implementation approaches"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"speech-recognition",children:"Speech Recognition"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement audio preprocessing and VAD"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate speech recognition provider"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handle noise and far-field audio challenges"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"natural-language-understanding",children:"Natural Language Understanding"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design intent taxonomy for robot commands"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement pattern-based intent classification"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create entity extraction for robot-relevant entities"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test NLU on varied user input"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"dialogue-management",children:"Dialogue Management"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement conversation context tracking"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate GPT for response generation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add robot-specific constraints and safety"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Handle dialogue state and turn-taking"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"multi-modal-integration-1",children:"Multi-Modal Integration"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create modality-specific controllers"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement coordinated multi-modal expression"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Design appropriate expression scenarios"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test synchronization and timing"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"integration",children:"Integration"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Combine all components into complete system"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test end-to-end conversational flow"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Evaluate system performance"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Identify and address edge cases"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"transition-to-next-section",children:"Transition to Next Section"}),"\n",(0,t.jsxs)(n.p,{children:["After completing Week 13, you will be ready to explore ",(0,t.jsx)(n.strong,{children:"Robot Learning and Adaptation"})," in Part 7, where you will learn about:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reinforcement learning for robot skill acquisition"}),"\n",(0,t.jsx)(n.li,{children:"Imitation learning from human demonstrations"}),"\n",(0,t.jsx)(n.li,{children:"Online adaptation and few-shot learning"}),"\n",(0,t.jsx)(n.li,{children:"Transfer learning from simulation to reality"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"quick-preview-part-7-topics",children:"Quick Preview: Part 7 Topics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Markov Decision Processes and policy learning"}),"\n",(0,t.jsx)(n.li,{children:"Reward shaping and task specification"}),"\n",(0,t.jsx)(n.li,{children:"Behavior cloning and demonstration learning"}),"\n",(0,t.jsx)(n.li,{children:"Domain randomization and adaptation"}),"\n",(0,t.jsx)(n.li,{children:"Meta-learning for rapid adaptation"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Criterion"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Description"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Speech Recognition"}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Functional ASR with preprocessing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Intent Classification"}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Accurate intent recognition"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Entity Extraction"}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Complete entity identification"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Dialogue Management"}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"Natural, context-aware responses"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Multi-Modal Integration"}),(0,t.jsx)(n.td,{children:"15%"}),(0,t.jsx)(n.td,{children:"Coordinated expression"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Code Quality"}),(0,t.jsx)(n.td,{children:"5%"}),(0,t.jsx)(n.td,{children:"Clean, documented implementation"})]})]})]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"tips-for-success",children:"Tips for Success"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Start with simple patterns"}),": Begin with basic keyword matching before adding complexity"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Test incrementally"}),": Verify each component works before integration"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use real audio data"}),": Test with actual robot microphone input when possible"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Plan for failure"}),": Design graceful degradation when components fail"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Iterate on prompts"}),": GPT responses often need prompt refinement"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Document edge cases"}),": Note unusual inputs that cause problems"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Part 6: Conversational Robotics"})," | ",(0,t.jsx)(n.a,{href:"part-6-conversational/conversational-robotics",children:"Chapter 6: Conversational Robotics"})," | ",(0,t.jsx)(n.a,{href:"appendix/D-assessment-rubrics",children:"Appendix: Assessment Rubrics"})]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function l(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);