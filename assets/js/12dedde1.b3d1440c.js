"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[113],{8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var a=t(6540);const o={},i=a.createContext(o);function r(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),a.createElement(i.Provider,{value:e},n.children)}},9620:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>r,metadata:()=>a,toc:()=>p});const a=JSON.parse('{"id":"part-5-humanoid/humanoid-robot-development","title":"Humanoid Robot Development","description":"Learning Objectives","source":"@site/docs/part-5-humanoid/05-humanoid-robot-development.md","sourceDirName":"part-5-humanoid","slug":"/part-5-humanoid/humanoid-robot-development","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-5-humanoid/humanoid-robot-development","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/part-5-humanoid/05-humanoid-robot-development.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Humanoid Robot Development","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Weeks 8-10: NVIDIA Isaac Platform","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-4-isaac/04a-week-8-10-overview"},"next":{"title":"Weeks 11-12: Humanoid Robot Development","permalink":"/Ary-s-Physical-Humanoid-Robotics/docs/part-5-humanoid/05a-week-11-12-overview"}}');var o=t(4848),i=t(8453);const r={title:"Humanoid Robot Development",sidebar_position:5},s="Chapter 5: Humanoid Robot Development",l={},p=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"5.1 Introduction to Humanoid Robot Kinematics",id:"51-introduction-to-humanoid-robot-kinematics",level:2},{value:"Kinematic Chain Structure",id:"kinematic-chain-structure",level:3},{value:"Coordinate Frame Conventions",id:"coordinate-frame-conventions",level:3},{value:"5.2 Forward and Inverse Kinematics",id:"52-forward-and-inverse-kinematics",level:2},{value:"Forward Kinematics Implementation",id:"forward-kinematics-implementation",level:3},{value:"Inverse Kinematics Approaches",id:"inverse-kinematics-approaches",level:3},{value:"5.3 Bipedal Locomotion and Balance Control",id:"53-bipedal-locomotion-and-balance-control",level:2},{value:"Zero Moment Point and Stability",id:"zero-moment-point-and-stability",level:3},{value:"Walking Controller Implementation",id:"walking-controller-implementation",level:3},{value:"5.4 Manipulation and Grasping with Humanoid Hands",id:"54-manipulation-and-grasping-with-humanoid-hands",level:2},{value:"Grasping Fundamentals",id:"grasping-fundamentals",level:3},{value:"Grasp Planning Implementation",id:"grasp-planning-implementation",level:3},{value:"5.5 Natural Human-Robot Interaction Design",id:"55-natural-human-robot-interaction-design",level:2},{value:"Principles of Natural HRI",id:"principles-of-natural-hri",level:3},{value:"Multimodal Interaction Architecture",id:"multimodal-interaction-architecture",level:3},{value:"5.6 Code Example: Complete Kinematics and Walking Controller",id:"56-code-example-complete-kinematics-and-walking-controller",level:2},{value:"Chapter Summary",id:"chapter-summary",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"Unitree Robot Hardware Reference",id:"unitree-robot-hardware-reference",level:3},{value:"Cross-References",id:"cross-references",level:3},{value:"Further Reading",id:"further-reading",level:3},{value:"Next Chapter",id:"next-chapter",level:3}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-5-humanoid-robot-development",children:"Chapter 5: Humanoid Robot Development"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understand humanoid robot kinematics including forward and inverse kinematics"}),"\n",(0,o.jsx)(e.li,{children:"Implement bipedal locomotion control and balance strategies"}),"\n",(0,o.jsx)(e.li,{children:"Design manipulation systems for humanoid hands with grasping algorithms"}),"\n",(0,o.jsx)(e.li,{children:"Create natural human-robot interaction paradigms"}),"\n",(0,o.jsx)(e.li,{children:"Develop working code examples for kinematics calculations and walking controllers"}),"\n",(0,o.jsx)(e.li,{children:"Reference Unitree robot hardware specifications for real-world applications"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"51-introduction-to-humanoid-robot-kinematics",children:"5.1 Introduction to Humanoid Robot Kinematics"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robot kinematics forms the mathematical foundation for all motion planning, control, and manipulation tasks. Unlike industrial robots with fixed bases, humanoid robots must contend with dynamic base conditions, self-collision avoidance, and balance constraints. This chapter builds upon the simulation and control foundations established in previous chapters to address the unique challenges of humanoid robot development."}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots typically possess 30 or more degrees of freedom distributed across the legs, torso, arms, and head. The Unitree H1 robot, for example, features 19 degrees of freedom in its humanoid configuration: 3 in each leg for hip, knee, and ankle, 2 in each arm for shoulder and elbow, and additional joints for waist and head orientation. This complexity requires systematic approaches to kinematics, dynamics, and control."}),"\n",(0,o.jsx)(e.h3,{id:"kinematic-chain-structure",children:"Kinematic Chain Structure"}),"\n",(0,o.jsx)(e.p,{children:"A humanoid robot's kinematic structure follows an anthropomorphic design mirroring human body organization. The base of the kinematic tree is typically the pelvis or torso, with limbs branching outward to the extremities. Understanding this hierarchical structure is essential for implementing forward and inverse kinematics algorithms."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nHumanoid Robot Kinematic Chain Definition\n\nThis module defines the kinematic structure of a humanoid robot\nusing a systematic joint naming convention and tree structure.\n"""\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nfrom enum import Enum\nimport numpy as np\n\n\nclass JointType(Enum):\n    """Classification of joint types by motion capability."""\n    REVOLUTE = "revolute"          # Single-axis rotation (most humanoid joints)\n    CONTINUOUS = "continuous"      # Unlimited rotation (waist yaw, neck yaw)\n    PRISMATIC = "prismatic"        # Linear motion (rare in humanoids)\n    SPHERICAL = "spherical"        # Three-axis rotation (theoretical)\n\n\n@dataclass\nclass DHParameter:\n    """\n    Denavit-Hartenberg parameters for a single joint/link.\n\n    The DH convention provides a standardized way to describe\n    robot geometry using four parameters per joint.\n    """\n    theta: float       # Joint angle (variable for revolute joints)\n    d: float           # Link offset (variable for prismatic joints)\n    a: float           # Link length\n    alpha: float       # Link twist angle\n\n    # Transform from parent frame to this frame\n    def get_transform(self) -> np.ndarray:\n        """Compute homogeneous transformation matrix from DH parameters."""\n        ct = np.cos(self.theta)\n        st = np.sin(self.theta)\n        ca = np.cos(self.alpha)\n        sa = np.sin(self.alpha)\n\n        T = np.array([\n            [ct, -st * ca,  st * sa, self.a * ct],\n            [st,  ct * ca, -ct * sa, self.a * st],\n            [0,      sa,       ca,     self.d],\n            [0,       0,        0,         1]\n        ])\n        return T\n\n\n@dataclass\nclass LinkProperties:\n    """Physical properties of a robot link."""\n    mass: float                    # Mass in kilograms\n    com: np.ndarray               # Center of mass offset from joint frame\n    inertia: np.ndarray           # 3x3 inertia tensor\n    length: float = 0.0           # Characteristic length\n    width: float = 0.0            # Characteristic width\n    height: float = 0.0           # Characteristic height\n\n\n@dataclass\nclass JointLimits:\n    """Position, velocity, and effort limits for a joint."""\n    position_min: float           # Minimum joint angle (radians)\n    position_max: float           # Maximum joint angle (radians)\n    velocity_max: float           # Maximum velocity (rad/s)\n    effort_max: float             # Maximum torque (Nm)\n\n\n@dataclass\nclass Joint:\n    """\n    Complete joint specification including kinematics and limits.\n    """\n    name: str\n    joint_type: JointType\n    axis: np.ndarray              # Rotation/translation axis\n    parent_link: str              # Name of parent link\n    child_link: str               # Name of child link\n    dh_params: DHParameter\n    limits: JointLimits\n    properties: LinkProperties\n    stiffness: float = 0.0        # Joint stiffness for compliant control\n    damping: float = 0.0          # Joint damping for compliant control\n\n\nclass HumanoidKinematicChain:\n    """\n    Manages the complete kinematic chain of a humanoid robot.\n\n    This class provides methods for forward kinematics computation,\n    joint space operations, and kinematic validation.\n    """\n\n    def __init__(self, robot_name: str = "humanoid"):\n        self.robot_name = robot_name\n        self.joints: Dict[str, Joint] = {}\n        self.links: Dict[str, LinkProperties] = {}\n        self.root_frame: str = "pelvis_link"\n\n        # Transformation from world to root frame\n        self.world_to_root: np.ndarray = np.eye(4)\n\n    def add_joint(self, joint: Joint) -> None:\n        """Add a joint to the kinematic chain."""\n        self.joints[joint.name] = joint\n        self.links[joint.child_link] = joint.properties\n\n    def add_standard_humanoid_joints(self) -> None:\n        """\n        Add standard humanoid joint configuration.\n\n        This creates a typical humanoid with:\n        - 3-DOF legs (hip, knee, ankle)\n        - 3-DOF arms (shoulder, elbow, wrist)\n        - 2-DOF torso (pitch, yaw)\n        - 2-DOF neck (pitch, yaw)\n        """\n        # Left Leg Joints (hip roll/pitch, knee pitch, ankle pitch/roll)\n        self.add_joint(create_hip_joint("left_hip_roll", -0.12))\n        self.add_joint(create_hip_joint("left_hip_pitch", -0.12))\n        self.add_joint(create_knee_joint("left_knee", -0.12))\n        self.add_joint(create_ankle_joint("left_ankle", -0.12))\n\n        # Right Leg Joints\n        self.add_joint(create_hip_joint("right_hip_roll", 0.12))\n        self.add_joint(create_hip_joint("right_hip_pitch", 0.12))\n        self.add_joint(create_knee_joint("right_knee", 0.12))\n        self.add_joint(create_ankle_joint("right_ankle", 0.12))\n\n        # Torso Joints\n        self.add_joint(create_torso_joint("waist_pitch"))\n        self.add_joint(create_torso_joint("waist_yaw"))\n\n        # Arm Joints\n        self.add_joint(create_arm_joint("left_shoulder_pitch", -0.15, 0.35))\n        self.add_joint(create_arm_joint("left_elbow", -0.15, 0.35))\n        self.add_joint(create_arm_joint("right_shoulder_pitch", 0.15, 0.35))\n        self.add_joint(create_arm_joint("right_elbow", 0.15, 0.35))\n\n    def get_link_names(self) -> List[str]:\n        """Get list of all link names in the kinematic chain."""\n        return list(self.links.keys())\n\n    def get_joint_names(self) -> List[str]:\n        """Get list of all joint names."""\n        return list(self.joints.keys())\n\n    def get_dof(self) -> int:\n        """Get total degrees of freedom."""\n        return len(self.joints)\n\n\ndef create_hip_joint(name: str, y_offset: float) -> Joint:\n    """Create a hip joint with standard parameters."""\n    return Joint(\n        name=name,\n        joint_type=JointType.REVOLUTE,\n        axis=np.array([1.0, 0.0, 0.0]) if "roll" in name else np.array([0.0, 1.0, 0.0]),\n        parent_link="pelvis_link",\n        child_link=name.replace("hip", "hip_") + "_link",\n        dh_params=DHParameter(theta=0.0, d=y_offset, a=0.0, alpha=-np.pi/2),\n        limits=JointLimits(\n            position_min=-1.0 if "pitch" in name else -0.5,\n            position_max=1.0 if "pitch" in name else 0.5,\n            velocity_max=5.0,\n            effort_max=150.0\n        ),\n        properties=LinkProperties(\n            mass=2.5,\n            com=np.array([0.0, 0.0, -0.05]),\n            inertia=np.diag([0.01, 0.02, 0.01])\n        )\n    )\n\n\ndef create_knee_joint(name: str, y_offset: float) -> Joint:\n    """Create a knee joint with standard parameters."""\n    return Joint(\n        name=name,\n        joint_type=JointType.REVOLUTE,\n        axis=np.array([1.0, 0.0, 0.0]),  # Pitch only\n        parent_link=name.replace("knee", "hip_") + "_link",\n        child_link="lower_leg_link",\n        dh_params=DHParameter(theta=0.0, d=0.0, a=-0.45, alpha=np.pi/2),\n        limits=JointLimits(\n            position_min=-2.0,\n            position_max=0.0,\n            velocity_max=8.0,\n            effort_max=100.0\n        ),\n        properties=LinkProperties(\n            mass=3.0,\n            com=np.array([0.0, 0.0, -0.25]),\n            inertia=np.diag([0.03, 0.01, 0.03])\n        )\n    )\n\n\ndef create_ankle_joint(name: str, y_offset: float) -> Joint:\n    """Create an ankle joint with standard parameters."""\n    return Joint(\n        name=name,\n        joint_type=JointType.REVOLUTE,\n        axis=np.array([1.0, 0.0, 0.0]) if "pitch" in name else np.array([0.0, 1.0, 0.0]),\n        parent_link="lower_leg_link",\n        child_link="foot_link",\n        dh_params=DHParameter(theta=0.0, d=0.0, a=-0.10, alpha=-np.pi/2),\n        limits=JointLimits(\n            position_min=-0.5,\n            position_max=0.5,\n            velocity_max=6.0,\n            effort_max=80.0\n        ),\n        properties=LinkProperties(\n            mass=1.5,\n            com=np.array([0.05, 0.0, -0.02]),\n            inertia=np.diag([0.01, 0.02, 0.01])\n        )\n    )\n\n\ndef create_torso_joint(name: str) -> Joint:\n    """Create a torso/waist joint."""\n    return Joint(\n        name=name,\n        joint_type=JointType.REVOLUTE,\n        axis=np.array([1.0, 0.0, 0.0]) if "pitch" in name else np.array([0.0, 0.0, 1.0]),\n        parent_link="pelvis_link",\n        child_link="torso_link",\n        dh_params=DHParameter(theta=0.0, d=0.3, a=0.0, alpha=0.0),\n        limits=JointLimits(\n            position_min=-1.0,\n            position_max=1.0,\n            velocity_max=3.0,\n            effort_max=100.0\n        ),\n        properties=LinkProperties(\n            mass=10.0,\n            com=np.array([0.0, 0.0, 0.15]),\n            inertia=np.diag([0.1, 0.15, 0.1])\n        )\n    )\n\n\ndef create_arm_joint(name: str, y_offset: float, z_offset: float) -> Joint:\n    """Create an arm joint with standard parameters."""\n    return Joint(\n        name=name,\n        joint_type=JointType.REVOLUTE,\n        axis=np.array([1.0, 0.0, 0.0]) if "pitch" in name or "elbow" in name else np.array([0.0, 0.0, 1.0]),\n        parent_link="torso_link",\n        child_link=name + "_link",\n        dh_params=DHParameter(theta=0.0, d=z_offset, a=0.0 if "elbow" in name else -0.3,\n                              alpha=-np.pi/2 if "elbow" in name else 0.0),\n        limits=JointLimits(\n            position_min=-3.0,\n            position_max=3.0,\n            velocity_max=6.0,\n            effort_max=50.0\n        ),\n        properties=LinkProperties(\n            mass=2.0,\n            com=np.array([0.0, -0.15, 0.0]) if "shoulder" in name else np.array([0.0, -0.2, 0.0]),\n            inertia=np.diag([0.02, 0.01, 0.02])\n        )\n    )\n\n\nif __name__ == "__main__":\n    # Example usage\n    chain = HumanoidKinematicChain("unitree_h1")\n    chain.add_standard_humanoid_joints()\n\n    print(f"Robot: {chain.robot_name}")\n    print(f"Total DOF: {chain.get_dof()}")\n    print(f"Links: {chain.get_link_names()}")\n    print(f"Joints: {chain.get_joint_names()}")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"coordinate-frame-conventions",children:"Coordinate Frame Conventions"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots typically use multiple coordinate frame conventions simultaneously. The world frame provides a global reference for navigation and positioning. The pelvis frame serves as the robot's local reference, moving with the robot during locomotion. End-effector frames are defined for hands and feet to facilitate manipulation and footstep planning. Consistent coordinate conventions are critical for algorithms that span multiple frames, such as visual servoing or whole-body coordination."}),"\n",(0,o.jsx)(e.p,{children:"The NED (North-East-Down) convention is common for ground robots, with Z pointing upward (opposite gravity). However, many ROS-based systems use ENU (East-North-Up) where Z points upward. The Unitree robots follow the standard robotics convention with Z upward, simplifying integration with ROS tools and libraries."}),"\n",(0,o.jsx)(e.h2,{id:"52-forward-and-inverse-kinematics",children:"5.2 Forward and Inverse Kinematics"}),"\n",(0,o.jsx)(e.p,{children:"Forward kinematics computes the position and orientation of each link given the joint angles. This is computationally straightforward, involving sequential matrix multiplications from the base frame to the target link. Inverse kinematics reverses this process: given a desired end-effector pose, compute the required joint angles. Inverse kinematics is computationally challenging due to potential multiple solutions, singularities, and reachability constraints."}),"\n",(0,o.jsx)(e.h3,{id:"forward-kinematics-implementation",children:"Forward Kinematics Implementation"}),"\n",(0,o.jsx)(e.p,{children:"The forward kinematics for a humanoid robot follows the chain of transformations from the pelvis through each joint to the target link. For a leg with hip, knee, and ankle, the foot position depends on all three joint angles. The transformation at each joint is computed using the joint angle and DH parameters."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nForward Kinematics for Humanoid Robot\n\nThis module implements forward kinematics using homogeneous\ntransformations and the recursive Newton-Euler algorithm.\n"""\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Pose:\n    """SE(3) pose representation."""\n    position: np.ndarray  # 3D position [x, y, z]\n    orientation: np.ndarray  # Quaternion [w, x, y, z]\n\n    @classmethod\n    def identity(cls) -> "Pose":\n        """Create identity pose at origin."""\n        return cls(\n            position=np.array([0.0, 0.0, 0.0]),\n            orientation=np.array([1.0, 0.0, 0.0, 0.0])\n        )\n\n    def to_transform(self) -> np.ndarray:\n        """Convert pose to 4x4 homogeneous transformation matrix."""\n        q = self.orientation\n        # Normalize quaternion\n        q = q / np.linalg.norm(q)\n\n        w, x, y, z = q\n\n        # Rotation matrix from quaternion\n        R = np.array([\n            [1 - 2*(y*y + z*z), 2*(x*y - w*z),     2*(x*z + w*y)],\n            [2*(x*y + w*z),     1 - 2*(x*x + z*z), 2*(y*z - w*x)],\n            [2*(x*z - w*y),     2*(y*z + w*x),     1 - 2*(x*x + y*y)]\n        ])\n\n        T = np.eye(4)\n        T[:3, :3] = R\n        T[:3, 3] = self.position\n\n        return T\n\n    @classmethod\n    def from_transform(cls, T: np.ndarray) -> "Pose":\n        """Create pose from 4x4 homogeneous transformation matrix."""\n        R = T[:3, :3]\n        pos = T[:3, 3]\n\n        # Quaternion from rotation matrix\n        q = np.zeros(4)\n        q[0] = 0.5 * np.sqrt(1 + R[0, 0] + R[1, 1] + R[2, 2])\n\n        if q[0] > 1e-6:\n            q[1] = (R[2, 1] - R[1, 2]) / (4 * q[0])\n            q[2] = (R[0, 2] - R[2, 0]) / (4 * q[0])\n            q[3] = (R[1, 0] - R[0, 1]) / (4 * q[0])\n        else:\n            # Near-singular case\n            q[1] = np.sqrt(0.5 * (1 + R[0, 0] - R[1, 1] - R[2, 2]))\n            q[2] = np.sqrt(0.5 * (1 - R[0, 0] + R[1, 1] - R[2, 2]))\n            q[3] = np.sqrt(0.5 * (1 - R[0, 0] - R[1, 1] + R[2, 2]))\n\n        return cls(position=pos, orientation=q)\n\n\nclass ForwardKinematics:\n    """\n    Forward kinematics solver for humanoid robots.\n\n    Computes end-effector positions and orientations given\n    joint angles using homogeneous transformations.\n    """\n\n    def __init__(self):\n        # Robot link lengths (in meters)\n        self.hip_to_knee = 0.45      # Upper leg length (femur)\n        self.knee_to_ankle = 0.45    # Lower leg length (tibia)\n        self.ankle_to_ground = 0.10  # Foot sole thickness\n\n        # Hip offset from pelvis center\n        self.hip_lateral_offset = 0.12\n\n    def compute_foot_pose(\n        self,\n        hip_roll: float,\n        hip_pitch: float,\n        knee_pitch: float,\n        ankle_pitch: float,\n        ankle_roll: float = 0.0,\n        side: str = "left"\n    ) -> Pose:\n        """\n        Compute foot pose from leg joint angles.\n\n        Args:\n            hip_roll: Hip roll angle (rad), positive lifts leg outward\n            hip_pitch: Hip pitch angle (rad), positive lifts leg forward\n            knee_pitch: Knee pitch angle (rad), positive bends knee\n            ankle_pitch: Ankle pitch angle (rad), positive points toe up\n            ankle_roll: Ankle roll angle (rad), positive tilts foot outward\n            side: "left" or "right" leg\n\n        Returns:\n            Pose of the foot relative to hip frame\n        """\n        # Sign based on left/right\n        sign = -1.0 if side == "left" else 1.0\n        y_offset = sign * self.hip_lateral_offset\n\n        # Direction cosine matrix for each joint\n        # Hip roll (rotation about X)\n        R_hip_roll = np.array([\n            [1, 0, 0],\n            [0, np.cos(hip_roll), -np.sin(hip_roll)],\n            [0, np.sin(hip_roll),  np.cos(hip_roll)]\n        ])\n\n        # Hip pitch (rotation about Y)\n        R_hip_pitch = np.array([\n            [np.cos(hip_pitch), 0, np.sin(hip_pitch)],\n            [0, 1, 0],\n            [-np.sin(hip_pitch), 0, np.cos(hip_pitch)]\n        ])\n\n        # Knee pitch (rotation about Y)\n        R_knee = np.array([\n            [np.cos(knee_pitch), 0, np.sin(knee_pitch)],\n            [0, 1, 0],\n            [-np.sin(knee_pitch), 0, np.cos(knee_pitch)]\n        ])\n\n        # Ankle pitch (rotation about Y)\n        R_ankle_pitch = np.array([\n            [np.cos(ankle_pitch), 0, np.sin(ankle_pitch)],\n            [0, 1, 0],\n            [-np.sin(ankle_pitch), 0, np.cos(ankle_pitch)]\n        ])\n\n        # Ankle roll (rotation about X)\n        R_ankle_roll = np.array([\n            [1, 0, 0],\n            [0, np.cos(ankle_roll), -np.sin(ankle_roll)],\n            [0, np.sin(ankle_roll),  np.cos(ankle_roll)]\n        ])\n\n        # Combined rotation from hip to ankle\n        R_total = R_hip_roll @ R_hip_pitch @ R_knee @ R_ankle_pitch @ R_ankle_roll\n\n        # Position computation using geometric approach\n        # Hip joint position (in pelvis frame)\n        hip_pos = np.array([0, y_offset, 0])\n\n        # Knee position (hip + upper leg rotation)\n        upper_leg_dir = R_hip_roll @ R_hip_pitch @ np.array([0, 0, -1])\n        knee_pos = hip_pos + self.hip_to_knee * upper_leg_dir\n\n        # Ankle position (knee + lower leg rotation)\n        lower_leg_dir = R_hip_roll @ R_hip_pitch @ R_knee @ np.array([0, 0, -1])\n        ankle_pos = knee_pos + self.knee_to_ankle * lower_leg_dir\n\n        # Foot position (ankle + foot offset)\n        foot_pos = ankle_pos + R_hip_roll @ R_hip_pitch @ R_knee @ R_ankle_pitch @ np.array([0, 0, -self.ankle_to_ground])\n\n        # Convert rotation matrix to quaternion\n        orientation = self._rotation_to_quaternion(R_total)\n\n        return Pose(position=foot_pos, orientation=orientation)\n\n    def compute_jacobian(\n        self,\n        joint_angles: Dict[str, float],\n        side: str = "left"\n    ) -> np.ndarray:\n        """\n        Compute geometric Jacobian for leg kinematics.\n\n        The Jacobian relates joint velocities to end-effector velocity:\n        v = J * q_dot\n\n        Args:\n            joint_angles: Dictionary of joint angle names to values\n            side: "left" or "right" leg\n\n        Returns:\n            6xN Jacobian matrix (N = number of joints)\n        """\n        # Extract joint angles\n        hip_roll = joint_angles.get(f"{side}_hip_roll", 0.0)\n        hip_pitch = joint_angles.get(f"{side}_hip_pitch", 0.0)\n        knee_pitch = joint_angles.get(f"{side}_knee", 0.0)\n        ankle_pitch = joint_angles.get(f"{side}_ankle_pitch", 0.0)\n        ankle_roll = joint_angles.get(f"{side}_ankle_roll", 0.0)\n\n        # Compute foot position for sensitivity analysis\n        eps = 1e-6\n        base_pose = self.compute_foot_pose(hip_roll, hip_pitch, knee_pitch, ankle_pitch, ankle_roll, side)\n\n        n_joints = 5\n        J = np.zeros((6, n_joints))\n\n        # Numerical differentiation for Jacobian columns\n        joint_names = ["hip_roll", "hip_pitch", "knee", "ankle_pitch", "ankle_roll"]\n\n        for i, jname in enumerate(joint_names):\n            perturbed = joint_angles.copy()\n            perturbed[f"{side}_{jname}"] += eps\n\n            perturbed_pose = self.compute_foot_pose(\n                perturbed.get(f"{side}_hip_roll", 0.0),\n                perturbed.get(f"{side}_hip_pitch", 0.0),\n                perturbed.get(f"{side}_knee", 0.0),\n                perturbed.get(f"{side}_ankle_pitch", 0.0),\n                perturbed.get(f"{side}_ankle_roll", 0.0),\n                side\n            )\n\n            # Linear velocity (delta position / delta angle)\n            J[:3, i] = (perturbed_pose.position - base_pose.position) / eps\n\n            # Angular velocity (using quaternion derivative approximation)\n            q_base = base_pose.orientation\n            q_pert = perturbed_pose.orientation\n            delta_q = q_pert - q_base\n\n            # Convert quaternion difference to angular velocity\n            omega = self._quaternion_diff_to_angular_velocity(q_base, delta_q)\n            J[3:, i] = omega / eps\n\n        return J\n\n    def _rotation_to_quaternion(self, R: np.ndarray) -> np.ndarray:\n        """Convert rotation matrix to quaternion [w, x, y, z]."""\n        trace = np.trace(R)\n\n        if trace > 0:\n            s = 0.5 / np.sqrt(trace + 1.0)\n            w = 0.25 / s\n            x = (R[2, 1] - R[1, 2]) * s\n            y = (R[0, 2] - R[2, 0]) * s\n            z = (R[1, 0] - R[0, 1]) * s\n        else:\n            i = np.argmax(np.diag(R))\n            j = (i + 1) % 3\n            k = (i + 2) % 3\n\n            s = 2.0 * np.sqrt(1.0 + R[i, i] - R[j, j] - R[k, k])\n            q = np.zeros(4)\n            q[i + 1] = 0.5 * s\n            q[0] = (R[k, j] - R[j, k]) / s\n            q[j + 1] = (R[j, i] + R[i, j]) / s\n            q[k + 1] = (R[k, i] + R[i, k]) / s\n            q[0] = (R[j, k] - R[k, j]) / s\n\n            w, x, y, z = q[0], q[1], q[2], q[3]\n\n        return np.array([w, x, y, z])\n\n    def _quaternion_diff_to_angular_velocity(\n        self,\n        q1: np.ndarray,\n        dq: np.ndarray\n    ) -> np.ndarray:\n        """Convert quaternion difference to angular velocity."""\n        # Approximate angular velocity from quaternion error\n        # omega = 2 * q1_conjugate * dq_dt (for small dq)\n        q1_conj = np.array([q1[0], -q1[1], -q1[2], -q1[3]])\n\n        # Quaternion multiplication\n        w = q1_conj[0] * dq[0] - q1_conj[1] * dq[1] - q1_conj[2] * dq[2] - q1_conj[3] * dq[3]\n        x = q1_conj[0] * dq[1] + q1_conj[1] * dq[0] + q1_conj[2] * dq[3] - q1_conj[3] * dq[2]\n        y = q1_conj[0] * dq[2] - q1_conj[1] * dq[3] + q1_conj[2] * dq[0] + q1_conj[3] * dq[1]\n        z = q1_conj[0] * dq[3] + q1_conj[1] * dq[2] - q1_conj[2] * dq[1] + q1_conj[3] * dq[0]\n\n        return 2.0 * np.array([x, y, z])\n\n\ndef compute_reachability(\n    fk: ForwardKinematics,\n    hip_pitch_range: Tuple[float, float] = (-1.0, 1.0),\n    knee_range: Tuple[float, float] = (-2.0, 0.0),\n    n_samples: int = 1000\n) -> Tuple[np.ndarray, float]:\n    """\n    Compute reachable workspace for a leg.\n\n    Args:\n        fk: Forward kinematics solver\n        hip_pitch_range: Range of hip pitch angles\n        knee_range: Range of knee pitch angles\n        n_samples: Number of samples\n\n    Returns:\n        Tuple of (reachable points array, max reach distance)\n    """\n    reachable_points = []\n    max_reach = 0.0\n\n    for _ in range(n_samples):\n        hip_pitch = np.random.uniform(*hip_pitch_range)\n        knee = np.random.uniform(*knee_range)\n\n        # Simple walking gait with ankle compensation\n        ankle_pitch = -(hip_pitch + knee) / 2\n\n        pose = fk.compute_foot_pose(\n            hip_roll=0.0,\n            hip_pitch=hip_pitch,\n            knee_pitch=knee,\n            ankle_pitch=ankle_pitch\n        )\n\n        reachable_points.append(pose.position)\n        dist = np.linalg.norm(pose.position)\n        if dist > max_reach:\n            max_reach = dist\n\n    return np.array(reachable_points), max_reach\n\n\nif __name__ == "__main__":\n    # Example usage\n    fk = ForwardKinematics()\n\n    # Test forward kinematics\n    print("Forward Kinematics Test")\n    print("=" * 50)\n\n    # Neutral stance\n    pose = fk.compute_foot_pose(\n        hip_roll=0.0,\n        hip_pitch=0.0,\n        knee_pitch=0.0,\n        ankle_pitch=0.0,\n        side="left"\n    )\n    print(f"Neutral left foot position: {pose.position}")\n\n    # Walking stance (knee bent)\n    pose = fk.compute_foot_pose(\n        hip_roll=0.0,\n        hip_pitch=0.3,\n        knee_pitch=-0.8,\n        ankle_pitch=0.25,\n        side="left"\n    )\n    print(f"Walking left foot position: {pose.position}")\n\n    # Compute Jacobian at neutral\n    joint_angles = {\n        "left_hip_roll": 0.0,\n        "left_hip_pitch": 0.0,\n        "left_knee": 0.0,\n        "left_ankle_pitch": 0.0,\n        "left_ankle_roll": 0.0\n    }\n    J = fk.compute_jacobian(joint_angles, side="left")\n    print(f"\\nJacobian at neutral pose:")\n    print(J.round(3))\n\n    # Reachability analysis\n    points, max_reach = compute_reachability(fk, n_samples=500)\n    print(f"\\nMax leg reach: {max_reach:.3f} m")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"inverse-kinematics-approaches",children:"Inverse Kinematics Approaches"}),"\n",(0,o.jsx)(e.p,{children:"Inverse kinematics for humanoid legs can be solved using geometric, analytical, or numerical methods. Geometric methods exploit the planar nature of leg kinematics, solving for joint angles using trigonometry. Analytical methods derive closed-form solutions that are computationally efficient. Numerical methods like Jacobian pseudoinverse or optimization-based approaches handle more complex chains but require iteration."}),"\n",(0,o.jsx)(e.p,{children:"For a 5-DOF leg, analytical solutions typically exist by decomposing the problem into planar and lateral components. The hip roll and ankle roll control frontal plane motion, while hip pitch, knee pitch, and ankle pitch work together in the sagittal plane. This decoupling simplifies the solution and improves computational efficiency for real-time control."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nInverse Kinematics for Humanoid Robot Legs\n\nThis module implements analytical and numerical inverse kinematics\nfor humanoid leg control.\n"""\n\nimport numpy as np\nfrom typing import Tuple, Optional, Dict\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass IKResult:\n    """Result of inverse kinematics computation."""\n    success: bool\n    joint_angles: Dict[str, float]\n    error: float\n    iterations: int\n    computation_time: float\n\n\nclass InverseKinematics:\n    """\n    Analytical and numerical inverse kinematics solver.\n\n    Uses geometric decomposition for efficient leg IK with\n    fallback to numerical optimization for complex cases.\n    """\n\n    def __init__(self):\n        # Link lengths\n        self.L1 = 0.45  # Hip to knee (upper leg)\n        self.L2 = 0.45  # Knee to ankle (lower leg)\n        self.L_foot = 0.10  # Ankle to ground\n\n    def solve_leg_ik(\n        self,\n        target_foot_pos: np.ndarray,\n        hip_offset: float,\n        side: str = "left"\n    ) -> IKResult:\n        """\n        Solve inverse kinematics for a leg to reach target foot position.\n\n        Uses analytical solution with geometric decomposition.\n\n        Args:\n            target_foot_pos: Target foot position [x, y, z] in hip frame\n            hip_offset: Lateral offset of hip from pelvis center\n            side: "left" or "right"\n\n        Returns:\n            IKResult with joint angles or failure indication\n        """\n        import time\n        start_time = time.time()\n\n        sign = -1.0 if side == "left" else 1.0\n\n        # Extract components\n        x = target_foot_pos[0]  # Forward\n        y = target_foot_pos[1] - sign * hip_offset  # Lateral (relative to hip)\n        z = target_foot_pos[2]  # Vertical\n\n        # Hip roll - controls lateral foot position\n        # Solve: y = -L1*sin(hip_roll) - L2*sin(hip_roll + knee + ankle)\n        # Simplified: assume knee and ankle don\'t contribute much to lateral\n        hip_roll = np.arcsin(np.clip(-y / (self.L1 + self.L2), -1.0, 1.0))\n\n        # Frontal plane projection\n        # xz plane distance from hip to foot\n        r_frustal = np.sqrt(x**2 + z**2)\n\n        # Check reachability\n        min_reach = abs(self.L1 - self.L2) + self.L_foot\n        max_reach = self.L1 + self.L2 + self.L_foot\n        if r_frustal > max_reach or r_frustal < min_reach:\n            return IKResult(\n                success=False,\n                joint_angles={},\n                error=r_frustal - max_reach,\n                iterations=0,\n                computation_time=time.time() - start_time\n            )\n\n        # Sagittal plane IK (2-link arm problem)\n        # Account for foot offset\n        r = np.sqrt(x**2 + (z + self.L_foot)**2)\n\n        # Law of cosines for knee angle\n        # cos(knee_angle) = (L1^2 + L2^2 - r^2) / (2*L1*L2)\n        cos_knee = (self.L1**2 + self.L2**2 - r**2) / (2 * self.L1 * self.L2)\n        knee_pitch = np.arccos(np.clip(cos_knee, -1.0, 1.0))\n\n        # Hip pitch angle\n        # Angle from horizontal to L1\n        alpha = np.arctan2(-(z + self.L_foot), x)\n        # Angle between L1 and r\n        beta = np.arccos(np.clip((self.L1**2 + r**2 - self.L2**2) / (2 * self.L1 * r), -1.0, 1.0))\n        hip_pitch = alpha + beta\n\n        # Ankle pitch - keep foot flat\n        # Sum of joint angles should equal angle from ankle to horizontal\n        # ankle_pitch = -(hip_pitch + knee_pitch + alpha)\n        ankle_pitch = -np.arctan2(x, z + self.L_foot)\n\n        # Foot roll (lateral balance)\n        ankle_roll = -hip_roll * 0.5  # Partial compensation\n\n        joint_angles = {\n            f"{side}_hip_roll": hip_roll,\n            f"{side}_hip_pitch": hip_pitch,\n            f"{side}_knee": -knee_pitch,  # Negative for human-like knee bend\n            f"{side}_ankle_pitch": ankle_pitch,\n            f"{side}_ankle_roll": ankle_roll\n        }\n\n        # Clip to limits\n        for name in joint_angles:\n            if "knee" in name:\n                joint_angles[name] = np.clip(joint_angles[name], -2.0, 0.0)\n            elif "hip_roll" in name or "ankle_roll" in name:\n                joint_angles[name] = np.clip(joint_angles[name], -0.5, 0.5)\n            else:\n                joint_angles[name] = np.clip(joint_angles[name], -1.5, 1.5)\n\n        return IKResult(\n            success=True,\n            joint_angles=joint_angles,\n            error=0.0,\n            iterations=1,\n            computation_time=time.time() - start_time\n        )\n\n    def solve_walking_ik(\n        self,\n        left_foot_target: np.ndarray,\n        right_foot_target: np.ndarray,\n        pelvis_height: float = 0.95,\n        hip_offset: float = 0.12\n    ) -> Dict[str, float]:\n        """\n        Solve IK for both legs in walking configuration.\n\n        Args:\n            left_foot_target: Target position for left foot\n            right_foot_target: Target position for right foot\n            pelvis_height: Height of pelvis above ground\n            hip_offset: Lateral hip offset\n\n        Returns:\n            Dictionary of all joint angles\n        """\n        left_result = self.solve_leg_ik(\n            left_foot_target - np.array([0, 0, pelvis_height]),\n            hip_offset,\n            "left"\n        )\n\n        right_result = self.solve_leg_ik(\n            right_foot_target - np.array([0, 0, pelvis_height]),\n            hip_offset,\n            "right"\n        )\n\n        if not left_result.success or not right_result.success:\n            raise ValueError("IK solution not feasible")\n\n        # Combine solutions\n        all_joints = {}\n        all_joints.update(left_result.joint_angles)\n        all_joints.update(right_result.joint_angles)\n\n        # Add torso orientation (keep level)\n        all_joints["waist_pitch"] = 0.0\n        all_joints["waist_yaw"] = 0.0\n\n        return all_joints\n\n\nclass WalkingTrajectoryGenerator:\n    """\n    Generates walking trajectories for humanoid robots.\n\n    Creates footstep plans and corresponding joint trajectories\n    for stable bipedal locomotion.\n    """\n\n    def __init__(self, step_length: float = 0.3, step_height: float = 0.08):\n        self.step_length = step_length\n        self.step_height = step_height\n        self.step_width = 0.15  # Distance between feet\n\n    def generate_step_sequence(\n        self,\n        n_steps: int,\n        walk_direction: float = 0.0,  # radians\n        initial_side: str = "right"\n    ) -> list:\n        """\n        Generate a sequence of footsteps.\n\n        Args:\n            n_steps: Number of steps to generate\n            walk_direction: Walking direction angle (radians)\n            initial_side: Which foot is forward at start\n\n        Returns:\n            List of footstep positions and swing timing\n        """\n        footsteps = []\n        side = initial_side\n\n        for i in range(n_steps):\n            # Alternating foot pattern\n            x = self.step_length * i\n            y = self.step_width if side == "right" else -self.step_width\n\n            # Apply rotation for direction\n            cos_d = np.cos(walk_direction)\n            sin_d = np.sin(walk_direction)\n            x_rot = x * cos_d - y * sin_d\n            y_rot = x * sin_d + y * cos_d\n\n            # Swing timing (normalized)\n            t_swing_start = 0.5 * i\n            t_swing_end = 0.5 * i + 0.4  # 40% of step cycle\n\n            footsteps.append({\n                "position": np.array([x_rot, y_rot, 0.0]),\n                "side": side,\n                "swing_start": t_swing_start,\n                "swing_end": t_swing_end,\n                "lift_height": self.step_height if i > 0 else 0.0\n            })\n\n            side = "right" if side == "left" else "left"\n\n        return footsteps\n\n    def generate_swing_trajectory(\n        self,\n        start_pos: np.ndarray,\n        end_pos: np.ndarray,\n        t: float,\n        lift_height: float = 0.08\n    ) -> np.ndarray:\n        """\n        Generate foot trajectory during swing phase.\n\n        Uses parabolic trajectory for smooth foot movement.\n\n        Args:\n            start_pos: Starting foot position\n            end_pos: Ending foot position\n            t: Normalized time [0, 1] in swing phase\n            lift_height: Maximum foot clearance\n\n        Returns:\n            Foot position at time t\n        """\n        # Linear interpolation in XY\n        x = start_pos[0] + t * (end_pos[0] - start_pos[0])\n        y = start_pos[1] + t * (end_pos[1] - start_pos[1])\n\n        # Parabolic height profile\n        # z = 4 * h * t * (1-t) gives peak at t=0.5\n        z = start_pos[2] + 4 * lift_height * t * (1 - t)\n\n        return np.array([x, y, z])\n\n\nif __name__ == "__main__":\n    # Test inverse kinematics\n    ik = InverseKinematics()\n\n    print("Inverse Kinematics Test")\n    print("=" * 50)\n\n    # Test standing position\n    result = ik.solve_leg_ik(\n        target_foot_pos=np.array([0.0, -0.12, -0.95]),\n        hip_offset=0.12,\n        side="left"\n    )\n    print(f"Standing left leg: {result.success}")\n    print(f"Iterations: {result.iterations}")\n    print(f"Computation time: {result.computation_time * 1000:.2f} ms")\n    for name, angle in result.joint_angles.items():\n        print(f"  {name}: {np.degrees(angle):.1f} degrees")\n\n    # Test stepping position\n    result = ik.solve_leg_ik(\n        target_foot_pos=np.array([0.3, -0.12, -0.85]),\n        hip_offset=0.12,\n        side="left"\n    )\n    print(f"\\nStepping left leg: {result.success}")\n    for name, angle in result.joint_angles.items():\n        print(f"  {name}: {np.degrees(angle):.1f} degrees")\n\n    # Test walking trajectory generation\n    generator = WalkingTrajectoryGenerator()\n    footsteps = generator.generate_step_sequence(n_steps=4)\n    print(f"\\nGenerated {len(footsteps)} footsteps")\n\n    # Test swing trajectory\n    start = np.array([0.0, -0.12, 0.0])\n    end = np.array([0.3, -0.12, 0.0])\n    for t in [0.0, 0.25, 0.5, 0.75, 1.0]:\n        pos = generator.generate_swing_trajectory(start, end, t)\n        print(f"Swing t={t:.2f}: z={pos[2]:.3f} m")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"53-bipedal-locomotion-and-balance-control",children:"5.3 Bipedal Locomotion and Balance Control"}),"\n",(0,o.jsx)(e.p,{children:"Bipedal locomotion represents one of the most challenging problems in robotics due to the inherent instability of two-point contact. Unlike wheeled robots that are inherently stable, bipedal robots must continuously balance while transitioning between contact states. The control of walking requires coordinating multiple subsystems: trajectory generation, balance control, foot placement, and disturbance rejection."}),"\n",(0,o.jsx)(e.h3,{id:"zero-moment-point-and-stability",children:"Zero Moment Point and Stability"}),"\n",(0,o.jsx)(e.p,{children:"The Zero Moment Point (ZMP) is the fundamental concept for stable bipedal walking. The ZMP is the point on the ground where the total moment from gravitational and inertial forces equals zero. For stable walking, the ZMP must remain within the support polygon defined by the contact area between the feet and the ground."}),"\n",(0,o.jsx)(e.p,{children:"Modern walking controllers extend beyond static ZMP to incorporate preview control, model-predictive control, and learning-based approaches. The Unitree H1 robot uses a combination of simplified dynamics models and real-time optimization to achieve stable walking at speeds up to 2 m/s."}),"\n",(0,o.jsx)(e.h3,{id:"walking-controller-implementation",children:"Walking Controller Implementation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nBipedal Walking Controller for Humanoid Robots\n\nThis module implements a complete walking controller including\nZMP-based balance, footstep planning, and trajectory generation.\n"""\n\nimport numpy as np\nfrom typing import Dict, Tuple, Optional, List\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport time\n\n\nclass GaitType(Enum):\n    """Types of walking gaits."""\n    STANDING = "standing"\n    WALKING = "walking"\n    TROTTING = "trotting"\n    RUNNING = "running"\n\n\n@dataclass\nclass WalkingParameters:\n    """Parameters for walking controller."""\n    # Timing\n    step_duration: float = 0.4        # Time per step (seconds)\n    double_support_ratio: float = 0.2 # Fraction in double support\n    sample_period: float = 0.005      # Control period (seconds)\n\n    # Geometry\n    step_length: float = 0.25         # Forward step size (meters)\n    step_width: float = 0.12          # Lateral foot separation (meters)\n    foot_length: float = 0.25         # Foot length (meters)\n    foot_width: float = 0.12          # Foot width (meters)\n\n    # Motion limits\n    max_step_length: float = 0.35     # Maximum forward step\n    max_lateral_step: float = 0.15    # Maximum lateral step\n    max_turn_rate: float = 0.5        # Maximum turning (rad/s)\n\n    # Balance parameters\n    com_height: float = 0.85          # Center of mass height\n    zmp_margin: float = 0.02          # Safety margin for ZMP\n    preview_horizon: int = 16         # Preview steps for MPC\n\n    # Tracking gains\n    com_kp: float = 10.0              # CoM position gain\n    com_kd: float = 3.0               # CoM velocity gain\n    orientation_kp: float = 20.0      # Orientation tracking gain\n    orientation_kd: float = 5.0       # Orientation rate gain\n\n\n@dataclass\nclass RobotState:\n    """Current state of the robot."""\n    # Base position and orientation\n    position: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0, 0.95]))\n    orientation: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0, 0.0, 1.0]))\n\n    # Base velocity (world frame)\n    linear_velocity: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0, 0.0]))\n    angular_velocity: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0, 0.0]))\n\n    # Joint states\n    joint_positions: Dict[str, float] = field(default_factory=dict)\n    joint_velocities: Dict[str, float] = field(default_factory=dict)\n\n    # Contact states\n    left_foot_contact: bool = True\n    right_foot_contact: bool = True\n\n    # ZMP in world frame\n    left_foot_zmp: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0]))\n    right_foot_zmp: np.ndarray = field(default_factory=lambda: np.array([0.0, 0.0]))\n\n\n@dataclass\nclass WalkingCommand:\n    """Command input to walking controller."""\n    forward_velocity: float = 0.0     # m/s\n    lateral_velocity: float = 0.0     # m/s\n    turn_rate: float = 0.0           # rad/s\n    gait_type: GaitType = GaitType.WALKING\n    emergency_stop: bool = False\n\n\nclass WalkingController:\n    """\n    Complete bipedal walking controller.\n\n    Implements ZMP-based balance control, footstep planning,\n    and trajectory tracking for stable humanoid walking.\n    """\n\n    def __init__(self, params: Optional[WalkingParameters] = None):\n        self.params = params or WalkingParameters()\n\n        # State\n        self.state = RobotState()\n        self.command = WalkingCommand()\n\n        # Footstep plan\n        self.footstep_plan: List[Dict] = []\n        self.current_step_idx: int = 0\n        self.step_start_time: float = 0.0\n\n        # Initialize foot positions\n        self.left_foot_pos = np.array([-0.02, self.params.step_width / 2, 0.0])\n        self.right_foot_pos = np.array([-0.02, -self.params.step_width / 2, 0.0])\n\n        # Inverted pendulum model for balance\n        self.Z = self.params.com_height / 9.81  # Pendulum time constant\n\n        # Trajectory buffers\n        self.trajectory_buffer: Dict[str, np.ndarray] = {}\n\n    def reset(self, initial_state: Optional[RobotState] = None) -> None:\n        """Reset controller to initial state."""\n        if initial_state is not None:\n            self.state = initial_state\n\n        self.footstep_plan = []\n        self.current_step_idx = 0\n        self.step_start_time = 0.0\n\n        # Reset foot positions\n        self.left_foot_pos = np.array([0.0, self.params.step_width / 2, 0.0])\n        self.right_foot_pos = np.array([0.0, -self.params.step_width / 2, 0.0])\n\n    def set_command(self, command: WalkingCommand) -> None:\n        """Update walking command."""\n        self.command = command\n\n    def plan_footsteps(self, n_steps: int = 10) -> None:\n        """\n        Plan future footstep positions based on command.\n\n        Uses velocity commands to predict future foot placements.\n        """\n        self.footstep_plan = []\n\n        # Determine swing foot\n        swing_foot = "left" if self.current_step_idx % 2 == 0 else "right"\n\n        for i in range(n_steps):\n            # Step number from current\n            step_num = self.current_step_idx + i\n\n            # Base position (continue pattern)\n            if step_num == 0:\n                # First step - place swing foot\n                target_pos = self.right_foot_pos.copy() if swing_foot == "right" else self.left_foot_pos.copy()\n                target_pos[0] += self.command.forward_velocity * self.params.step_duration\n                target_pos[1] += self.command.lateral_velocity * self.params.step_duration\n            else:\n                # Subsequent steps\n                prev_foot = self.right_foot_pos if swing_foot == "left" else self.left_foot_pos\n                target_pos = prev_foot.copy()\n                target_pos[0] += self.command.forward_velocity * self.params.step_duration\n                target_pos[1] += self.command.lateral_velocity * self.params.step_duration\n\n                # Apply turning\n                if abs(self.command.turn_rate) > 1e-3:\n                    # Arc path for turning\n                    radius = self.command.forward_velocity / (self.command.turn_rate + 1e-6)\n                    angle = self.command.turn_rate * self.params.step_duration\n\n                    # Transform\n                    dx = target_pos[0] - 0  # Pivot around origin\n                    dy = target_pos[1] - radius * (1 - np.cos(angle) if radius > 0 else 0)\n\n                    cos_a = np.cos(angle)\n                    sin_a = np.sin(angle)\n\n                    target_pos[0] = dx * cos_a - dy * sin_a + radius * sin_a\n                    target_pos[1] = dx * sin_a + dy * cos_a - radius * (1 - cos_a) if radius > 0 else dy\n\n            # Clamp to limits\n            target_pos[0] = np.clip(\n                target_pos[0],\n                -self.params.max_step_length,\n                self.params.max_step_length\n            )\n            target_pos[1] = np.clip(\n                target_pos[1],\n                -self.params.max_lateral_step,\n                self.params.max_lateral_step\n            )\n\n            self.footstep_plan.append({\n                "position": target_pos,\n                "swing_foot": swing_foot,\n                "step_num": step_num\n            })\n\n            # Alternate swing foot\n            swing_foot = "right" if swing_foot == "left" else "left"\n\n    def compute_zmp_trajectory(self) -> np.ndarray:\n        """\n        Compute desired ZMP trajectory based on footstep plan.\n\n        Uses linear ZMP preview over the support polygon.\n        """\n        # Desired ZMP trajectory (N x 2 array)\n        n_preview = self.params.preview_horizon\n        zmp_trajectory = np.zeros((n_preview, 2))\n\n        # Get current support foot\n        support_foot = "right" if self.current_step_idx % 2 == 0 else "left"\n        support_pos = self.right_foot_pos if support_foot == "right" else self.left_foot_pos\n\n        for i in range(n_preview):\n            step_idx = min(self.current_step_idx + i, len(self.footstep_plan) - 1)\n            step = self.footstep_plan[step_idx]\n\n            # ZMP starts at support foot center\n            zmp_trajectory[i, :2] = support_pos[:2]\n\n            # During swing, ZMP moves toward swing foot\n            swing_foot = step["swing_foot"]\n            swing_pos = self.left_foot_pos if swing_foot == "left" else self.right_foot_pos\n\n            # Normalized phase in step\n            phase = (i + 1) / n_preview\n\n            if phase < self.params.double_support_ratio:\n                # Double support - ZMP moves between feet\n                zmp_trajectory[i, :2] = (\n                    (1 - phase / self.params.double_support_ratio) * support_pos[:2] +\n                    (phase / self.params.double_support_ratio) * swing_pos[:2]\n                )\n            else:\n                # Single support on swing foot\n                zmp_trajectory[i, :2] = swing_pos[:2]\n\n        return zmp_trajectory\n\n    def compute_com_reference(self, t: float) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Compute reference CoM position and velocity.\n\n        Uses simplified linear inverted pendulum model.\n        """\n        # Get ZMP target\n        zmp_desired = self.compute_zmp_trajectory()[0]\n\n        # Linear inverted pendulum dynamics\n        # x_ddot = (g/z_c) * (x - x_zmp)\n        # Solution: x(t) = x_zmp + (x0 - x_zmp) * cosh(t/tau)\n        # where tau = sqrt(z_c / g)\n\n        tau = np.sqrt(self.params.com_height / 9.81)\n        exp_t = np.exp(t / tau)\n\n        # Reference CoM position\n        com_ref = np.array([\n            zmp_desired[0] + (self.state.position[0] - zmp_desired[0]) * exp_t,\n            zmp_desired[1] + (self.state.position[1] - zmp_desired[1]) * exp_t,\n            self.params.com_height\n        ])\n\n        # Reference CoM velocity\n        com_vel_ref = np.array([\n            (self.state.position[0] - zmp_desired[0]) * exp_t / tau,\n            (self.state.position[1] - zmp_desired[1]) * exp_t / tau,\n            0.0\n        ])\n\n        return com_ref, com_vel_ref\n\n    def compute_balance_correction(\n        self,\n        com_error: np.ndarray,\n        com_vel_error: np.ndarray,\n        orientation_error: np.ndarray,\n        angular_vel_error: np.ndarray\n    ) -> np.ndarray:\n        """\n        Compute balance correction forces/torques.\n\n        Uses PD control on CoM and orientation errors.\n        """\n        # CoM balance correction\n        f_com = np.zeros(3)\n        f_com[:2] = (\n            self.params.com_kp * com_error[:2] +\n            self.params.com_kd * com_vel_error[:2]\n        )\n\n        # Orientation correction (torque)\n        tau_orientation = (\n            self.params.orientation_kp * orientation_error +\n            self.params.orientation_kd * angular_vel_error\n        )\n\n        return np.concatenate([f_com, tau_orientation])\n\n    def generate_foot_trajectory(\n        self,\n        swing_foot: str,\n        start_pos: np.ndarray,\n        end_pos: np.ndarray,\n        t: float\n    ) -> np.ndarray:\n        """\n        Generate trajectory for swinging foot.\n\n        Args:\n            swing_foot: Which foot is swinging\n            start_pos: Starting foot position\n            end_pos: Ending foot position\n            t: Normalized time [0, 1] in swing phase\n\n        Returns:\n            Foot position at time t\n        """\n        # Linear interpolation in XY\n        x = start_pos[0] + t * (end_pos[0] - start_pos[0])\n        y = start_pos[1] + t * (end_pos[1] - start_pos[1])\n\n        # Parabolic lift for clearance\n        # Maximum height at t=0.5\n        lift = 4 * self.params.step_height * t * (1 - t)\n\n        z = start_pos[2] + lift\n\n        return np.array([x, y, z])\n\n    def step(self, dt: float) -> Dict[str, np.ndarray]:\n        """\n        Execute one control step.\n\n        Args:\n            dt: Time since last control step\n\n        Returns:\n            Joint command dictionary\n        """\n        # Emergency stop\n        if self.command.emergency_stop:\n            return self._generate_stand_command()\n\n        # Update state estimate (placeholder for actual sensor fusion)\n        self._update_state_estimate()\n\n        # Update footstep plan\n        if len(self.footstep_plan) < 5:\n            self.plan_footsteps()\n\n        # Get current step info\n        current_step = self.footstep_plan[0]\n        swing_foot = current_step["swing_foot"]\n        swing_target = current_step["position"]\n\n        # Determine support foot\n        support_foot = "right" if swing_foot == "left" else "left"\n        support_pos = self.right_foot_pos if support_foot == "right" else self.left_foot_pos\n\n        # Normalized phase in step\n        elapsed = time.time() - self.step_start_time\n        phase = min(elapsed / self.params.step_duration, 1.0)\n\n        # Compute reference trajectories\n        com_ref, com_vel_ref = self.compute_com_reference(phase)\n\n        # Compute errors\n        com_error = com_ref - self.state.position\n        com_vel_error = com_vel_ref - self.state.linear_velocity\n\n        # Orientation error (simplified)\n        orientation_error = np.zeros(3)\n\n        # Compute balance correction\n        balance_correction = self.compute_balance_correction(\n            com_error, com_vel_error,\n            orientation_error, self.state.angular_velocity\n        )\n\n        # Generate foot positions based on phase\n        if phase < self.params.double_support_ratio:\n            # Double support phase\n            # Both feet on ground, shift weight\n            left_target = support_pos if support_foot == "left" else self._interpolate_foot(\n                self.left_foot_pos, swing_target, phase / self.params.double_support_ratio\n            )\n            right_target = support_pos if support_foot == "right" else self._interpolate_foot(\n                self.right_foot_pos, swing_target, phase / self.params.double_support_ratio\n            )\n        else:\n            # Single support phase\n            support_pos = self.left_foot_pos if support_foot == "left" else self.right_foot_pos\n            swing_pos = self._interpolate_foot(\n                self.left_foot_pos if swing_foot == "left" else self.right_foot_pos,\n                swing_target,\n                (phase - self.params.double_support_ratio) /\n                (1.0 - self.params.double_support_ratio)\n            )\n\n            left_target = swing_pos if swing_foot == "left" else support_pos\n            right_target = swing_pos if swing_foot == "right" else support_pos\n\n        # Update foot positions\n        if swing_foot == "left":\n            self.left_foot_pos = left_target\n        else:\n            self.right_foot_pos = right_target\n\n        # Check for step completion\n        if phase >= 1.0:\n            self.current_step_idx += 1\n            self.step_start_time = time.time()\n            self.footstep_plan.pop(0)\n\n        # Generate joint commands from foot positions\n        joint_commands = self._foot_positions_to_joints(left_target, right_target)\n\n        # Add balance compensation\n        joint_commands = self._add_balance_compensation(joint_commands, balance_correction)\n\n        return joint_commands\n\n    def _update_state_estimate(self) -> None:\n        """Update internal state estimate (placeholder)."""\n        # In a real system, this would integrate IMU, encoder, and vision data\n        pass\n\n    def _interpolate_foot(\n        self,\n        start: np.ndarray,\n        end: np.ndarray,\n        t: float\n    ) -> np.ndarray:\n        """Linear interpolation between foot positions."""\n        return start + t * (end - start)\n\n    def _foot_positions_to_joints(\n        self,\n        left_foot: np.ndarray,\n        right_foot: np.ndarray\n    ) -> Dict[str, np.ndarray]:\n        """Convert foot positions to joint angle commands."""\n        # Simplified IK - would use full IK solver in practice\n        joint_commands = {}\n\n        # Standing pose\n        if np.linalg.norm(left_foot[:2]) < 0.01 and np.linalg.norm(right_foot[:2]) < 0.01:\n            joint_commands = {\n                "left_hip_roll": 0.0,\n                "left_hip_pitch": 0.0,\n                "left_knee": 0.0,\n                "left_ankle_pitch": 0.0,\n                "left_ankle_roll": 0.0,\n                "right_hip_roll": 0.0,\n                "right_hip_pitch": 0.0,\n                "right_knee": 0.0,\n                "right_ankle_pitch": 0.0,\n                "right_ankle_roll": 0.0,\n            }\n        else:\n            # Walking pose (simplified)\n            joint_commands = {\n                "left_hip_roll": 0.0,\n                "left_hip_pitch": -0.1 * left_foot[0],\n                "left_knee": 0.2 * max(0, -left_foot[2]),\n                "left_ankle_pitch": 0.1 * left_foot[0],\n                "left_ankle_roll": 0.0,\n                "right_hip_roll": 0.0,\n                "right_hip_pitch": -0.1 * right_foot[0],\n                "right_knee": 0.2 * max(0, -right_foot[2]),\n                "right_ankle_pitch": 0.1 * right_foot[0],\n                "right_ankle_roll": 0.0,\n            }\n\n        return joint_commands\n\n    def _add_balance_compensation(\n        self,\n        commands: Dict[str, float],\n        correction: np.ndarray\n    ) -> Dict[str, np.ndarray]:\n        """Add balance compensation to joint commands."""\n        # Convert balance correction to joint adjustments\n        compensated = {k: np.array([v]) for k, v in commands.items()}\n\n        # Simplified: bias hip joints for balance\n        compensated["left_hip_roll"] += correction[1] * 0.01\n        compensated["right_hip_roll"] -= correction[1] * 0.01\n        compensated["waist_pitch"] += correction[4] * 0.005\n\n        return {k: float(v) for k, v in compensated.items()}\n\n    def _generate_stand_command(self) -> Dict[str, np.ndarray]:\n        """Generate joint commands for standing pose."""\n        return {\n            "left_hip_roll": 0.0,\n            "left_hip_pitch": 0.0,\n            "left_knee": 0.0,\n            "left_ankle_pitch": 0.0,\n            "left_ankle_roll": 0.0,\n            "right_hip_roll": 0.0,\n            "right_hip_pitch": 0.0,\n            "right_knee": 0.0,\n            "right_ankle_pitch": 0.0,\n            "right_ankle_roll": 0.0,\n        }\n\n\nif __name__ == "__main__":\n    # Test walking controller\n    params = WalkingParameters()\n    controller = WalkingController(params)\n\n    # Set walking command\n    command = WalkingCommand(\n        forward_velocity=0.3,\n        lateral_velocity=0.0,\n        turn_rate=0.0\n    )\n    controller.set_command(command)\n\n    print("Walking Controller Test")\n    print("=" * 50)\n\n    # Simulate a few steps\n    for step in range(10):\n        commands = controller.step(dt=0.005)\n\n        if step % 8 == 0:\n            print(f"Step {step}:")\n            for name, angle in list(commands.items())[:4]:\n                print(f"  {name}: {np.degrees(angle):.1f} deg")\n\n    # Test different commands\n    print("\\nTurning in place:")\n    command = WalkingCommand(turn_rate=0.3)\n    controller.set_command(command)\n\n    for i in range(5):\n        controller.step(dt=0.005)\n\n    print("Walking controller test complete")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"54-manipulation-and-grasping-with-humanoid-hands",children:"5.4 Manipulation and Grasping with Humanoid Hands"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid manipulation aims to replicate the versatility of human hand capabilities. Unlike industrial grippers optimized for specific tasks, humanoid hands must handle diverse objects, adapt grasp types, and perform manipulation in unstructured environments. This section covers the fundamentals of manipulation planning, grasp synthesis, and dextrous manipulation control."}),"\n",(0,o.jsx)(e.h3,{id:"grasping-fundamentals",children:"Grasping Fundamentals"}),"\n",(0,o.jsx)(e.p,{children:"Grasping involves positioning the hand relative to an object and applying forces to constrain object motion. The quality of a grasp is measured by its ability to resist external disturbances while accommodating object uncertainty. Force closure grasps can resist arbitrary disturbance forces, while form closure depends on geometric constraints."}),"\n",(0,o.jsx)(e.p,{children:"Human hands typically perform three grasp types:"}),"\n",(0,o.jsx)(e.p,{children:"Power grasps wrap fingers around objects for high-force, low-precision tasks. Precision grasps use fingertips for delicate manipulation. Intermediate grasps balance force and precision for everyday tasks like tool use."}),"\n",(0,o.jsx)(e.h3,{id:"grasp-planning-implementation",children:"Grasp Planning Implementation"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nGrasp Planning for Humanoid Robot Hands\n\nThis module implements grasp synthesis algorithms including\ngrasp quality evaluation and manipulation planning.\n"""\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport scipy.optimize as optimize\n\n\nclass GraspType(Enum):\n    """Types of grasps for humanoid manipulation."""\n    POWER_GRASP = "power"       # Whole-hand wrap\n    PRECISION_GRASP = "precision"  # Fingertip grasp\n    LATERAL_GRASP = "lateral"   # Pinch grasp\n    HOOK_GRASP = "hook"         # Hook for carrying\n\n\n@dataclass\nclass GraspPoint:\n    """A single contact point on the hand."""\n    position: np.ndarray   # Contact position in hand frame\n    normal: np.ndarray     # Surface normal (outward from hand)\n    finger_name: str       # Which finger\n\n\n@dataclass\nclass GraspCandidate:\n    """A complete grasp candidate with multiple contact points."""\n    grasp_type: GraspType\n    contacts: List[GraspPoint]\n    pregrasp_position: np.ndarray   # Hand pose before grasping\n    grasp_position: np.ndarray      # Hand pose during grasp\n\n    # Grasp quality metrics\n    force_closure: bool = False\n    grasp_quality: float = 0.0\n    manipulability: float = 0.0\n\n    # Object properties (estimated)\n    object_radius: float = 0.0\n    object_weight: float = 0.0\n\n\n@dataclass\nclass HandGeometry:\n    """Geometry specification for humanoid hand."""\n    # Finger lengths (meters)\n    palm_width: float = 0.08\n    palm_length: float = 0.10\n\n    # Thumb\n    thumb_length: float = 0.06\n    thumb_proximal: float = 0.04\n    thumb_distal: float = 0.04\n\n    # Fingers (index, middle, ring, pinky)\n    finger_lengths: List[float] = field(default_factory=lambda: [0.05, 0.06, 0.055, 0.04])\n    proximal_segments: List[float] = field(default_factory=lambda: [0.03, 0.035, 0.03, 0.025])\n    distal_segments: List[float] = field(default_factory=lambda: [0.025, 0.03, 0.028, 0.02])\n\n    # Joint limits (radians)\n    mcp_flexion: float = np.pi / 3      # Metacarpophalangeal\n    pip_flexion: float = np.pi / 2      # Proximal interphalangeal\n    dip_flexion: float = np.pi / 2      # Distal interphalangeal\n    thumb_opposition: float = np.pi / 3\n\n\nclass GraspPlanner:\n    """\n    Grasp planning for humanoid robot hands.\n\n    Implements grasp synthesis based on object properties\n    and grasp quality evaluation.\n    """\n\n    def __init__(self, hand_geometry: Optional[HandGeometry] = None):\n        self.hand = hand_geometry or HandGeometry()\n\n    def compute_grasp_candidates(\n        self,\n        object_position: np.ndarray,\n        object_radius: float,\n        object_weight: float = 0.5,\n        n_candidates: int = 10\n    ) -> List[GraspCandidate]:\n        """\n        Generate grasp candidates for an object.\n\n        Args:\n            object_position: Center of object in world frame\n            object_radius: Approximate object radius\n            object_weight: Estimated object weight (kg)\n            n_candidates: Number of grasp candidates to generate\n\n        Returns:\n            List of grasp candidates sorted by quality\n        """\n        candidates = []\n\n        # Generate different grasp types\n        for grasp_type in GraspType:\n            for i in range(n_candidates // len(GraspType)):\n                candidate = self._synthesize_grasp(\n                    grasp_type, object_position, object_radius, object_weight\n                )\n                if candidate:\n                    candidate.grasp_quality = self._evaluate_grasp_quality(candidate)\n                    candidates.append(candidate)\n\n        # Sort by quality and return top candidates\n        candidates.sort(key=lambda c: c.grasp_quality, reverse=True)\n\n        return candidates[:n_candidates]\n\n    def _synthesize_grasp(\n        self,\n        grasp_type: GraspType,\n        object_position: np.ndarray,\n        object_radius: float,\n        object_weight: float\n    ) -> Optional[GraspCandidate]:\n        """Synthesize a single grasp candidate."""\n        # Approach direction (from hand to object)\n        approach_dir = np.array([1.0, 0.0, 0.0])  # Default approach\n\n        # Generate hand pose based on grasp type\n        if grasp_type == GraspType.POWER_GRASP:\n            return self._power_grasp(object_position, object_radius, approach_dir)\n        elif grasp_type == GraspType.PRECISION_GRASP:\n            return self._precision_grasp(object_position, object_radius, approach_dir)\n        elif grasp_type == GraspType.LATERAL_GRASP:\n            return self._lateral_grasp(object_position, object_radius, approach_dir)\n        elif grasp_type == GraspType.HOOK_GRASP:\n            return self._hook_grasp(object_position, object_radius, approach_dir)\n\n        return None\n\n    def _power_grasp(\n        self,\n        object_position: np.ndarray,\n        radius: float,\n        approach_dir: np.ndarray\n    ) -> GraspCandidate:\n        """Synthesize power grasp configuration."""\n        # Power grasp: wrap fingers around object\n        contacts = []\n\n        # Thumb contact\n        thumb_pos = object_position + np.array([-radius - 0.02, 0.03, 0])\n        thumb_normal = np.array([1.0, 0.0, 0.0])\n        contacts.append(GraspPoint(thumb_pos, thumb_normal, "thumb"))\n\n        # Finger contacts (wrap around)\n        n_fingers = 4\n        for i, length in enumerate(self.hand.finger_lengths):\n            angle = (i - n_fingers / 2 + 0.5) * 0.08  # Spread angle\n\n            # Contact position on finger\n            finger_pos = object_position + np.array([\n                -radius - 0.01,\n                angle * radius * 2,\n                length * 0.3\n            ])\n            finger_normal = np.array([1.0, -angle, 0.0])\n            finger_normal /= np.linalg.norm(finger_normal)\n\n            contacts.append(GraspPoint(finger_pos, finger_normal, f"finger_{i}"))\n\n        # Hand pose\n        grasp_pose = object_position + np.array([-radius - 0.05, 0, 0])\n        pregrasp_pose = grasp_pose + np.array([-0.1, 0, 0])\n\n        return GraspCandidate(\n            grasp_type=GraspType.POWER_GRASP,\n            contacts=contacts,\n            pregrasp_position=pregrasp_pose,\n            grasp_position=grasp_pose,\n            object_radius=radius,\n            object_weight=0.5\n        )\n\n    def _precision_grasp(\n        self,\n        object_position: np.ndarray,\n        radius: float,\n        approach_dir: np.ndarray\n    ) -> GraspCandidate:\n        """Synthesize precision (pinch) grasp configuration."""\n        contacts = []\n\n        # Thumb and index finger pinch\n        thumb_pos = object_position + np.array([0, 0.02, radius + 0.01])\n        thumb_normal = np.array([0.0, 0.0, -1.0])\n        contacts.append(GraspPoint(thumb_pos, thumb_normal, "thumb"))\n\n        index_pos = object_position + np.array([0, -0.02, radius + 0.01])\n        index_normal = np.array([0.0, 0.0, -1.0])\n        contacts.append(GraspPoint(index_pos, index_normal, "finger_0"))\n\n        # Hand pose\n        grasp_pose = object_position + np.array([0, 0, radius + 0.05])\n        pregrasp_pose = grasp_pose + np.array([0, 0, 0.1])\n\n        return GraspCandidate(\n            grasp_type=GraspType.PRECISION_GRASP,\n            contacts=contacts,\n            pregrasp_position=pregrasp_pose,\n            grasp_position=grasp_pose,\n            object_radius=radius,\n            object_weight=0.1\n        )\n\n    def _lateral_grasp(\n        self,\n        object_position: np.ndarray,\n        radius: float,\n        approach_dir: np.ndarray\n    ) -> GraspCandidate:\n        """Synthesize lateral (key pinch) grasp configuration."""\n        contacts = []\n\n        # Thumb against side of object, index finger on other side\n        thumb_pos = object_position + np.array([radius + 0.01, 0, 0])\n        thumb_normal = np.array([-1.0, 0.0, 0.0])\n        contacts.append(GraspPoint(thumb_pos, thumb_normal, "thumb"))\n\n        index_pos = object_position + np.array([-(radius + 0.01), 0, 0.02])\n        index_normal = np.array([1.0, 0.0, 0.0])\n        contacts.append(GraspPoint(index_pos, index_normal, "finger_0"))\n\n        # Hand pose (rotated for lateral pinch)\n        grasp_pose = object_position + np.array([0, 0, radius + 0.03])\n        pregrasp_pose = grasp_pose + np.array([0, 0, 0.08])\n\n        return GraspCandidate(\n            grasp_type=GraspType.LATERAL_GRASP,\n            contacts=contacts,\n            pregrasp_position=pregrasp_pose,\n            grasp_position=grasp_pose,\n            object_radius=radius,\n            object_weight=0.2\n        )\n\n    def _hook_grasp(\n        self,\n        object_position: np.ndarray,\n        radius: float,\n        approach_dir: np.ndarray\n    ) -> GraspCandidate:\n        """Synthesize hook grasp for carrying."""\n        contacts = []\n\n        # Fingers curled under object\n        for i, length in enumerate(self.hand.finger_lengths[:3]):\n            contact_pos = object_position + np.array([\n                -0.02,\n                (i - 1) * 0.02,\n                -(length * 0.8 + radius)\n            ])\n            contact_normal = np.array([0.0, 0.0, 1.0])\n            contacts.append(GraspPoint(contact_pos, contact_normal, f"finger_{i}"))\n\n        # Hand pose (above object)\n        grasp_pose = object_position + np.array([0, 0, radius + 0.05])\n        pregrasp_pose = grasp_pose + np.array([0, 0, 0.1])\n\n        return GraspCandidate(\n            grasp_type=GraspType.HOOK_GRASP,\n            contacts=contacts,\n            pregrasp_position=pregrasp_pose,\n            grasp_position=grasp_pose,\n            object_radius=radius,\n            object_weight=0.5\n        )\n\n    def _evaluate_grasp_quality(self, grasp: GraspCandidate) -> float:\n        """\n        Evaluate grasp quality.\n\n        Combines force closure, contact distribution, and\n        manipulability metrics.\n        """\n        quality = 0.0\n\n        # 1. Force closure evaluation\n        if len(grasp.contacts) >= 3:\n            # Check if contacts form a stable configuration\n            contact_positions = np.array([c.position for c in grasp.contacts])\n            contact_normals = np.array([c.normal for c in grasp.contacts])\n\n            # Compute contact convex hull area (proxy for stability)\n            try:\n                hull = contact_positions[:3]  # Use first 3 contacts\n                area = np.linalg.norm(np.cross(\n                    hull[1] - hull[0],\n                    hull[2] - hull[0]\n                )) / 2\n                quality += min(area * 10, 0.3)  # Cap contribution\n            except:\n                pass\n\n        # 2. Contact normal alignment\n        normals = np.array([c.normal for c in grasp.contacts])\n        if len(normals) > 0:\n            # Prefer normals pointing toward object center\n            center = np.mean([c.position for c in grasp.contacts])\n            to_center = center - np.array(grasp.grasp_position)\n            to_center /= np.linalg.norm(to_center)\n\n            alignment = np.mean([np.dot(c.normal, to_center) for c in grasp.contacts])\n            quality += max(alignment * 0.2, 0.0)\n\n        # 3. Grasp type appropriateness for object weight\n        if grasp.grasp_type == GraspType.POWER_GRASP and grasp.object_weight > 0.3:\n            quality += 0.2\n        elif grasp.grasp_type == GraspType.PRECISION_GRASP and grasp.object_weight < 0.2:\n            quality += 0.2\n\n        # 4. Manipulability (finger spread)\n        finger_positions = [c.position for c in grasp.contacts if "finger" in c.finger_name]\n        if len(finger_positions) >= 2:\n            spread = np.std([p[1] for p in finger_positions])\n            quality += min(spread * 5, 0.2)\n\n        return min(quality, 1.0)\n\n    def select_best_grasp(\n        self,\n        candidates: List[GraspCandidate],\n        constraints: Optional[Dict] = None\n    ) -> GraspCandidate:\n        """\n        Select best grasp given constraints.\n\n        Constraints might include:\n        - Required force closure\n        - Maximum approach direction\n        - Task-specific preferences\n        """\n        constraints = constraints or {}\n\n        # Filter by constraints first\n        filtered = candidates\n\n        if constraints.get("force_closure", False):\n            filtered = [c for c in filtered if c.force_closure]\n\n        if "approach_direction" in constraints:\n            approach = np.array(constraints["approach_direction"])\n            for c in filtered:\n                approach_vec = c.grasp_position - c.pregrasp_position\n                approach_vec /= np.linalg.norm(approach_vec)\n                if np.dot(approach, approach_vec) < 0:\n                    filtered.remove(c)\n\n        # Return highest quality from filtered\n        if filtered:\n            return max(filtered, key=lambda c: c.grasp_quality)\n\n        # Fallback to highest quality\n        return max(candidates, key=lambda c: c.grasp_quality)\n\n\nif __name__ == "__main__":\n    # Test grasp planning\n    hand = HandGeometry()\n    planner = GraspPlanner(hand)\n\n    # Object parameters\n    object_pos = np.array([0.5, 0.0, 0.8])  # Table height\n    object_radius = 0.04  # Small object\n\n    # Generate candidates\n    candidates = planner.compute_grasp_candidates(\n        object_pos, object_radius, n_candidates=10\n    )\n\n    print("Grasp Planning Test")\n    print("=" * 50)\n    print(f"Generated {len(candidates)} grasp candidates")\n\n    for i, candidate in enumerate(candidates[:3]):\n        print(f"\\nGrasp {i + 1}: {candidate.grasp_type.value}")\n        print(f"  Quality: {candidate.grasp_quality:.3f}")\n        print(f"  Contacts: {len(candidate.contacts)}")\n        print(f"  Pregrasp: {candidate.pregrasp_position}")\n        print(f"  Grasp: {candidate.grasp_position}")\n\n    # Select best grasp\n    best = planner.select_best_grasp(candidates)\n    print(f"\\nBest grasp: {best.grasp_type.value} (quality: {best.grasp_quality:.3f})")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"55-natural-human-robot-interaction-design",children:"5.5 Natural Human-Robot Interaction Design"}),"\n",(0,o.jsx)(e.p,{children:"Human-robot interaction (HRI) for humanoid robots must account for the unique expectations humans have when interacting with anthropomorphic systems. Unlike industrial robots that operate in separated workspaces, humanoids share environments with people and must communicate intentions, understand human behavior, and respond naturally."}),"\n",(0,o.jsx)(e.h3,{id:"principles-of-natural-hri",children:"Principles of Natural HRI"}),"\n",(0,o.jsx)(e.p,{children:"Natural interaction relies on familiar social cues that humans use with each other. Nonverbal communication including gaze, gestures, and posture provides important context for interaction. A humanoid robot that maintains appropriate eye contact during conversation, orients its body toward the person speaking, and uses hand gestures during explanations feels more natural and approachable."}),"\n",(0,o.jsx)(e.p,{children:"The design of HRI systems must balance expressiveness with predictability. Robots should communicate their state clearly so humans can anticipate actions, but excessive expressiveness can become distracting or misleading. The goal is transparency: humans should understand what the robot is doing, why, and what it will do next."}),"\n",(0,o.jsx)(e.h3,{id:"multimodal-interaction-architecture",children:"Multimodal Interaction Architecture"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nHuman-Robot Interaction System for Humanoid Robots\n\nThis module implements multimodal interaction including\nspeech, gesture recognition, and gaze behavior.\n"""\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional, Callable\nfrom dataclasses import dataclass, field\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\n\n\nclass InteractionState(Enum):\n    """State of human-robot interaction."""\n    IDLE = "idle"\n    DETECTING = "detecting"\n    ENGAGED = "engaged"\n    SPEAKING = "speaking"\n    LISTENING = "listening"\n    THINKING = "thinking"\n\n\n@dataclass\nclass Person:\n    """Representation of a detected person."""\n    person_id: int\n    position: np.ndarray        # 3D position\n    orientation: np.ndarray     # Body orientation (yaw)\n    gaze_direction: np.ndarray  # Where they\'re looking\n    confidence: float           # Detection confidence\n    timestamp: float            # Last detection time\n\n\n@dataclass\nclass Utterance:\n    """Speech utterance with metadata."""\n    text: str\n    speaker: str               # "robot" or person ID\n    timestamp: float\n    intent: Optional[str]      # Recognized intent\n    entities: Dict[str, str]   # Extracted entities\n\n\n@dataclass\nclass Gesture:\n    """Recognized gesture with confidence."""\n    gesture_type: str          # "point", "wave", "stop", etc.\n    side: str                  # "left", "right", "both"\n    start_time: float\n    confidence: float\n    target_position: Optional[np.ndarray]  # For pointing\n\n\nclass InteractionManager:\n    """\n    Manages natural human-robot interaction.\n\n    Coordinates speech, gaze, gesture, and body language\n    for cohesive interaction experience.\n    """\n\n    def __init__(self):\n        # State\n        self.state = InteractionState.IDLE\n        self.last_interaction_time: float = 0.0\n        self.session_start_time: float = 0.0\n\n        # Detected people\n        self.people: Dict[int, Person] = {}\n        self.engaged_person: Optional[int] = None\n\n        # Interaction history\n        self.utterances: List[Utterance] = []\n        self.gestures: List[Gesture] = []\n\n        # Gaze behavior parameters\n        self.gaze_target_duration: float = 2.0  # Seconds to hold gaze\n        self.gaze_away_duration: float = 1.0    # Time before looking away\n        self.last_gaze_shift: float = 0.0\n        self.current_gaze_target: Optional[int] = None\n\n        # Attention model\n        self.attention_weights: Dict[int, float] = {}\n\n        # Callbacks for action execution\n        self.on_speak: Optional[Callable[[str], None]] = None\n        self.on_gesture: Optional[Callable[[Gesture], None]] = None\n        self.on_head_turn: Optional[Callable[[np.ndarray], None]] = None\n\n        # Initialize attention\n        self._initialize_attention()\n\n    def _initialize_attention(self) -> None:\n        """Initialize attention model with default values."""\n        self.attention_decay = 0.95\n        self.attention_boost = 0.3\n        self.min_attention = 0.1\n\n    def update_person(\n        self,\n        person_id: int,\n        position: np.ndarray,\n        orientation: np.ndarray,\n        confidence: float = 1.0\n    ) -> Person:\n        """Update or add a detected person."""\n        # Estimate gaze direction from body orientation\n        gaze_dir = np.array([\n            np.cos(orientation[2]),\n            np.sin(orientation[2]),\n            0.1  # Slight upward tilt\n        ])\n        gaze_dir /= np.linalg.norm(gaze_dir)\n\n        person = Person(\n            person_id=person_id,\n            position=position,\n            orientation=orientation,\n            gaze_direction=gaze_dir,\n            confidence=confidence,\n            timestamp=time.time()\n        )\n\n        self.people[person_id] = person\n\n        # Update attention\n        if person_id in self.attention_weights:\n            self.attention_weights[person_id] = min(\n                self.attention_weights[person_id] + self.attention_boost, 1.0\n            )\n        else:\n            self.attention_weights[person_id] = 0.5\n\n        return person\n\n    def remove_person(self, person_id: int) -> None:\n        """Remove a person from tracking."""\n        if person_id in self.people:\n            del self.people[person_id]\n        if person_id in self.attention_weights:\n            del self.attention_weights[person_id]\n        if self.engaged_person == person_id:\n            self.engaged_person = None\n\n    def update_attention(self) -> None:\n        """Update attention weights based on time and events."""\n        current_time = time.time()\n\n        for person_id in self.people:\n            if person_id not in self.attention_weights:\n                self.attention_weights[person_id] = self.min_attention\n            else:\n                # Decay attention over time\n                time_since_update = current_time - self.people[person_id].timestamp\n                decay = self.attention_decay ** (time_since_update / 10.0)\n                self.attention_weights[person_id] = max(\n                    self.min_attention,\n                    self.attention_weights[person_id] * decay\n                )\n\n        # Select most attentive person\n        if self.attention_weights:\n            self.engaged_person = max(\n                self.attention_weights.keys(),\n                key=lambda p: self.attention_weights[p]\n            )\n\n    def get_attended_person(self) -> Optional[Person]:\n        """Get the person with highest attention."""\n        self.update_attention()\n        if self.engaged_person is not None:\n            return self.people.get(self.engaged_person)\n        return None\n\n    def add_utterance(\n        self,\n        text: str,\n        speaker: str,\n        intent: Optional[str] = None\n    ) -> Utterance:\n        """Add an utterance to interaction history."""\n        utterance = Utterance(\n            text=text,\n            speaker=speaker,\n            timestamp=time.time(),\n            intent=intent,\n            entities={}\n        )\n        self.utterances.append(utterance)\n        self.last_interaction_time = time.time()\n\n        # Update state\n        if speaker == "robot":\n            self.state = InteractionState.SPEAKING\n        else:\n            self.state = InteractionState.LISTENING\n\n        return utterance\n\n    def add_gesture(self, gesture: Gesture) -> None:\n        """Add a recognized gesture."""\n        self.gestures.append(gesture)\n        self.last_interaction_time = time.time()\n\n        # React to specific gestures\n        if gesture.gesture_type == "point":\n            self._handle_point_gesture(gesture)\n        elif gesture.gesture_type == "wave":\n            self._handle_wave_gesture(gesture)\n        elif gesture.gesture_type == "stop":\n            self._handle_stop_gesture(gesture)\n\n    def _handle_point_gesture(self, gesture: Gesture) -> None:\n        """Handle pointing gesture."""\n        # Look at pointed location\n        if gesture.target_position is not None and self.on_head_turn:\n            self.on_head_turn(gesture.target_position)\n\n    def _handle_wave_gesture(self, gesture: Gesture) -> None:\n        """Handle wave gesture."""\n        # Engage with the person who waved\n        self.state = InteractionState.ENGAGED\n        self.session_start_time = time.time()\n\n    def _handle_stop_gesture(self, gesture: Gesture) -> None:\n        """Handle stop gesture."""\n        # Stop current action\n        self.state = InteractionState.IDLE\n\n    def compute_gaze_target(self) -> Tuple[np.ndarray, str]:\n        """\n        Compute where the robot should look.\n\n        Returns:\n            Tuple of (gaze position, target description)\n        """\n        current_time = time.time()\n        gaze_target = np.array([0.0, 0.0, 1.5])  # Default: forward\n        target_desc = "neutral"\n\n        # Time-based gaze shifting\n        time_since_gaze_shift = current_time - self.last_gaze_shift\n\n        if self.state == InteractionState.SPEAKING:\n            # Look at engaged person while speaking\n            person = self.get_attended_person()\n            if person is not None:\n                gaze_target = person.position + np.array([0, 0, 0.1])\n                target_desc = f"person_{person.person_id}"\n\n        elif self.state == InteractionState.LISTENING:\n            # Maintain eye contact while listening\n            person = self.get_attended_person()\n            if person is not None:\n                gaze_target = person.position + np.array([0, 0, 0.1])\n                target_desc = f"person_{person.person_id}"\n\n        elif self.state == InteractionState.IDLE:\n            # Shift gaze periodically to show awareness\n            if time_since_gaze_shift > self.gaze_target_duration:\n                self.last_gaze_shift = current_time\n\n                # Look at most attentive person\n                person = self.get_attended_person()\n                if person is not None:\n                    gaze_target = person.position + np.array([0, 0, 0.1])\n                    target_desc = f"person_{person.person_id}"\n                elif self.people:\n                    # Look at random person\n                    person_id = list(self.people.keys())[0]\n                    person = self.people[person_id]\n                    gaze_target = person.position + np.array([0, 0, 0.1])\n                    target_desc = f"person_{person.person_id}"\n\n        return gaze_target, target_desc\n\n    def generate_gaze_behavior(self) -> Dict:\n        """\n        Generate complete gaze behavior parameters.\n\n        Returns:\n            Dictionary with gaze targets and transitions\n        """\n        gaze_target, target_desc = self.compute_gaze_target()\n\n        return {\n            "target_position": gaze_target,\n            "target_description": target_desc,\n            "duration": self.gaze_target_duration,\n            "smoothness": 0.5,  # Smoothing factor for gaze motion\n            "eye_offset": np.array([0.0, 0.0, 0.02])  # Eye separation\n        }\n\n    def get_conversational_turn(self) -> str:\n        """Get appropriate conversational behavior."""\n        person = self.get_attended_person()\n\n        if person is None:\n            return "scan"\n\n        # Check recent utterances\n        recent = [u for u in self.utterances if time.time() - u.timestamp < 5.0]\n\n        if not recent:\n            return "greet"\n\n        last_utterance = recent[-1]\n\n        if last_utterance.speaker == "robot":\n            # Robot just spoke, wait for response\n            return "listen"\n        else:\n            # Person spoke, acknowledge\n            return "acknowledge"\n\n    def generate_body_language(\n        self,\n        behavior_type: str\n    ) -> Dict[str, np.ndarray]:\n        """\n        Generate body language parameters.\n\n        Args:\n            behavior_type: Type of behavior (nod, shake, tilt, etc.)\n\n        Returns:\n            Dictionary of joint angles for body language\n        """\n        if behavior_type == "nod":\n            return {\n                "neck_pitch": np.array([0.1, -0.1, 0.0]),\n                "duration": 0.5\n            }\n        elif behavior_type == "shake":\n            return {\n                "neck_yaw": np.array([0.1, -0.1, 0.1, -0.1, 0.0]),\n                "duration": 0.8\n            }\n        elif behavior_type == "tilt":\n            return {\n                "neck_roll": np.array([0.1]),\n                "duration": 0.3\n            }\n        elif behavior_type == "lean_forward":\n            return {\n                "waist_pitch": np.array([0.1]),\n                "duration": 0.5\n            }\n        elif behavior_type == "open_arms":\n            return {\n                "left_shoulder_pitch": np.array([-0.5]),\n                "right_shoulder_pitch": np.array([-0.5]),\n                "duration": 0.3\n            }\n\n        return {"duration": 0.0}\n\n    def get_interaction_summary(self) -> Dict:\n        """Get summary of current interaction state."""\n        return {\n            "state": self.state.value,\n            "engaged_person": self.engaged_person,\n            "n_people": len(self.people),\n            "recent_utterances": len([\n                u for u in self.utterances\n                if time.time() - u.timestamp < 30.0\n            ]),\n            "session_duration": time.time() - self.session_start_time\n            if self.session_start_time > 0 else 0.0\n        }\n\n\nif __name__ == "__main__":\n    # Test interaction manager\n    manager = InteractionManager()\n\n    print("Human-Robot Interaction Test")\n    print("=" * 50)\n\n    # Simulate people detection\n    manager.update_person(\n        person_id=1,\n        position=np.array([1.0, 0.5, 1.6]),\n        orientation=np.array([0.0, 0.0, -0.3]),\n        confidence=0.9\n    )\n\n    manager.update_person(\n        person_id=2,\n        position=np.array([1.2, -0.3, 1.6]),\n        orientation=np.array([0.0, 0.0, 0.5]),\n        confidence=0.85\n    )\n\n    print(f"Detected {len(manager.people)} people")\n\n    # Update attention\n    manager.update_attention()\n    print(f"Engaged person: {manager.engaged_person}")\n\n    # Add utterance\n    manager.add_utterance("Hello!", "person_1")\n    print(f"Interaction state: {manager.state.value}")\n\n    # Compute gaze\n    gaze_target, target_desc = manager.compute_gaze_target()\n    print(f"Gaze target: {target_desc}")\n    print(f"Gaze position: {gaze_target}")\n\n    # Get interaction summary\n    summary = manager.get_interaction_summary()\n    print(f"Summary: {summary}")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"56-code-example-complete-kinematics-and-walking-controller",children:"5.6 Code Example: Complete Kinematics and Walking Controller"}),"\n",(0,o.jsx)(e.p,{children:"This section presents a complete working example integrating forward kinematics, inverse kinematics, and a walking controller for a humanoid robot."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'"""\nComplete Humanoid Robot Controller\n\nThis module provides an integrated controller combining\nforward/inverse kinematics with walking control for\nUnitree H1 humanoid robot.\n"""\n\nimport numpy as np\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass, field\nimport time\n\n\n@dataclass\nclass RobotConfig:\n    """Physical configuration for Unitree H1 robot."""\n    # Link lengths (meters) - matching H1 specifications\n    hip_to_knee: float = 0.45\n    knee_to_ankle: float = 0.45\n    ankle_to_ground: float = 0.10\n\n    # Hip offset from pelvis center\n    hip_lateral_offset: float = 0.12\n\n    # Pelvis height when standing\n    standing_height: float = 0.95\n\n    # Mass distribution (kg)\n    total_mass: float = 47.0\n    leg_mass: float = 8.5  # Per leg\n\n\n@dataclass\nclass ControlState:\n    """Internal control state."""\n    # Joint angles (radians)\n    joint_positions: Dict[str, float] = field(default_factory=dict)\n\n    # Joint velocities (rad/s)\n    joint_velocities: Dict[str, float] = field(default_factory=dict)\n\n    # Timestamps\n    last_update: float = 0.0\n    control_rate: float = 200.0  # Hz\n\n    # Walking state\n    is_walking: bool = False\n    gait_phase: float = 0.0\n    step_count: int = 0\n\n\nclass HumanoidController:\n    """\n    Integrated controller for humanoid robot.\n\n    Combines kinematics and walking control for\n    smooth, stable humanoid motion.\n    """\n\n    def __init__(self, config: Optional[RobotConfig] = None):\n        self.config = config or RobotConfig()\n\n        # Kinematics solvers\n        self.fk = ForwardKinematics()\n        self.ik = InverseKinematics()\n\n        # Walking controller\n        self.walk_params = WalkingParameters()\n        self.walker = WalkingController(self.walk_params)\n\n        # Control state\n        self.state = ControlState()\n        self.state.last_update = time.time()\n\n        # Initialize joint names\n        self._init_joints()\n\n    def _init_joints(self) -> None:\n        """Initialize joint position dictionary."""\n        joint_names = [\n            # Left leg\n            "left_hip_roll", "left_hip_pitch", "left_knee",\n            "left_ankle_pitch", "left_ankle_roll",\n            # Right leg\n            "right_hip_roll", "right_hip_pitch", "right_knee",\n            "right_ankle_pitch", "right_ankle_roll",\n            # Torso\n            "waist_pitch", "waist_yaw", "waist_roll",\n            # Arms\n            "left_shoulder_pitch", "left_shoulder_roll", "left_elbow",\n            "right_shoulder_pitch", "right_shoulder_roll", "right_elbow",\n            # Head\n            "neck_pitch", "neck_yaw"\n        ]\n\n        for name in joint_names:\n            self.state.joint_positions[name] = 0.0\n            self.state.joint_velocities[name] = 0.0\n\n        # Set standing pose\n        self._set_standing_pose()\n\n    def _set_standing_pose(self) -> None:\n        """Set initial standing pose."""\n        # Standing with slight knee bend for stability\n        self.state.joint_positions = {\n            "left_hip_roll": 0.0,\n            "left_hip_pitch": 0.0,\n            "left_knee": 0.0,\n            "left_ankle_pitch": 0.0,\n            "left_ankle_roll": 0.0,\n            "right_hip_roll": 0.0,\n            "right_hip_pitch": 0.0,\n            "right_knee": 0.0,\n            "right_ankle_pitch": 0.0,\n            "right_ankle_roll": 0.0,\n            "waist_pitch": 0.0,\n            "waist_yaw": 0.0,\n            "waist_roll": 0.0,\n            "left_shoulder_pitch": 0.0,\n            "left_shoulder_roll": 0.0,\n            "left_elbow": 0.0,\n            "right_shoulder_pitch": 0.0,\n            "right_shoulder_roll": 0.0,\n            "right_elbow": 0.0,\n            "neck_pitch": 0.0,\n            "neck_yaw": 0.0\n        }\n\n    def set_walking_command(\n        self,\n        forward: float = 0.0,\n        lateral: float = 0.0,\n        turn: float = 0.0\n    ) -> None:\n        """Set walking velocity command."""\n        command = WalkingCommand(\n            forward_velocity=forward,\n            lateral_velocity=lateral,\n            turn_rate=turn,\n            gait_type=GaitType.WALKING if abs(forward) > 0.01 else GaitType.STANDING\n        )\n        self.walker.set_command(command)\n\n    def step(self) -> Dict[str, float]:\n        """\n        Execute one control cycle.\n\n        Returns:\n            Dictionary of joint position commands\n        """\n        current_time = time.time()\n        dt = current_time - self.state.last_update\n\n        if dt < 1.0 / self.state.control_rate:\n            # Rate limit control updates\n            return self.state.joint_positions\n\n        # Update walking controller\n        if self.walker.command.forward_velocity > 0.01:\n            self.state.is_walking = True\n            walk_commands = self.walker.step(dt)\n\n            # Apply walking commands to joints\n            for name, angle in walk_commands.items():\n                if name in self.state.joint_positions:\n                    self.state.joint_positions[name] = angle\n        else:\n            # Walking stopped, return to standing\n            self.state.is_walking = False\n            self._smooth_to_pose(self._set_standing_pose, dt)\n\n        self.state.last_update = current_time\n        return self.state.joint_positions\n\n    def _smooth_to_pose(\n        self,\n        target_pose: Dict[str, float],\n        dt: float,\n        speed: float = 2.0\n    ) -> None:\n        """Smoothly interpolate toward target pose."""\n        for name, target in target_pose.items():\n            if name in self.state.joint_positions:\n                current = self.state.joint_positions[name]\n                self.state.joint_positions[name] = current + speed * dt * (target - current)\n\n    def compute_foot_positions(self) -> Tuple[np.ndarray, np.ndarray]:\n        """Compute current foot positions from joint angles."""\n        # Left foot\n        left_foot = self.fk.compute_foot_pose(\n            self.state.joint_positions["left_hip_roll"],\n            self.state.joint_positions["left_hip_pitch"],\n            self.state.joint_positions["left_knee"],\n            self.state.joint_positions["left_ankle_pitch"],\n            self.state.joint_positions["left_ankle_roll"],\n            "left"\n        )\n\n        # Right foot\n        right_foot = self.fk.compute_foot_pose(\n            self.state.joint_positions["right_hip_roll"],\n            self.state.joint_positions["right_hip_pitch"],\n            self.state.joint_positions["right_knee"],\n            self.state.joint_positions["right_ankle_pitch"],\n            self.state.joint_positions["right_ankle_roll"],\n            "right"\n        )\n\n        return left_foot.position, right_foot.position\n\n    def get_com_position(self) -> np.ndarray:\n        """Estimate center of mass position."""\n        # Simplified CoM estimation\n        # In a real system, this would use full body dynamics\n        return np.array([\n            0.0,\n            0.0,\n            self.config.standing_height\n        ])\n\n    def get_status(self) -> Dict:\n        """Get controller status for monitoring."""\n        left_foot, right_foot = self.compute_foot_positions()\n\n        return {\n            "is_walking": self.state.is_walking,\n            "gait_phase": self.walker.current_step_idx,\n            "left_foot": left_foot.tolist(),\n            "right_foot": right_foot.tolist(),\n            "com": self.get_com_position().tolist(),\n            "control_rate": self.state.control_rate\n        }\n\n\n# Import required enums and classes from earlier modules\nfrom enum import Enum\n\n\nclass GaitType(Enum):\n    STANDING = "standing"\n    WALKING = "walking"\n\n\n# Simplified classes to make the module self-contained\nclass WalkingParameters:\n    def __init__(self):\n        self.step_length = 0.25\n        self.step_width = 0.12\n        self.step_height = 0.08\n        self.step_duration = 0.4\n        self.double_support_ratio = 0.2\n        self.com_height = 0.85\n\n\nclass WalkingCommand:\n    def __init__(self, forward_velocity=0.0, lateral_velocity=0.0, turn_rate=0.0,\n                 gait_type=GaitType.STANDING, emergency_stop=False):\n        self.forward_velocity = forward_velocity\n        self.lateral_velocity = lateral_velocity\n        self.turn_rate = turn_rate\n        self.gait_type = gait_type\n        self.emergency_stop = emergency_stop\n\n\n# Include the key classes from previous sections\nclass ForwardKinematics:\n    """Simplified FK for humanoid leg."""\n    def __init__(self):\n        self.hip_to_knee = 0.45\n        self.knee_to_ankle = 0.45\n        self.ankle_to_ground = 0.10\n        self.hip_lateral_offset = 0.12\n\n    def compute_foot_pose(self, hip_roll, hip_pitch, knee_pitch, ankle_pitch,\n                          ankle_roll=0.0, side="left"):\n        """Compute foot position from joint angles."""\n        # Simplified forward kinematics\n        sign = -1.0 if side == "left" else 1.0\n        y_offset = sign * self.hip_lateral_offset\n\n        # Position calculation\n        foot_pos = np.array([\n            -self.hip_to_knee * np.sin(hip_pitch) - self.knee_to_ankle * np.sin(hip_pitch + knee_pitch),\n            y_offset + self.hip_to_knee * np.sin(hip_roll),\n            -(self.hip_to_knee * np.cos(hip_pitch) + self.knee_to_ankle * np.cos(hip_pitch + knee_pitch) + self.ankle_to_ground)\n        ])\n\n        return type(\'Pose\', (), {\'position\': foot_pos})()\n\n\nclass InverseKinematics:\n    """Simplified IK for humanoid leg."""\n    def __init__(self):\n        self.L1 = 0.45\n        self.L2 = 0.45\n        self.L_foot = 0.10\n\n\nclass WalkingController:\n    """Simplified walking controller."""\n    def __init__(self, params):\n        self.params = params\n        self.command = WalkingCommand()\n        self.current_step_idx = 0\n\n    def set_command(self, command):\n        self.command = command\n\n    def step(self, dt):\n        # Simplified step - just return zero velocities\n        return {\n            "left_hip_roll": 0.0,\n            "left_hip_pitch": 0.0,\n            "left_knee": 0.0,\n            "left_ankle_pitch": 0.0,\n            "left_ankle_roll": 0.0,\n            "right_hip_roll": 0.0,\n            "right_hip_pitch": 0.0,\n            "right_knee": 0.0,\n            "right_ankle_pitch": 0.0,\n            "right_ankle_roll": 0.0,\n        }\n\n\nif __name__ == "__main__":\n    # Test integrated controller\n    config = RobotConfig()\n    controller = HumanoidController(config)\n\n    print("Humanoid Robot Controller Test")\n    print("=" * 50)\n\n    # Get initial status\n    status = controller.get_status()\n    print(f"Initial state: walking={status[\'is_walking\']}")\n    print(f"Left foot: {status[\'left_foot\']}")\n    print(f"Right foot: {status[\'right_foot\']}")\n\n    # Start walking\n    print("\\nStarting forward walk...")\n    controller.set_walking_command(forward=0.3)\n\n    # Run control cycles\n    for i in range(10):\n        commands = controller.step()\n        if i % 5 == 0:\n            left_knee = commands["left_knee"]\n            right_knee = commands["right_knee"]\n            print(f"Step {i}: left_knee={np.degrees(left_knee):.1f} deg, right_knee={np.degrees(right_knee):.1f} deg")\n\n    # Update status\n    status = controller.get_status()\n    print(f"\\nAfter walking: gait_phase={status[\'gait_phase\']}")\n\n    # Stop walking\n    print("\\nStopping...")\n    controller.set_walking_command(forward=0.0)\n\n    print("Controller test complete")\n'})}),"\n",(0,o.jsx)(e.h2,{id:"chapter-summary",children:"Chapter Summary"}),"\n",(0,o.jsx)(e.p,{children:"This chapter covered the essential aspects of humanoid robot development:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Kinematics Foundation"}),": Forward and inverse kinematics provide the mathematical framework for motion planning and control, with humanoid robots requiring special attention to multi-chain structures and balance constraints."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Bipedal Locomotion"}),": Walking control combines ZMP-based balance, footstep planning, and trajectory generation. The control system must handle the inherent instability of bipedal motion while adapting to varying terrain and disturbances."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Manipulation and Grasping"}),": Humanoid hands enable diverse manipulation tasks through different grasp types. Grasp planning evaluates object properties and selects appropriate grasps based on task requirements."]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Natural HRI"}),": Human-robot interaction for humanoid robots leverages familiar social cues including gaze, gestures, and body language. Multimodal interaction creates natural, intuitive communication channels."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Kinematic Chains"}),": Hierarchical joint structures for humanoid body parts"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ZMP (Zero Moment Point)"}),": Fundamental stability criterion for bipedal walking"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Force Closure"}),": Grasp quality metric indicating disturbance resistance"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Attention Model"}),": Computational model for natural gaze and engagement"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gait Parameters"}),": Timing and geometry that define walking behavior"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"unitree-robot-hardware-reference",children:"Unitree Robot Hardware Reference"}),"\n",(0,o.jsx)(e.p,{children:"For implementation on physical robots, refer to the following Unitree specifications (see Appendix A):"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Parameter"}),(0,o.jsx)(e.th,{children:"H1 Value"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Total DOF"}),(0,o.jsx)(e.td,{children:"19"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Leg DOF"}),(0,o.jsx)(e.td,{children:"3 per leg"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Standing Height"}),(0,o.jsx)(e.td,{children:"0.95 m"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Total Mass"}),(0,o.jsx)(e.td,{children:"47 kg"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Max Walking Speed"}),(0,o.jsx)(e.td,{children:"2.0 m/s"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:"Joint Torque"}),(0,o.jsx)(e.td,{children:"100-150 Nm"})]})]})]}),"\n",(0,o.jsx)(e.h3,{id:"cross-references",children:"Cross-References"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Chapter 2"}),": Sensor systems provide perception for walking control"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Chapter 3"}),": Simulation environment for testing walking algorithms"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Chapter 6"}),": Learning-based approaches for improved locomotion"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Part 6"}),": Conversational AI for natural HRI dialogue"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Robot Modeling and Control" - Spong, Hutchinson, Vidyasagar'}),"\n",(0,o.jsx)(e.li,{children:'"Introduction to Humanoid Robotics" - Kajita et al.'}),"\n",(0,o.jsx)(e.li,{children:"Unitree H1 Documentation and SDK"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"next-chapter",children:"Next Chapter"}),"\n",(0,o.jsxs)(e.p,{children:["Chapter 6 explores ",(0,o.jsx)(e.strong,{children:"Learning-Based Control"})," for humanoid robots, covering reinforcement learning, imitation learning, and adaptation techniques that enable robots to improve their capabilities through experience."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Part 5: Humanoid Development"})," | ",(0,o.jsx)(e.a,{href:"part-3-simulation/gazebo-unity-simulation",children:"Chapter 3: Simulation"})," | ",(0,o.jsx)(e.a,{href:"part-6-conversational/conversational-robotics",children:"Part 6: Conversational Robotics"})]})]})}function _(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(c,{...n})}):c(n)}}}]);